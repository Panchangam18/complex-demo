{
  "level_1": [
    {
      "goal": "Create and validate a complete Terraform configuration for AWS VPC with multiple subnets and security groups",
      "steps": [
        {
          "name": "Analyze existing Terraform project structure",
          "type": "prompt",
          "prompt": "Examine the terraform/ directory structure to understand module organization, naming conventions, and existing patterns. Identify the appropriate location for VPC-related configurations.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create VPC configuration with CIDR block",
          "type": "prompt",
          "prompt": "Create a Terraform configuration for a VPC with CIDR block 10.0.0.0/16. Enable DNS support and DNS hostnames. Include appropriate tags for environment='dev' and project='devops-demo'.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/vpc/main.tf"
          ]
        },
        {
          "name": "Configure public and private subnets",
          "type": "prompt",
          "prompt": "Create public subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) and private subnets (10.0.11.0/24, 10.0.12.0/24, 10.0.13.0/24) across 3 availability zones. Include appropriate tags and map_public_ip_on_launch settings.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/vpc/subnets.tf"
          ]
        },
        {
          "name": "Create Internet Gateway and NAT Gateways",
          "type": "prompt",
          "prompt": "Configure an Internet Gateway for the VPC and create NAT Gateways in each public subnet for outbound internet access from private subnets. Include Elastic IPs for NAT Gateways.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/vpc/gateways.tf"
          ]
        },
        {
          "name": "Configure route tables and associations",
          "type": "prompt",
          "prompt": "Create route tables for public subnets (with routes to Internet Gateway) and private subnets (with routes to NAT Gateways). Associate each subnet with its appropriate route table.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/vpc/routes.tf"
          ]
        },
        {
          "name": "Create security groups for multi-tier architecture",
          "type": "prompt",
          "prompt": "Create security groups for: web tier (allow 80/443 from internet), app tier (allow traffic from web tier), database tier (allow 5432 from app tier), and bastion host (allow SSH from specific IPs).",
          "parameters": {},
          "files": [
            "terraform/modules/aws/vpc/security_groups.tf"
          ]
        },
        {
          "name": "Format all Terraform files",
          "type": "cli",
          "command": "terraform fmt -recursive terraform/modules/aws/vpc/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Initialize Terraform module",
          "type": "cli",
          "command": "terraform init",
          "parameters": {
            "working_directory": "terraform/modules/aws/vpc/"
          },
          "files": []
        },
        {
          "name": "Validate Terraform configuration",
          "type": "cli",
          "command": "terraform validate",
          "parameters": {
            "working_directory": "terraform/modules/aws/vpc/"
          },
          "files": []
        },
        {
          "name": "Run tfsec security scan",
          "type": "cli",
          "command": "tfsec terraform/modules/aws/vpc/ --format json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate Terraform plan",
          "type": "cli",
          "command": "terraform plan -out=vpc.tfplan",
          "parameters": {
            "working_directory": "terraform/modules/aws/vpc/"
          },
          "files": []
        },
        {
          "name": "Verify plan creates expected resources",
          "type": "prompt",
          "prompt": "Review the terraform plan output and verify it will create exactly: 1 VPC, 6 subnets, 1 Internet Gateway, 3 NAT Gateways, 3 Elastic IPs, 5 route tables, and 4 security groups with proper configurations.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Write Kubernetes manifests for a microservices application with service mesh integration",
      "steps": [
        {
          "name": "Analyze existing Kubernetes configurations",
          "type": "prompt",
          "prompt": "Examine the k8s/ directory structure to understand the existing manifests, namespace organization, and deployment patterns. Review any existing service mesh configurations.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create namespace configuration with labels",
          "type": "prompt",
          "prompt": "Create a namespace manifest for 'microservices-demo' with appropriate labels for service mesh injection (istio-injection=enabled) and environment tags.",
          "parameters": {},
          "files": [
            "k8s/namespaces/microservices-demo.yaml"
          ]
        },
        {
          "name": "Write frontend service deployment manifest",
          "type": "prompt",
          "prompt": "Create a Kubernetes deployment for the frontend service with 3 replicas, resource limits (cpu: 200m, memory: 256Mi), readiness/liveness probes, and anti-affinity rules for high availability.",
          "parameters": {},
          "files": [
            "k8s/envs/dev/microservices/frontend-deployment.yaml"
          ]
        },
        {
          "name": "Create backend API deployment manifest",
          "type": "prompt",
          "prompt": "Write deployment manifest for backend API service with 5 replicas, horizontal pod autoscaling configuration, persistent volume claims for data, and init containers for database migration.",
          "parameters": {},
          "files": [
            "k8s/envs/dev/microservices/backend-deployment.yaml"
          ]
        },
        {
          "name": "Configure services and ingress",
          "type": "prompt",
          "prompt": "Create service definitions for frontend (ClusterIP) and backend (ClusterIP), plus an Ingress resource with TLS termination, path-based routing, and rate limiting annotations.",
          "parameters": {},
          "files": [
            "k8s/envs/dev/microservices/services.yaml",
            "k8s/envs/dev/microservices/ingress.yaml"
          ]
        },
        {
          "name": "Add ConfigMaps and Secrets",
          "type": "prompt",
          "prompt": "Create ConfigMaps for application configuration (API endpoints, feature flags) and Secret manifests for database credentials and API keys with proper base64 encoding.",
          "parameters": {},
          "files": [
            "k8s/envs/dev/microservices/configmaps.yaml",
            "k8s/envs/dev/microservices/secrets.yaml"
          ]
        },
        {
          "name": "Write service mesh traffic policies",
          "type": "prompt",
          "prompt": "Create Istio VirtualService and DestinationRule resources for canary deployments (90/10 traffic split), circuit breaking, and retry policies for the microservices.",
          "parameters": {},
          "files": [
            "k8s/envs/dev/microservices/istio-policies.yaml"
          ]
        },
        {
          "name": "Validate Kubernetes manifests syntax",
          "type": "cli",
          "command": "kubectl apply --dry-run=client -f k8s/envs/dev/microservices/ --recursive",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run Kubernetes security scanner",
          "type": "cli",
          "command": "kubesec scan k8s/envs/dev/microservices/*.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate service mesh configurations",
          "type": "cli",
          "command": "istioctl analyze k8s/envs/dev/microservices/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check manifest best practices",
          "type": "cli",
          "command": "kube-score score k8s/envs/dev/microservices/*.yaml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create comprehensive Docker Compose configuration for local development environment",
      "steps": [
        {
          "name": "Review existing Docker configurations",
          "type": "prompt",
          "prompt": "Analyze the existing Dockerfiles and any docker-compose files in the codebase. Understand the application architecture and service dependencies.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create multi-service Docker Compose file",
          "type": "prompt",
          "prompt": "Write a docker-compose.yml with services for: frontend (Vue.js), backend (Node.js), PostgreSQL database, Redis cache, and Nginx reverse proxy. Include proper networking and volume configurations.",
          "parameters": {},
          "files": [
            "docker-compose.yml"
          ]
        },
        {
          "name": "Configure environment variables",
          "type": "prompt",
          "prompt": "Create a .env.example file with all required environment variables for database connections, API keys, service URLs, and feature flags. Include detailed comments for each variable.",
          "parameters": {},
          "files": [
            ".env.example"
          ]
        },
        {
          "name": "Add development overrides",
          "type": "prompt",
          "prompt": "Create docker-compose.override.yml for development-specific settings including volume mounts for hot reloading, exposed ports for debugging, and development environment variables.",
          "parameters": {},
          "files": [
            "docker-compose.override.yml"
          ]
        },
        {
          "name": "Create health check configurations",
          "type": "prompt",
          "prompt": "Add health check definitions for each service in docker-compose.yml including appropriate intervals, timeouts, retries, and start periods based on service startup times.",
          "parameters": {},
          "files": [
            "docker-compose.yml"
          ]
        },
        {
          "name": "Validate Docker Compose syntax",
          "type": "cli",
          "command": "docker-compose config",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check for security issues",
          "type": "cli",
          "command": "docker-compose config | docker run --rm -i hadolint/hadolint:latest-alpine hadolint --ignore DL3008 --ignore DL3009 -",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify service dependencies",
          "type": "prompt",
          "prompt": "Review the docker-compose configuration to ensure proper service dependencies, startup order, and that all required environment variables are defined.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design and implement Ansible playbooks for configuration management across multiple environments",
      "steps": [
        {
          "name": "Analyze existing Ansible structure",
          "type": "prompt",
          "prompt": "Examine the ansible/ directory to understand the current inventory structure, playbook organization, roles, and variable management patterns.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create comprehensive inventory structure",
          "type": "prompt",
          "prompt": "Design a dynamic inventory structure with groups for web servers, app servers, databases, and monitoring. Include group variables for each environment (dev, staging, prod).",
          "parameters": {},
          "files": [
            "ansible/inventory/hosts.yml",
            "ansible/inventory/group_vars/all.yml"
          ]
        },
        {
          "name": "Write base system configuration playbook",
          "type": "prompt",
          "prompt": "Create a playbook for base system setup including user management, SSH hardening, firewall configuration, time synchronization, and system monitoring agents installation.",
          "parameters": {},
          "files": [
            "ansible/playbooks/base-configuration.yml"
          ]
        },
        {
          "name": "Develop application deployment playbook",
          "type": "prompt",
          "prompt": "Write a playbook for deploying the application with tasks for code deployment, dependency installation, configuration file templating, service management, and zero-downtime deployment strategies.",
          "parameters": {},
          "files": [
            "ansible/playbooks/app-deployment.yml"
          ]
        },
        {
          "name": "Create security hardening playbook",
          "type": "prompt",
          "prompt": "Implement a security hardening playbook following CIS benchmarks including kernel parameter tuning, audit logging, access controls, and automated security updates configuration.",
          "parameters": {},
          "files": [
            "ansible/playbooks/security-hardening.yml"
          ]
        },
        {
          "name": "Design monitoring setup playbook",
          "type": "prompt",
          "prompt": "Create playbook for installing and configuring monitoring stack: Prometheus node exporters, Grafana agents, log collectors, and custom application metrics exporters.",
          "parameters": {},
          "files": [
            "ansible/playbooks/monitoring-setup.yml"
          ]
        },
        {
          "name": "Implement backup configuration playbook",
          "type": "prompt",
          "prompt": "Write a playbook for setting up automated backups including database dumps, file system snapshots, backup rotation policies, and remote backup storage configuration.",
          "parameters": {},
          "files": [
            "ansible/playbooks/backup-configuration.yml"
          ]
        },
        {
          "name": "Lint all Ansible playbooks",
          "type": "cli",
          "command": "ansible-lint ansible/playbooks/*.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate playbook syntax",
          "type": "cli",
          "command": "ansible-playbook --syntax-check ansible/playbooks/*.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run security audit on playbooks",
          "type": "cli",
          "command": "ansible-playbook ansible/playbooks/security-hardening.yml --check --diff",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate playbook documentation",
          "type": "prompt",
          "prompt": "Review all created playbooks and verify they follow Ansible best practices, use proper variable management, implement idempotency, and include appropriate error handling.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create comprehensive IAM policies and security configurations for multi-account AWS organization",
      "steps": [
        {
          "name": "Analyze organization structure",
          "type": "prompt",
          "prompt": "Review the AWS organization structure to understand account hierarchy, organizational units (OUs), and existing permission boundaries. Identify cross-account access requirements.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design least-privilege IAM policies",
          "type": "prompt",
          "prompt": "Create IAM policy documents following least-privilege principles for: developers (read-only prod, full dev), DevOps engineers (limited prod, full dev/staging), and security team (audit access across all accounts).",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/policies/developer-policy.json",
            "terraform/modules/aws/iam/policies/devops-policy.json",
            "terraform/modules/aws/iam/policies/security-policy.json"
          ]
        },
        {
          "name": "Configure permission boundaries",
          "type": "prompt",
          "prompt": "Create permission boundary policies to prevent privilege escalation. Define boundaries that restrict IAM role creation, prevent deletion of audit trails, and limit resource creation to specific regions.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/permission-boundaries.tf"
          ]
        },
        {
          "name": "Implement service control policies",
          "type": "prompt",
          "prompt": "Write AWS Organizations SCPs to enforce security standards: require MFA for sensitive operations, prevent disabling of CloudTrail, enforce encryption, and restrict root account usage.",
          "parameters": {},
          "files": [
            "organizations/scps/security-baseline.json",
            "organizations/scps/data-protection.json"
          ]
        },
        {
          "name": "Create cross-account roles",
          "type": "prompt",
          "prompt": "Design Terraform configurations for cross-account roles enabling: centralized logging access, security audit access, and emergency break-glass access with proper trust relationships.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/cross-account-roles.tf"
          ]
        },
        {
          "name": "Validate IAM policies syntax",
          "type": "cli",
          "command": "aws iam simulate-principal-policy --policy-source-arn arn:aws:iam::123456789012:user/test-user --action-names s3:GetObject --resource-arns arn:aws:s3:::example-bucket/*",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run IAM policy validator",
          "type": "cli",
          "command": "aws accessanalyzer validate-policy --policy-document file://terraform/modules/aws/iam/policies/developer-policy.json --policy-type IDENTITY_POLICY",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check for overly permissive policies",
          "type": "cli",
          "command": "checkov -f terraform/modules/aws/iam/ --framework terraform --check CKV_AWS_109,CKV_AWS_110,CKV_AWS_111",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate IAM documentation",
          "type": "prompt",
          "prompt": "Create comprehensive documentation of all IAM policies, including use cases, permission mappings, and approval workflows for privilege escalation requests.",
          "parameters": {},
          "files": [
            "docs/iam-policy-documentation.md"
          ]
        },
        {
          "name": "Validate compliance requirements",
          "type": "prompt",
          "prompt": "Review all created IAM configurations to ensure they meet compliance requirements for SOC2, PCI-DSS, and HIPAA. Verify audit logging and access review processes are in place.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design and implement GitOps workflow with ArgoCD for multi-environment deployments",
      "steps": [
        {
          "name": "Review repository structure",
          "type": "prompt",
          "prompt": "Analyze the existing Git repository structure to design an optimal GitOps layout with separate repos/directories for application code, infrastructure code, and environment configurations.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create ArgoCD application manifests",
          "type": "prompt",
          "prompt": "Write ArgoCD Application CRDs for each environment (dev, staging, prod) with appropriate sync policies, automated pruning, and self-healing configurations.",
          "parameters": {},
          "files": [
            "k8s/argocd/applications/dev-app.yaml",
            "k8s/argocd/applications/staging-app.yaml",
            "k8s/argocd/applications/prod-app.yaml"
          ]
        },
        {
          "name": "Configure Kustomize overlays",
          "type": "prompt",
          "prompt": "Create Kustomize base configurations and environment-specific overlays for customizing deployments across environments with different resource limits, replicas, and configurations.",
          "parameters": {},
          "files": [
            "k8s/base/kustomization.yaml",
            "k8s/overlays/dev/kustomization.yaml",
            "k8s/overlays/staging/kustomization.yaml",
            "k8s/overlays/prod/kustomization.yaml"
          ]
        },
        {
          "name": "Implement sealed secrets",
          "type": "prompt",
          "prompt": "Create SealedSecret configurations for managing sensitive data in Git. Include database passwords, API keys, and TLS certificates encrypted with the cluster's public key.",
          "parameters": {},
          "files": [
            "k8s/sealed-secrets/database-credentials.yaml",
            "k8s/sealed-secrets/api-keys.yaml"
          ]
        },
        {
          "name": "Design progressive delivery strategy",
          "type": "prompt",
          "prompt": "Implement Flagger or Argo Rollouts configurations for canary deployments, blue-green deployments, and automated rollback based on metrics from Prometheus.",
          "parameters": {},
          "files": [
            "k8s/progressive-delivery/canary-analysis.yaml",
            "k8s/progressive-delivery/rollout-strategy.yaml"
          ]
        },
        {
          "name": "Create sync policies",
          "type": "prompt",
          "prompt": "Define ArgoCD sync policies with automated sync for dev, manual approval for staging, and restricted sync windows for production deployments.",
          "parameters": {},
          "files": [
            "k8s/argocd/policies/sync-policies.yaml"
          ]
        },
        {
          "name": "Validate ArgoCD configurations",
          "type": "cli",
          "command": "argocd app create --dry-run --validate -f k8s/argocd/applications/dev-app.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test Kustomize builds",
          "type": "cli",
          "command": "kustomize build k8s/overlays/dev | kubectl apply --dry-run=client -f -",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify sealed secrets encryption",
          "type": "cli",
          "command": "kubeseal --validate < k8s/sealed-secrets/database-credentials.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Lint GitOps repository",
          "type": "cli",
          "command": "yamllint -d '{extends: relaxed, rules: {line-length: {max: 120}}}' k8s/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document GitOps workflow",
          "type": "prompt",
          "prompt": "Create comprehensive documentation covering the GitOps workflow, branch strategies, promotion process between environments, and emergency manual deployment procedures.",
          "parameters": {},
          "files": [
            "docs/gitops-workflow-guide.md"
          ]
        }
      ]
    },
    {
      "goal": "Create Helm charts for microservices with advanced templating and dependency management",
      "steps": [
        {
          "name": "Initialize Helm chart structure",
          "type": "cli",
          "command": "helm create microservices-chart",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design values schema",
          "type": "prompt",
          "prompt": "Create a comprehensive values.schema.json file defining all configurable parameters with types, constraints, and defaults for the microservices deployment.",
          "parameters": {},
          "files": [
            "microservices-chart/values.schema.json"
          ]
        },
        {
          "name": "Implement service templates",
          "type": "prompt",
          "prompt": "Create Helm templates for deployments, services, ingresses, and HPAs with advanced templating features including range loops, conditionals, and helper functions.",
          "parameters": {},
          "files": [
            "microservices-chart/templates/deployment.yaml",
            "microservices-chart/templates/service.yaml",
            "microservices-chart/templates/ingress.yaml",
            "microservices-chart/templates/hpa.yaml"
          ]
        },
        {
          "name": "Add dependency charts",
          "type": "prompt",
          "prompt": "Configure Chart.yaml to include dependencies for PostgreSQL, Redis, and Prometheus operator. Set up conditional enabling and version constraints.",
          "parameters": {},
          "files": [
            "microservices-chart/Chart.yaml"
          ]
        },
        {
          "name": "Create helper templates",
          "type": "prompt",
          "prompt": "Develop _helpers.tpl with reusable template functions for generating resource names, labels, annotations, and security contexts consistently across all resources.",
          "parameters": {},
          "files": [
            "microservices-chart/templates/_helpers.tpl"
          ]
        },
        {
          "name": "Implement hooks and tests",
          "type": "prompt",
          "prompt": "Create Helm hooks for pre-install database migrations, post-upgrade cache clearing, and test jobs to validate deployments are functioning correctly.",
          "parameters": {},
          "files": [
            "microservices-chart/templates/hooks/db-migration.yaml",
            "microservices-chart/templates/tests/api-test.yaml"
          ]
        },
        {
          "name": "Configure environment values",
          "type": "prompt",
          "prompt": "Create environment-specific values files for dev, staging, and production with appropriate resource limits, replica counts, and feature flags.",
          "parameters": {},
          "files": [
            "microservices-chart/values-dev.yaml",
            "microservices-chart/values-staging.yaml",
            "microservices-chart/values-prod.yaml"
          ]
        },
        {
          "name": "Lint Helm chart",
          "type": "cli",
          "command": "helm lint microservices-chart/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test chart rendering",
          "type": "cli",
          "command": "helm template test-release microservices-chart/ -f microservices-chart/values-dev.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate against Kubernetes schemas",
          "type": "cli",
          "command": "helm template microservices-chart/ | kubeval --strict",
          "parameters": {},
          "files": []
        },
        {
          "name": "Package and index chart",
          "type": "cli",
          "command": "helm package microservices-chart/ && helm repo index .",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document chart usage",
          "type": "prompt",
          "prompt": "Create comprehensive README with installation instructions, configuration options, upgrade procedures, and troubleshooting guides for the Helm chart.",
          "parameters": {},
          "files": [
            "microservices-chart/README.md"
          ]
        }
      ]
    },
    {
      "goal": "Implement comprehensive logging and tracing solution with ELK stack and OpenTelemetry",
      "steps": [
        {
          "name": "Design logging architecture",
          "type": "prompt",
          "prompt": "Create an architectural design for centralized logging using Elasticsearch, Logstash, Kibana, and Filebeat. Include log retention policies, index lifecycle management, and multi-tenancy considerations.",
          "parameters": {},
          "files": [
            "docs/logging-architecture.md"
          ]
        },
        {
          "name": "Configure Elasticsearch cluster",
          "type": "prompt",
          "prompt": "Write Terraform configuration for a production-ready Elasticsearch cluster with 3 master nodes, 5 data nodes, and 2 coordinator nodes. Include snapshot policies and security settings.",
          "parameters": {},
          "files": [
            "terraform/modules/elasticsearch/main.tf",
            "terraform/modules/elasticsearch/security.tf"
          ]
        },
        {
          "name": "Create Logstash pipelines",
          "type": "prompt",
          "prompt": "Develop Logstash pipeline configurations for parsing application logs, system logs, and audit logs. Include grok patterns, enrichment filters, and error handling.",
          "parameters": {},
          "files": [
            "logstash/pipelines/application.conf",
            "logstash/pipelines/system.conf",
            "logstash/pipelines/audit.conf"
          ]
        },
        {
          "name": "Deploy Filebeat DaemonSet",
          "type": "prompt",
          "prompt": "Create Kubernetes DaemonSet configuration for Filebeat with autodiscovery, multiline pattern detection, and processor configurations for different log formats.",
          "parameters": {},
          "files": [
            "k8s/logging/filebeat-daemonset.yaml",
            "k8s/logging/filebeat-config.yaml"
          ]
        },
        {
          "name": "Implement OpenTelemetry collector",
          "type": "prompt",
          "prompt": "Configure OpenTelemetry collector for distributed tracing with receivers for Jaeger, Zipkin, and OTLP. Set up processors for sampling and exporters to multiple backends.",
          "parameters": {},
          "files": [
            "k8s/observability/otel-collector-config.yaml",
            "k8s/observability/otel-collector-deployment.yaml"
          ]
        },
        {
          "name": "Create Kibana dashboards",
          "type": "prompt",
          "prompt": "Design Kibana dashboards for application performance monitoring, error tracking, and security incident investigation. Include saved searches, visualizations, and alerts.",
          "parameters": {},
          "files": [
            "kibana/dashboards/application-performance.json",
            "kibana/dashboards/error-analysis.json",
            "kibana/dashboards/security-monitoring.json"
          ]
        },
        {
          "name": "Configure index templates",
          "type": "cli",
          "command": "curl -X PUT 'localhost:9200/_index_template/logs-template' -H 'Content-Type: application/json' -d @elasticsearch/templates/logs-template.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Set up index lifecycle policies",
          "type": "cli",
          "command": "curl -X PUT 'localhost:9200/_ilm/policy/logs-policy' -H 'Content-Type: application/json' -d @elasticsearch/policies/logs-ilm-policy.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test log ingestion pipeline",
          "type": "cli",
          "command": "echo '{\"@timestamp\":\"2024-01-15T10:00:00Z\",\"level\":\"ERROR\",\"message\":\"Test error message\"}' | nc localhost 5000",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify trace collection",
          "type": "cli",
          "command": "curl -X POST 'localhost:4318/v1/traces' -H 'Content-Type: application/json' -d @test-data/sample-trace.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure log retention and archival",
          "type": "prompt",
          "prompt": "Implement automated log retention policies with S3 archival for compliance. Set up lifecycle rules to move old indices to cold storage and delete after retention period.",
          "parameters": {},
          "files": [
            "scripts/log-archival.sh",
            "terraform/modules/s3/log-archive-bucket.tf"
          ]
        },
        {
          "name": "Document observability practices",
          "type": "prompt",
          "prompt": "Create comprehensive documentation covering logging standards, trace context propagation, dashboard usage, and troubleshooting procedures for the observability stack.",
          "parameters": {},
          "files": [
            "docs/observability-guide.md"
          ]
        }
      ]
    },
    {
      "goal": "Create comprehensive IAM policies and security configurations for multi-account AWS organization",
      "steps": [
        {
          "name": "Analyze organization structure",
          "type": "prompt",
          "prompt": "Review the AWS organization structure to understand account hierarchy, organizational units (OUs), and existing permission boundaries. Identify cross-account access requirements.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design least-privilege IAM policies",
          "type": "prompt",
          "prompt": "Create IAM policy documents following least-privilege principles for: developers (read-only prod, full dev), DevOps engineers (limited prod, full dev/staging), and security team (audit access across all accounts).",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/policies/developer-policy.json",
            "terraform/modules/aws/iam/policies/devops-policy.json",
            "terraform/modules/aws/iam/policies/security-policy.json"
          ]
        },
        {
          "name": "Configure permission boundaries",
          "type": "prompt",
          "prompt": "Create permission boundary policies to prevent privilege escalation. Define boundaries that restrict IAM role creation, prevent deletion of audit trails, and limit resource creation to specific regions.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/permission-boundaries.tf"
          ]
        },
        {
          "name": "Implement service control policies",
          "type": "prompt",
          "prompt": "Write AWS Organizations SCPs to enforce security standards: require MFA for sensitive operations, prevent disabling of CloudTrail, enforce encryption, and restrict root account usage.",
          "parameters": {},
          "files": [
            "organizations/scps/security-baseline.json",
            "organizations/scps/data-protection.json"
          ]
        },
        {
          "name": "Create cross-account roles",
          "type": "prompt",
          "prompt": "Design Terraform configurations for cross-account roles enabling: centralized logging access, security audit access, and emergency break-glass access with proper trust relationships.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/cross-account-roles.tf"
          ]
        },
        {
          "name": "Validate IAM policies syntax",
          "type": "cli",
          "command": "aws iam simulate-principal-policy --policy-source-arn arn:aws:iam::123456789012:user/test-user --action-names s3:GetObject --resource-arns arn:aws:s3:::example-bucket/*",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run IAM policy validator",
          "type": "cli",
          "command": "aws accessanalyzer validate-policy --policy-document file://terraform/modules/aws/iam/policies/developer-policy.json --policy-type IDENTITY_POLICY",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check for overly permissive policies",
          "type": "cli",
          "command": "checkov -f terraform/modules/aws/iam/ --framework terraform --check CKV_AWS_109,CKV_AWS_110,CKV_AWS_111",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate IAM documentation",
          "type": "prompt",
          "prompt": "Create comprehensive documentation of all IAM policies, including use cases, permission mappings, and approval workflows for privilege escalation requests.",
          "parameters": {},
          "files": [
            "docs/iam-policy-documentation.md"
          ]
        },
        {
          "name": "Validate compliance requirements",
          "type": "prompt",
          "prompt": "Review all created IAM configurations to ensure they meet compliance requirements for SOC2, PCI-DSS, and HIPAA. Verify audit logging and access review processes are in place.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design and implement GitOps workflow with ArgoCD for multi-environment deployments",
      "steps": [
        {
          "name": "Review repository structure",
          "type": "prompt",
          "prompt": "Analyze the existing Git repository structure to design an optimal GitOps layout with separate repos/directories for application code, infrastructure code, and environment configurations.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create ArgoCD application manifests",
          "type": "prompt",
          "prompt": "Write ArgoCD Application CRDs for each environment (dev, staging, prod) with appropriate sync policies, automated pruning, and self-healing configurations.",
          "parameters": {},
          "files": [
            "k8s/argocd/applications/dev-app.yaml",
            "k8s/argocd/applications/staging-app.yaml",
            "k8s/argocd/applications/prod-app.yaml"
          ]
        },
        {
          "name": "Configure Kustomize overlays",
          "type": "prompt",
          "prompt": "Create Kustomize base configurations and environment-specific overlays for customizing deployments across environments with different resource limits, replicas, and configurations.",
          "parameters": {},
          "files": [
            "k8s/base/kustomization.yaml",
            "k8s/overlays/dev/kustomization.yaml",
            "k8s/overlays/staging/kustomization.yaml",
            "k8s/overlays/prod/kustomization.yaml"
          ]
        },
        {
          "name": "Implement sealed secrets",
          "type": "prompt",
          "prompt": "Create SealedSecret configurations for managing sensitive data in Git. Include database passwords, API keys, and TLS certificates encrypted with the cluster's public key.",
          "parameters": {},
          "files": [
            "k8s/sealed-secrets/database-credentials.yaml",
            "k8s/sealed-secrets/api-keys.yaml"
          ]
        },
        {
          "name": "Design progressive delivery strategy",
          "type": "prompt",
          "prompt": "Implement Flagger or Argo Rollouts configurations for canary deployments, blue-green deployments, and automated rollback based on metrics from Prometheus.",
          "parameters": {},
          "files": [
            "k8s/progressive-delivery/canary-analysis.yaml",
            "k8s/progressive-delivery/rollout-strategy.yaml"
          ]
        },
        {
          "name": "Create sync policies",
          "type": "prompt",
          "prompt": "Define ArgoCD sync policies with automated sync for dev, manual approval for staging, and restricted sync windows for production deployments.",
          "parameters": {},
          "files": [
            "k8s/argocd/policies/sync-policies.yaml"
          ]
        },
        {
          "name": "Validate ArgoCD configurations",
          "type": "cli",
          "command": "argocd app create --dry-run --validate -f k8s/argocd/applications/dev-app.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test Kustomize builds",
          "type": "cli",
          "command": "kustomize build k8s/overlays/dev | kubectl apply --dry-run=client -f -",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify sealed secrets encryption",
          "type": "cli",
          "command": "kubeseal --validate < k8s/sealed-secrets/database-credentials.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Lint GitOps repository",
          "type": "cli",
          "command": "yamllint -d '{extends: relaxed, rules: {line-length: {max: 120}}}' k8s/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document GitOps workflow",
          "type": "prompt",
          "prompt": "Create comprehensive documentation covering the GitOps workflow, branch strategies, promotion process between environments, and emergency manual deployment procedures.",
          "parameters": {},
          "files": [
            "docs/gitops-workflow-guide.md"
          ]
        }
      ]
    },
    {
      "goal": "Create Helm charts for microservices with advanced templating and dependency management",
      "steps": [
        {
          "name": "Initialize Helm chart structure",
          "type": "cli",
          "command": "helm create microservices-chart",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design values schema",
          "type": "prompt",
          "prompt": "Create a comprehensive values.schema.json file defining all configurable parameters with types, constraints, and defaults for the microservices deployment.",
          "parameters": {},
          "files": [
            "microservices-chart/values.schema.json"
          ]
        },
        {
          "name": "Implement service templates",
          "type": "prompt",
          "prompt": "Create Helm templates for deployments, services, ingresses, and HPAs with advanced templating features including range loops, conditionals, and helper functions.",
          "parameters": {},
          "files": [
            "microservices-chart/templates/deployment.yaml",
            "microservices-chart/templates/service.yaml",
            "microservices-chart/templates/ingress.yaml",
            "microservices-chart/templates/hpa.yaml"
          ]
        },
        {
          "name": "Add dependency charts",
          "type": "prompt",
          "prompt": "Configure Chart.yaml to include dependencies for PostgreSQL, Redis, and Prometheus operator. Set up conditional enabling and version constraints.",
          "parameters": {},
          "files": [
            "microservices-chart/Chart.yaml"
          ]
        },
        {
          "name": "Create helper templates",
          "type": "prompt",
          "prompt": "Develop _helpers.tpl with reusable template functions for generating resource names, labels, annotations, and security contexts consistently across all resources.",
          "parameters": {},
          "files": [
            "microservices-chart/templates/_helpers.tpl"
          ]
        },
        {
          "name": "Implement hooks and tests",
          "type": "prompt",
          "prompt": "Create Helm hooks for pre-install database migrations, post-upgrade cache clearing, and test jobs to validate deployments are functioning correctly.",
          "parameters": {},
          "files": [
            "microservices-chart/templates/hooks/db-migration.yaml",
            "microservices-chart/templates/tests/api-test.yaml"
          ]
        },
        {
          "name": "Configure environment values",
          "type": "prompt",
          "prompt": "Create environment-specific values files for dev, staging, and production with appropriate resource limits, replica counts, and feature flags.",
          "parameters": {},
          "files": [
            "microservices-chart/values-dev.yaml",
            "microservices-chart/values-staging.yaml",
            "microservices-chart/values-prod.yaml"
          ]
        },
        {
          "name": "Lint Helm chart",
          "type": "cli",
          "command": "helm lint microservices-chart/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test chart rendering",
          "type": "cli",
          "command": "helm template test-release microservices-chart/ -f microservices-chart/values-dev.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate against Kubernetes schemas",
          "type": "cli",
          "command": "helm template microservices-chart/ | kubeval --strict",
          "parameters": {},
          "files": []
        },
        {
          "name": "Package and index chart",
          "type": "cli",
          "command": "helm package microservices-chart/ && helm repo index .",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document chart usage",
          "type": "prompt",
          "prompt": "Create comprehensive README with installation instructions, configuration options, upgrade procedures, and troubleshooting guides for the Helm chart.",
          "parameters": {},
          "files": [
            "microservices-chart/README.md"
          ]
        }
      ]
    },
    {
      "goal": "Implement comprehensive logging and tracing solution with ELK stack and OpenTelemetry",
      "steps": [
        {
          "name": "Design logging architecture",
          "type": "prompt",
          "prompt": "Create an architectural design for centralized logging using Elasticsearch, Logstash, Kibana, and Filebeat. Include log retention policies, index lifecycle management, and multi-tenancy considerations.",
          "parameters": {},
          "files": [
            "docs/logging-architecture.md"
          ]
        },
        {
          "name": "Configure Elasticsearch cluster",
          "type": "prompt",
          "prompt": "Write Terraform configuration for a production-ready Elasticsearch cluster with 3 master nodes, 5 data nodes, and 2 coordinator nodes. Include snapshot policies and security settings.",
          "parameters": {},
          "files": [
            "terraform/modules/elasticsearch/main.tf",
            "terraform/modules/elasticsearch/security.tf"
          ]
        },
        {
          "name": "Create Logstash pipelines",
          "type": "prompt",
          "prompt": "Develop Logstash pipeline configurations for parsing application logs, system logs, and audit logs. Include grok patterns, enrichment filters, and error handling.",
          "parameters": {},
          "files": [
            "logstash/pipelines/application.conf",
            "logstash/pipelines/system.conf",
            "logstash/pipelines/audit.conf"
          ]
        },
        {
          "name": "Deploy Filebeat DaemonSet",
          "type": "prompt",
          "prompt": "Create Kubernetes DaemonSet configuration for Filebeat with autodiscovery, multiline pattern detection, and processor configurations for different log formats.",
          "parameters": {},
          "files": [
            "k8s/logging/filebeat-daemonset.yaml",
            "k8s/logging/filebeat-config.yaml"
          ]
        },
        {
          "name": "Implement OpenTelemetry collector",
          "type": "prompt",
          "prompt": "Configure OpenTelemetry collector for distributed tracing with receivers for Jaeger, Zipkin, and OTLP. Set up processors for sampling and exporters to multiple backends.",
          "parameters": {},
          "files": [
            "k8s/observability/otel-collector-config.yaml",
            "k8s/observability/otel-collector-deployment.yaml"
          ]
        },
        {
          "name": "Create Kibana dashboards",
          "type": "prompt",
          "prompt": "Design Kibana dashboards for application performance monitoring, error tracking, and security incident investigation. Include saved searches, visualizations, and alerts.",
          "parameters": {},
          "files": [
            "kibana/dashboards/application-performance.json",
            "kibana/dashboards/error-analysis.json",
            "kibana/dashboards/security-monitoring.json"
          ]
        },
        {
          "name": "Configure index templates",
          "type": "cli",
          "command": "curl -X PUT 'localhost:9200/_index_template/logs-template' -H 'Content-Type: application/json' -d @elasticsearch/templates/logs-template.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Set up index lifecycle policies",
          "type": "cli",
          "command": "curl -X PUT 'localhost:9200/_ilm/policy/logs-policy' -H 'Content-Type: application/json' -d @elasticsearch/policies/logs-ilm-policy.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test log ingestion pipeline",
          "type": "cli",
          "command": "echo '{\"@timestamp\":\"2024-01-15T10:00:00Z\",\"level\":\"ERROR\",\"message\":\"Test error message\"}' | nc localhost 5000",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify trace collection",
          "type": "cli",
          "command": "curl -X POST 'localhost:4318/v1/traces' -H 'Content-Type: application/json' -d @test-data/sample-trace.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure log retention and archival",
          "type": "prompt",
          "prompt": "Implement automated log retention policies with S3 archival for compliance. Set up lifecycle rules to move old indices to cold storage and delete after retention period.",
          "parameters": {},
          "files": [
            "scripts/log-archival.sh",
            "terraform/modules/s3/log-archive-bucket.tf"
          ]
        },
        {
          "name": "Document observability practices",
          "type": "prompt",
          "prompt": "Create comprehensive documentation covering logging standards, trace context propagation, dashboard usage, and troubleshooting procedures for the observability stack.",
          "parameters": {},
          "files": [
            "docs/observability-guide.md"
          ]
        }
      ]
    },
    {
      "goal": "Create serverless application infrastructure with AWS Lambda, API Gateway, and DynamoDB",
      "steps": [
        {
          "name": "Design serverless architecture",
          "type": "prompt",
          "prompt": "Create architectural design for a serverless microservices application using Lambda functions, API Gateway for REST APIs, and DynamoDB for data storage. Include event-driven patterns.",
          "parameters": {},
          "files": [
            "docs/serverless-architecture.md"
          ]
        },
        {
          "name": "Write Lambda function code",
          "type": "prompt",
          "prompt": "Develop Lambda functions in Python for CRUD operations on a product catalog. Include proper error handling, input validation, and AWS SDK integration for DynamoDB access.",
          "parameters": {},
          "files": [
            "lambda-functions/products/handler.py",
            "lambda-functions/products/requirements.txt"
          ]
        },
        {
          "name": "Create SAM template",
          "type": "prompt",
          "prompt": "Write AWS SAM (Serverless Application Model) template defining Lambda functions, API Gateway endpoints, DynamoDB tables, and IAM roles with least-privilege permissions.",
          "parameters": {},
          "files": [
            "serverless/template.yaml"
          ]
        },
        {
          "name": "Configure API Gateway",
          "type": "prompt",
          "prompt": "Design API Gateway configuration with request/response models, input validation, CORS settings, and usage plans with API keys for rate limiting.",
          "parameters": {},
          "files": [
            "serverless/api-spec.yaml"
          ]
        },
        {
          "name": "Add Lambda layers",
          "type": "prompt",
          "prompt": "Create Lambda layers for shared dependencies and utilities. Include AWS SDK extensions, common validation functions, and logging utilities.",
          "parameters": {},
          "files": [
            "lambda-layers/shared-utils/python/",
            "lambda-layers/build-layers.sh"
          ]
        },
        {
          "name": "Implement caching strategy",
          "type": "prompt",
          "prompt": "Configure API Gateway caching and DynamoDB DAX (DynamoDB Accelerator) for improved performance. Set appropriate TTL values and cache key parameters.",
          "parameters": {},
          "files": [
            "serverless/caching-config.yaml"
          ]
        },
        {
          "name": "Validate SAM template",
          "type": "cli",
          "command": "sam validate --template serverless/template.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Build Lambda packages",
          "type": "cli",
          "command": "sam build --template serverless/template.yaml --use-container",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run local testing",
          "type": "cli",
          "command": "sam local start-api --template serverless/template.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test API endpoints locally",
          "type": "cli",
          "command": "curl -X POST http://localhost:3000/products -H 'Content-Type: application/json' -d '{\"name\":\"Test Product\",\"price\":99.99}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run security scanning",
          "type": "cli",
          "command": "cfn-lint serverless/template.yaml && safety check -r lambda-functions/products/requirements.txt",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate serverless best practices",
          "type": "prompt",
          "prompt": "Review the serverless application for best practices: cold start optimization, proper timeout settings, memory allocation, and error handling patterns.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement comprehensive backup and disaster recovery solution across cloud providers",
      "steps": [
        {
          "name": "Assess backup requirements",
          "type": "prompt",
          "prompt": "Document RPO (Recovery Point Objective) and RTO (Recovery Time Objective) requirements for different data types: databases, application state, and file storage.",
          "parameters": {},
          "files": [
            "backup-strategy/requirements.md"
          ]
        },
        {
          "name": "Design backup architecture",
          "type": "prompt",
          "prompt": "Create multi-tier backup strategy with local snapshots, cross-region replication, and cross-cloud archival. Include retention policies and cost optimization.",
          "parameters": {},
          "files": [
            "backup-strategy/architecture.md"
          ]
        },
        {
          "name": "Configure database backups",
          "type": "prompt",
          "prompt": "Implement automated database backup scripts for PostgreSQL and MongoDB with point-in-time recovery capability. Include transaction log archival and encrypted backups.",
          "parameters": {},
          "files": [
            "scripts/backup/database-backup.sh",
            "scripts/backup/restore-database.sh"
          ]
        },
        {
          "name": "Set up file system snapshots",
          "type": "prompt",
          "prompt": "Configure EBS snapshot policies and lifecycle management for application data volumes. Include cross-region copying and AMI creation for full system recovery.",
          "parameters": {},
          "files": [
            "terraform/backup/ebs-snapshots.tf",
            "terraform/backup/snapshot-lifecycle.tf"
          ]
        },
        {
          "name": "Implement S3 cross-region replication",
          "type": "prompt",
          "prompt": "Configure S3 bucket replication rules for critical data with versioning enabled. Set up lifecycle policies for moving old versions to Glacier for cost optimization.",
          "parameters": {},
          "files": [
            "terraform/backup/s3-replication.tf"
          ]
        },
        {
          "name": "Create backup orchestration",
          "type": "prompt",
          "prompt": "Develop backup orchestration using AWS Backup or custom Lambda functions. Include pre/post backup scripts, notification mechanisms, and backup validation.",
          "parameters": {},
          "files": [
            "backup-orchestration/backup-jobs.yaml",
            "lambda-functions/backup-validator/"
          ]
        },
        {
          "name": "Test backup scripts",
          "type": "cli",
          "command": "bash scripts/backup/database-backup.sh --dry-run --verbose",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate S3 replication configuration",
          "type": "cli",
          "command": "aws s3api get-bucket-replication --bucket primary-data-bucket",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check backup policies syntax",
          "type": "cli",
          "command": "aws backup get-backup-plan --backup-plan-id $(aws backup list-backup-plans --query 'BackupPlansList[0].BackupPlanId' --output text)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Simulate restore process",
          "type": "cli",
          "command": "bash scripts/backup/restore-database.sh --point-in-time '2024-01-15T10:00:00Z' --dry-run",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document recovery procedures",
          "type": "prompt",
          "prompt": "Create detailed runbooks for various disaster recovery scenarios including data corruption, region failure, and ransomware recovery with step-by-step procedures.",
          "parameters": {},
          "files": [
            "runbooks/disaster-recovery/"
          ]
        }
      ]
    },
    {
      "goal": "Design and implement event-driven architecture using message queues and event streaming",
      "steps": [
        {
          "name": "Design event-driven architecture",
          "type": "prompt",
          "prompt": "Create architecture design for event-driven microservices using SQS, SNS, and Kinesis. Define event schemas, routing patterns, and error handling strategies.",
          "parameters": {},
          "files": [
            "architecture/event-driven-design.md"
          ]
        },
        {
          "name": "Define event schemas",
          "type": "prompt",
          "prompt": "Create JSON Schema definitions for all event types in the system. Include versioning strategy, required fields, and validation rules for each event type.",
          "parameters": {},
          "files": [
            "event-schemas/",
            "event-schemas/order-events.json",
            "event-schemas/user-events.json"
          ]
        },
        {
          "name": "Configure SQS queues",
          "type": "prompt",
          "prompt": "Set up SQS queues with appropriate configurations: standard vs FIFO, message retention, dead letter queues, and visibility timeout settings.",
          "parameters": {},
          "files": [
            "terraform/messaging/sqs-queues.tf"
          ]
        },
        {
          "name": "Implement SNS topics",
          "type": "prompt",
          "prompt": "Create SNS topics for event fanout with subscription filters. Configure topic policies, delivery retry policies, and cross-account access where needed.",
          "parameters": {},
          "files": [
            "terraform/messaging/sns-topics.tf"
          ]
        },
        {
          "name": "Design message processors",
          "type": "prompt",
          "prompt": "Develop message processor applications with proper error handling, idempotency, and batch processing capabilities. Include circuit breaker patterns for downstream services.",
          "parameters": {},
          "files": [
            "message-processors/order-processor/",
            "message-processors/notification-processor/"
          ]
        },
        {
          "name": "Create event store",
          "type": "prompt",
          "prompt": "Implement event sourcing pattern with DynamoDB or EventStore for maintaining event history. Include event replay capabilities and snapshot mechanisms.",
          "parameters": {},
          "files": [
            "event-store/schema.sql",
            "event-store/event-store-client.py"
          ]
        },
        {
          "name": "Validate queue configurations",
          "type": "cli",
          "command": "aws sqs get-queue-attributes --queue-url https://sqs.us-east-2.amazonaws.com/123456789012/order-processing-queue --attribute-names All",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test message flow",
          "type": "cli",
          "command": "aws sqs send-message --queue-url https://sqs.us-east-2.amazonaws.com/123456789012/test-queue --message-body '{\"event\":\"test\",\"timestamp\":\"2024-01-15T10:00:00Z\"}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify SNS subscriptions",
          "type": "cli",
          "command": "aws sns list-subscriptions-by-topic --topic-arn arn:aws:sns:us-east-2:123456789012:order-events",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate event schemas",
          "type": "cli",
          "command": "ajv validate -s event-schemas/order-events.json -d test-events/sample-order.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document event flow",
          "type": "prompt",
          "prompt": "Create comprehensive documentation of event flows, including sequence diagrams, event catalog, and troubleshooting guides for common issues.",
          "parameters": {},
          "files": [
            "docs/event-driven-architecture.md"
          ]
        }
      ]
    },
    {
      "goal": "Build multi-region active-active architecture with global load balancing",
      "steps": [
        {
          "name": "Design global architecture",
          "type": "prompt",
          "prompt": "Create architecture design for active-active deployment across multiple AWS regions with Route 53 global load balancing, cross-region data replication, and regional failover.",
          "parameters": {},
          "files": [
            "architecture/multi-region-design.md"
          ]
        },
        {
          "name": "Configure Route 53 health checks",
          "type": "prompt",
          "prompt": "Set up Route 53 health checks for each region with custom health check endpoints. Configure latency-based routing and failover policies.",
          "parameters": {},
          "files": [
            "terraform/global/route53-health-checks.tf"
          ]
        },
        {
          "name": "Implement CloudFront distribution",
          "type": "prompt",
          "prompt": "Configure CloudFront distribution with multiple origins (one per region), origin failover groups, and custom error pages for regional outages.",
          "parameters": {},
          "files": [
            "terraform/global/cloudfront.tf"
          ]
        },
        {
          "name": "Set up cross-region VPC peering",
          "type": "prompt",
          "prompt": "Establish VPC peering connections between all regions for internal service communication. Configure route tables and security groups for cross-region traffic.",
          "parameters": {},
          "files": [
            "terraform/networking/vpc-peering.tf"
          ]
        },
        {
          "name": "Configure DynamoDB global tables",
          "type": "prompt",
          "prompt": "Set up DynamoDB global tables for session data and user preferences with multi-region replication. Configure auto-scaling and backup policies.",
          "parameters": {},
          "files": [
            "terraform/data/dynamodb-global-tables.tf"
          ]
        },
        {
          "name": "Implement Aurora global database",
          "type": "prompt",
          "prompt": "Configure Aurora global database with read replicas in each region and automated failover. Set up connection routing for read/write split.",
          "parameters": {},
          "files": [
            "terraform/data/aurora-global.tf"
          ]
        },
        {
          "name": "Create regional deployment scripts",
          "type": "prompt",
          "prompt": "Develop deployment scripts that can deploy application stack to any region with region-specific configurations and service endpoints.",
          "parameters": {},
          "files": [
            "scripts/deploy-region.sh",
            "config/regional-configs/"
          ]
        },
        {
          "name": "Test health check endpoints",
          "type": "cli",
          "command": "for region in us-east-1 eu-west-1 ap-southeast-1; do curl -I https://health-check-$region.example.com/health; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate Route 53 configuration",
          "type": "cli",
          "command": "aws route53 get-hosted-zone --id Z123456789 --query 'HostedZone.ResourceRecordSetCount'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check VPC peering status",
          "type": "cli",
          "command": "aws ec2 describe-vpc-peering-connections --filters 'Name=status-code,Values=active'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify global table replication",
          "type": "cli",
          "command": "aws dynamodb describe-table --table-name user-sessions --query 'Table.Replicas[*].RegionName'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document failover procedures",
          "type": "prompt",
          "prompt": "Create detailed runbooks for regional failover scenarios, including DNS updates, database promotion, and traffic rerouting procedures.",
          "parameters": {},
          "files": [
            "runbooks/regional-failover.md"
          ]
        }
      ]
    },
    {
      "goal": "Create comprehensive API management solution with rate limiting and authentication",
      "steps": [
        {
          "name": "Design API architecture",
          "type": "prompt",
          "prompt": "Design RESTful API architecture with versioning strategy, resource naming conventions, and OpenAPI 3.0 specification. Include pagination, filtering, and sorting patterns.",
          "parameters": {},
          "files": [
            "api-design/openapi-spec.yaml",
            "api-design/design-guidelines.md"
          ]
        },
        {
          "name": "Implement API Gateway configuration",
          "type": "prompt",
          "prompt": "Configure AWS API Gateway with request/response transformations, model validation, and custom authorizers. Set up stage variables for environment-specific settings.",
          "parameters": {},
          "files": [
            "terraform/api-gateway/main.tf",
            "terraform/api-gateway/models.tf"
          ]
        },
        {
          "name": "Create rate limiting rules",
          "type": "prompt",
          "prompt": "Implement rate limiting using API Gateway usage plans and API keys. Configure different tiers (basic, premium, enterprise) with appropriate request quotas and burst limits.",
          "parameters": {},
          "files": [
            "terraform/api-gateway/usage-plans.tf"
          ]
        },
        {
          "name": "Develop custom authorizer",
          "type": "prompt",
          "prompt": "Create Lambda-based custom authorizer for JWT token validation. Include token caching, role-based access control, and integration with identity providers.",
          "parameters": {},
          "files": [
            "lambda-functions/api-authorizer/handler.py",
            "lambda-functions/api-authorizer/jwt-validator.py"
          ]
        },
        {
          "name": "Configure WAF rules",
          "type": "prompt",
          "prompt": "Set up AWS WAF rules for API protection including SQL injection prevention, XSS protection, and IP-based rate limiting for DDoS protection.",
          "parameters": {},
          "files": [
            "terraform/security/waf-rules.tf"
          ]
        },
        {
          "name": "Implement request logging",
          "type": "prompt",
          "prompt": "Configure structured logging for all API requests including request/response bodies, latency metrics, and error tracking. Set up log analysis dashboards.",
          "parameters": {},
          "files": [
            "terraform/api-gateway/logging.tf",
            "cloudwatch/api-dashboards.json"
          ]
        },
        {
          "name": "Validate OpenAPI specification",
          "type": "cli",
          "command": "swagger-cli validate api-design/openapi-spec.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test API endpoints",
          "type": "cli",
          "command": "newman run api-tests/postman-collection.json --environment api-tests/dev-environment.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check rate limiting",
          "type": "cli",
          "command": "for i in {1..15}; do curl -X GET https://api.example.com/v1/products -H 'x-api-key: test-key'; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test custom authorizer",
          "type": "cli",
          "command": "curl -X GET https://api.example.com/v1/secure/data -H 'Authorization: Bearer ${JWT_TOKEN}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate WAF rules",
          "type": "cli",
          "command": "aws wafv2 get-web-acl --scope REGIONAL --id ${WAF_ACL_ID} --name api-protection",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate API documentation",
          "type": "prompt",
          "prompt": "Create comprehensive API documentation including authentication guides, code examples in multiple languages, and interactive API explorer.",
          "parameters": {},
          "files": [
            "docs/api-documentation/"
          ]
        }
      ]
    },
    {
      "goal": "Implement infrastructure compliance and policy as code using Open Policy Agent",
      "steps": [
        {
          "name": "Define compliance requirements",
          "type": "prompt",
          "prompt": "Document compliance requirements for infrastructure including CIS benchmarks, company security policies, and regulatory requirements (SOC2, HIPAA).",
          "parameters": {},
          "files": [
            "compliance/requirements.md"
          ]
        },
        {
          "name": "Create OPA policies",
          "type": "prompt",
          "prompt": "Write Open Policy Agent (OPA) policies in Rego for infrastructure compliance: EC2 instance types, security group rules, S3 bucket policies, and tagging requirements.",
          "parameters": {},
          "files": [
            "opa-policies/aws/",
            "opa-policies/kubernetes/",
            "opa-policies/terraform/"
          ]
        },
        {
          "name": "Implement Terraform Sentinel policies",
          "type": "prompt",
          "prompt": "Create Terraform Sentinel policies for pre-deployment validation including cost controls, approved AMIs, and network isolation requirements.",
          "parameters": {},
          "files": [
            "sentinel-policies/cost-control.sentinel",
            "sentinel-policies/security.sentinel"
          ]
        },
        {
          "name": "Configure Kubernetes admission controllers",
          "type": "prompt",
          "prompt": "Set up OPA as Kubernetes admission controller to enforce pod security policies, resource limits, and image scanning requirements.",
          "parameters": {},
          "files": [
            "k8s/opa-gatekeeper/constraints.yaml",
            "k8s/opa-gatekeeper/constraint-templates.yaml"
          ]
        },
        {
          "name": "Create policy testing framework",
          "type": "prompt",
          "prompt": "Develop comprehensive test suites for all policies including positive and negative test cases. Include automated testing in CI/CD pipeline.",
          "parameters": {},
          "files": [
            "policy-tests/",
            "policy-tests/test-runner.sh"
          ]
        },
        {
          "name": "Build policy documentation",
          "type": "prompt",
          "prompt": "Generate policy documentation with rationale, examples of compliant/non-compliant resources, and remediation guidance for policy violations.",
          "parameters": {},
          "files": [
            "docs/policy-catalog.md"
          ]
        },
        {
          "name": "Test OPA policies",
          "type": "cli",
          "command": "opa test opa-policies/ -v",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate Terraform with policies",
          "type": "cli",
          "command": "conftest verify --policy opa-policies/terraform/ terraform/modules/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test Kubernetes policies",
          "type": "cli",
          "command": "kubectl apply --dry-run=server -f k8s/test-resources/non-compliant-pod.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run policy coverage report",
          "type": "cli",
          "command": "opa test opa-policies/ --coverage --format json | jq '.coverage'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate Sentinel policies",
          "type": "cli",
          "command": "sentinel test sentinel-policies/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate compliance report",
          "type": "prompt",
          "prompt": "Create automated compliance reporting showing policy coverage, violation trends, and remediation status across all environments.",
          "parameters": {},
          "files": [
            "scripts/generate-compliance-report.py"
          ]
        }
      ]
    },
    {
      "goal": "Design and validate a Step Functions workflow for multi-stage data processing pipeline",
      "steps": [
        {
          "name": "Analyze existing Step Functions patterns",
          "type": "prompt",
          "prompt": "Examine the codebase for existing Step Functions state machines, identifying naming conventions, error handling patterns, and integration with Lambda functions.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create state machine definition for ETL pipeline",
          "type": "prompt",
          "prompt": "Design a Step Functions state machine that orchestrates: data extraction from S3, transformation via Lambda, validation checks, and loading to RDS. Include error handling with retry logic and failure notifications.",
          "parameters": {},
          "files": [
            "stepfunctions/data-pipeline/etl-workflow.json"
          ]
        },
        {
          "name": "Implement Lambda functions for each stage",
          "type": "prompt",
          "prompt": "Create Lambda functions for extract (read from S3), transform (data cleaning and enrichment), and load (write to RDS) stages. Include proper error handling and CloudWatch logging.",
          "parameters": {},
          "files": [
            "lambda/etl/extract.py",
            "lambda/etl/transform.py",
            "lambda/etl/load.py"
          ]
        },
        {
          "name": "Configure IAM roles and policies",
          "type": "prompt",
          "prompt": "Create IAM roles for Step Functions execution and Lambda functions with least-privilege access to S3, RDS, CloudWatch, and SNS for notifications.",
          "parameters": {},
          "files": [
            "terraform/modules/step-functions/iam.tf"
          ]
        },
        {
          "name": "Validate Step Functions definition",
          "type": "cli",
          "command": "aws stepfunctions validate-state-machine-definition --definition file://stepfunctions/data-pipeline/etl-workflow.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test Lambda functions locally",
          "type": "cli",
          "command": "python -m pytest lambda/etl/tests/ -v --cov=lambda/etl",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create and validate GitHub Actions workflow for multi-environment deployment with approval gates",
      "steps": [
        {
          "name": "Analyze existing GitHub Actions workflows",
          "type": "prompt",
          "prompt": "Review .github/workflows directory to understand current CI/CD patterns, reusable workflows, and secret management practices.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design multi-stage deployment workflow",
          "type": "prompt",
          "prompt": "Create a GitHub Actions workflow that builds application, runs tests, deploys to staging with automated tests, requires manual approval, then deploys to production. Include rollback capabilities.",
          "parameters": {},
          "files": [
            ".github/workflows/deploy-multi-env.yml"
          ]
        },
        {
          "name": "Implement reusable deployment action",
          "type": "prompt",
          "prompt": "Create a composite GitHub Action for standardized deployments that handles Docker builds, ECR pushes, ECS updates, and health checks. Make it reusable across environments.",
          "parameters": {},
          "files": [
            ".github/actions/deploy-ecs/action.yml"
          ]
        },
        {
          "name": "Configure environment-specific variables",
          "type": "prompt",
          "prompt": "Set up GitHub environments (dev, staging, prod) with specific secrets, variables, and protection rules. Include required reviewers for production deployments.",
          "parameters": {},
          "files": [
            ".github/environments/staging.yml",
            ".github/environments/production.yml"
          ]
        },
        {
          "name": "Validate workflow syntax",
          "type": "cli",
          "command": "actionlint .github/workflows/deploy-multi-env.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test workflow locally with act",
          "type": "cli",
          "command": "act -j build --dry-run -W .github/workflows/deploy-multi-env.yml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design Azure AKS cluster configuration with advanced networking and security features",
      "steps": [
        {
          "name": "Analyze Azure infrastructure patterns",
          "type": "prompt",
          "prompt": "Review existing Azure configurations in the codebase to understand resource naming, network architecture, and security baseline implementations.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create AKS cluster with Azure CNI",
          "type": "prompt",
          "prompt": "Design Terraform configuration for AKS cluster with Azure CNI networking, multiple node pools (system and user), autoscaling enabled, and Azure AD integration for RBAC.",
          "parameters": {},
          "files": [
            "terraform/modules/azure/aks/main.tf"
          ]
        },
        {
          "name": "Configure network security",
          "type": "prompt",
          "prompt": "Implement network policies, Azure Firewall integration, and private endpoints for AKS API server. Configure ingress controller with WAF-enabled Application Gateway.",
          "parameters": {},
          "files": [
            "terraform/modules/azure/aks/network-security.tf"
          ]
        },
        {
          "name": "Set up monitoring and diagnostics",
          "type": "prompt",
          "prompt": "Configure Azure Monitor for containers, enable diagnostic logs, and set up alerts for cluster health, node issues, and pod failures. Include Log Analytics workspace configuration.",
          "parameters": {},
          "files": [
            "terraform/modules/azure/aks/monitoring.tf"
          ]
        },
        {
          "name": "Validate Terraform configuration",
          "type": "cli",
          "command": "terraform validate",
          "parameters": {
            "working_directory": "terraform/modules/azure/aks/"
          },
          "files": []
        },
        {
          "name": "Run Azure security best practices check",
          "type": "cli",
          "command": "terrascan scan -i terraform -d terraform/modules/azure/aks/ --policy-type azure",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create comprehensive ECS Fargate task definitions with advanced configurations",
      "steps": [
        {
          "name": "Review ECS patterns in codebase",
          "type": "prompt",
          "prompt": "Examine existing ECS configurations to understand task definition patterns, container configurations, and secret management approaches.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design multi-container task definition",
          "type": "prompt",
          "prompt": "Create ECS task definition with main application container, sidecar for log forwarding to Datadog, and init container for configuration. Include health checks and resource limits.",
          "parameters": {},
          "files": [
            "ecs/task-definitions/api-service.json"
          ]
        },
        {
          "name": "Configure secrets and environment variables",
          "type": "prompt",
          "prompt": "Set up task definition to use AWS Secrets Manager for database credentials and Parameter Store for configuration values. Include proper IAM task execution role.",
          "parameters": {},
          "files": [
            "terraform/modules/ecs/secrets.tf"
          ]
        },
        {
          "name": "Implement service auto-scaling",
          "type": "prompt",
          "prompt": "Configure ECS service with target tracking auto-scaling based on CPU and memory metrics. Include scale-in/out policies and CloudWatch alarms.",
          "parameters": {},
          "files": [
            "terraform/modules/ecs/autoscaling.tf"
          ]
        },
        {
          "name": "Validate task definition",
          "type": "cli",
          "command": "ecs-cli compose --file ecs/task-definitions/api-service.json validate",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check container security",
          "type": "cli",
          "command": "docker scout cves --format json --only-severity critical,high api-service:latest",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design comprehensive Grafana monitoring dashboard with alerts for microservices architecture",
      "steps": [
        {
          "name": "Analyze monitoring requirements",
          "type": "prompt",
          "prompt": "Review existing monitoring setup and identify key metrics for microservices: response times, error rates, throughput, resource usage. Plan dashboard layout with service dependencies visualization.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create dashboard JSON configuration",
          "type": "prompt",
          "prompt": "Design Grafana dashboard JSON with panels for: service health overview, API endpoint performance, database query times, cache hit rates, and infrastructure metrics. Include variable templates for environment and service selection.",
          "parameters": {},
          "files": [
            "grafana/dashboards/microservices-overview.json"
          ]
        },
        {
          "name": "Configure Prometheus queries",
          "type": "prompt",
          "prompt": "Write PromQL queries for each panel: rate calculations for requests, percentile latencies, error ratios, and resource saturation. Include recording rules for expensive queries.",
          "parameters": {},
          "files": [
            "prometheus/rules/microservices-recording.yml"
          ]
        },
        {
          "name": "Design alert rules for SLO monitoring",
          "type": "prompt",
          "prompt": "Create Grafana alert rules for SLO violations: 99.9% availability, p95 latency < 200ms, error rate < 0.1%. Configure multi-window multi-burn-rate alerts to reduce noise.",
          "parameters": {},
          "files": [
            "grafana/alerts/slo-alerts.yaml"
          ]
        },
        {
          "name": "Validate dashboard JSON syntax",
          "type": "cli",
          "command": "jq . grafana/dashboards/microservices-overview.json > /dev/null && echo 'Dashboard JSON is valid'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test PromQL queries",
          "type": "cli",
          "command": "promtool query instant http://localhost:9090 'rate(http_requests_total[5m])'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create Jenkins pipeline for automated security scanning and compliance checks",
      "steps": [
        {
          "name": "Review Jenkins pipeline patterns",
          "type": "prompt",
          "prompt": "Examine existing Jenkinsfile patterns in the codebase to understand shared libraries usage, credential management, and notification strategies.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design multi-stage security pipeline",
          "type": "prompt",
          "prompt": "Create Jenkinsfile with stages: dependency scanning (OWASP), SAST (SonarQube), container scanning (Trivy), infrastructure scanning (Checkov), and compliance validation (Open Policy Agent).",
          "parameters": {},
          "files": [
            "jenkins/pipelines/security-scan.Jenkinsfile"
          ]
        },
        {
          "name": "Implement shared library for scanning",
          "type": "prompt",
          "prompt": "Create Jenkins shared library with reusable functions for each security tool integration. Include result parsing, threshold checking, and report generation.",
          "parameters": {},
          "files": [
            "jenkins/shared-libraries/vars/securityScan.groovy"
          ]
        },
        {
          "name": "Configure quality gates",
          "type": "prompt",
          "prompt": "Define quality gate policies: critical vulnerabilities = 0, high vulnerabilities < 5, code coverage > 80%, security hotspots < 3. Include override mechanism with approval.",
          "parameters": {},
          "files": [
            "jenkins/config/quality-gates.yaml"
          ]
        },
        {
          "name": "Validate Jenkinsfile syntax",
          "type": "cli",
          "command": "jenkins-cli declarative-linter < jenkins/pipelines/security-scan.Jenkinsfile",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test shared library locally",
          "type": "cli",
          "command": "groovy -cp jenkins/shared-libraries/src jenkins/shared-libraries/test/SecurityScanTest.groovy",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design Elasticsearch index lifecycle management for log retention and optimization",
      "steps": [
        {
          "name": "Analyze log volume and patterns",
          "type": "prompt",
          "prompt": "Review application logging patterns to estimate daily log volume, identify high-cardinality fields, and determine retention requirements for different log types (app, audit, security).",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create index templates with mappings",
          "type": "prompt",
          "prompt": "Design Elasticsearch index templates for application logs with optimized mappings: keyword fields for filtering, numeric fields for aggregations, and text fields with appropriate analyzers.",
          "parameters": {},
          "files": [
            "elasticsearch/templates/logs-app-template.json"
          ]
        },
        {
          "name": "Configure ILM policies",
          "type": "prompt",
          "prompt": "Create ILM policies with phases: hot (7 days, 50GB max), warm (30 days, read-only, force merge), cold (90 days, frozen tier), delete. Include rollover conditions and shard allocation.",
          "parameters": {},
          "files": [
            "elasticsearch/policies/logs-ilm-policy.json"
          ]
        },
        {
          "name": "Design data streams configuration",
          "type": "prompt",
          "prompt": "Set up data streams for time-series log data with automatic rollover. Configure backing indices, aliases, and ensure compatibility with existing log shippers.",
          "parameters": {},
          "files": [
            "elasticsearch/data-streams/logs-config.json"
          ]
        },
        {
          "name": "Validate index template",
          "type": "cli",
          "command": "curl -X POST 'localhost:9200/_index_template/_simulate/logs-app' -H 'Content-Type: application/json' -d @elasticsearch/templates/logs-app-template.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test ILM policy simulation",
          "type": "cli",
          "command": "curl -X POST 'localhost:9200/_ilm/policy/logs-policy/_explain' -H 'Content-Type: application/json'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create comprehensive chaos engineering scenarios using Litmus and validation framework",
      "steps": [
        {
          "name": "Analyze system architecture for chaos targets",
          "type": "prompt",
          "prompt": "Review microservices architecture to identify critical paths, dependencies, and failure points. Map out scenarios for pod failures, network delays, and resource exhaustion.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design Litmus chaos experiments",
          "type": "prompt",
          "prompt": "Create ChaosEngine manifests for: pod deletion, network latency injection, CPU/memory stress, disk fill, and DNS chaos. Include proper selectors and run duration.",
          "parameters": {},
          "files": [
            "chaos/experiments/pod-chaos.yaml",
            "chaos/experiments/network-chaos.yaml"
          ]
        },
        {
          "name": "Implement validation probes",
          "type": "prompt",
          "prompt": "Create custom Litmus probes to validate system behavior during chaos: API availability checks, data consistency verification, and performance degradation thresholds.",
          "parameters": {},
          "files": [
            "chaos/probes/http-probe.yaml",
            "chaos/probes/cmd-probe.yaml"
          ]
        },
        {
          "name": "Configure chaos schedules",
          "type": "prompt",
          "prompt": "Set up ChaosSchedule resources for recurring experiments: daily pod failures in staging, weekly network tests, monthly disaster recovery drills.",
          "parameters": {},
          "files": [
            "chaos/schedules/recurring-chaos.yaml"
          ]
        },
        {
          "name": "Validate chaos experiment syntax",
          "type": "cli",
          "command": "kubectl --dry-run=client apply -f chaos/experiments/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check probe configurations",
          "type": "cli",
          "command": "yq eval '.spec.experiments[].spec.probe' chaos/experiments/*.yaml | grep -E 'name|type|mode'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design Consul service mesh configuration with health checking and traffic management",
      "steps": [
        {
          "name": "Analyze service architecture for mesh implementation",
          "type": "prompt",
          "prompt": "Review microservices architecture to identify service dependencies, communication patterns, and security requirements for Consul service mesh implementation.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create Consul service definitions",
          "type": "prompt",
          "prompt": "Design Consul service definitions for each microservice including: service name, tags, port configurations, health check endpoints, and Connect sidecar proxy settings.",
          "parameters": {},
          "files": [
            "consul/services/auth-api.json",
            "consul/services/payment-api.json",
            "consul/services/notification-api.json"
          ]
        },
        {
          "name": "Configure service intentions",
          "type": "prompt",
          "prompt": "Define Consul Connect intentions to control service-to-service communication. Set up allow/deny rules based on zero-trust principles with explicit permissions only where needed.",
          "parameters": {},
          "files": [
            "consul/intentions/service-intentions.hcl"
          ]
        },
        {
          "name": "Implement health check scripts",
          "type": "prompt",
          "prompt": "Create comprehensive health check scripts that validate: database connectivity, external API availability, cache responsiveness, and internal service endpoints. Include timeout and retry logic.",
          "parameters": {},
          "files": [
            "consul/health-checks/api-health.sh",
            "consul/health-checks/dependency-check.sh"
          ]
        },
        {
          "name": "Validate Consul configuration",
          "type": "cli",
          "command": "consul validate consul/services/*.json && consul validate consul/intentions/*.hcl",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test health check scripts",
          "type": "cli",
          "command": "bash -n consul/health-checks/*.sh && shellcheck consul/health-checks/*.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create GCP infrastructure with Terraform for multi-region deployment",
      "steps": [
        {
          "name": "Design GCP project structure",
          "type": "prompt",
          "prompt": "Plan GCP resource organization with separate projects for dev/staging/prod, shared VPC architecture, and proper IAM boundaries. Include folder hierarchy and billing account structure.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create GKE cluster configuration",
          "type": "prompt",
          "prompt": "Design Terraform configuration for regional GKE cluster with: 3 zones, auto-scaling node pools, workload identity, binary authorization, and private cluster settings.",
          "parameters": {},
          "files": [
            "terraform/gcp/gke-cluster.tf"
          ]
        },
        {
          "name": "Configure Cloud SQL with high availability",
          "type": "prompt",
          "prompt": "Set up Cloud SQL PostgreSQL instance with: automatic failover, point-in-time recovery, private IP only, automated backups, and read replicas in different regions.",
          "parameters": {},
          "files": [
            "terraform/gcp/cloud-sql.tf"
          ]
        },
        {
          "name": "Implement Cloud Load Balancer",
          "type": "prompt",
          "prompt": "Configure global HTTP(S) load balancer with: CDN enablement, SSL certificates, backend services across regions, health checks, and Cloud Armor security policies.",
          "parameters": {},
          "files": [
            "terraform/gcp/load-balancer.tf"
          ]
        },
        {
          "name": "Set up monitoring and logging",
          "type": "prompt",
          "prompt": "Configure Cloud Monitoring workspaces, log sinks to BigQuery for analysis, custom metrics, and alert policies for SLO tracking. Include log-based metrics for security events.",
          "parameters": {},
          "files": [
            "terraform/gcp/monitoring.tf"
          ]
        },
        {
          "name": "Validate Terraform configuration",
          "type": "cli",
          "command": "terraform init -backend=false && terraform validate",
          "parameters": {
            "working_directory": "terraform/gcp/"
          },
          "files": []
        },
        {
          "name": "Run security scanner",
          "type": "cli",
          "command": "gcloud beta terraform vet terraform/gcp/ --policy-library=policy-library/",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design comprehensive data pipeline with Apache Airflow and cloud storage",
      "steps": [
        {
          "name": "Analyze data flow requirements",
          "type": "prompt",
          "prompt": "Map out data sources, transformation requirements, and destinations. Identify data volume, velocity, and quality requirements for ETL/ELT pipeline design.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create Airflow DAG for data pipeline",
          "type": "prompt",
          "prompt": "Design Airflow DAG with tasks for: data extraction from multiple sources (APIs, databases, files), validation, transformation using Spark, and loading to data warehouse. Include error handling and retry logic.",
          "parameters": {},
          "files": [
            "airflow/dags/data_pipeline_dag.py"
          ]
        },
        {
          "name": "Implement custom operators",
          "type": "prompt",
          "prompt": "Create custom Airflow operators for: data quality checks, schema validation, SLA monitoring, and notification handling. Include unit tests for each operator.",
          "parameters": {},
          "files": [
            "airflow/plugins/operators/data_quality_operator.py",
            "airflow/plugins/operators/schema_validator_operator.py"
          ]
        },
        {
          "name": "Configure dynamic task generation",
          "type": "prompt",
          "prompt": "Implement dynamic task generation based on configuration files. Create tasks dynamically for each data source with parallel processing and proper dependencies.",
          "parameters": {},
          "files": [
            "airflow/dags/dynamic_pipeline_factory.py"
          ]
        },
        {
          "name": "Validate DAG syntax",
          "type": "cli",
          "command": "python -m py_compile airflow/dags/*.py && airflow dags list --subdir airflow/dags/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test custom operators",
          "type": "cli",
          "command": "pytest airflow/tests/operators/ -v --cov=airflow.plugins.operators",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create comprehensive API gateway configuration with rate limiting and authentication",
      "steps": [
        {
          "name": "Design API gateway architecture",
          "type": "prompt",
          "prompt": "Plan API gateway setup with Kong or AWS API Gateway including: route definitions, authentication methods (OAuth2, API keys, JWT), and backend service mappings.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure rate limiting policies",
          "type": "prompt",
          "prompt": "Create rate limiting configurations with: tiered limits by API key, global rate limits, per-endpoint limits, and burst handling. Include Redis backend for distributed rate limiting.",
          "parameters": {},
          "files": [
            "api-gateway/policies/rate-limiting.yaml"
          ]
        },
        {
          "name": "Implement request/response transformations",
          "type": "prompt",
          "prompt": "Design request/response transformation rules for: header manipulation, body transformation, protocol translation (REST to GraphQL), and response caching strategies.",
          "parameters": {},
          "files": [
            "api-gateway/transformations/request-transform.lua",
            "api-gateway/transformations/response-transform.lua"
          ]
        },
        {
          "name": "Set up API versioning strategy",
          "type": "prompt",
          "prompt": "Implement API versioning with: URL path versioning, header-based routing, canary deployments for new versions, and deprecation policies with sunset headers.",
          "parameters": {},
          "files": [
            "api-gateway/routing/version-routes.yaml"
          ]
        },
        {
          "name": "Validate gateway configuration",
          "type": "cli",
          "command": "kong config parse api-gateway/kong.yaml && kong config db_export --yes",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test rate limiting rules",
          "type": "cli",
          "command": "ab -n 1000 -c 10 -H 'X-API-Key: test-key' http://localhost:8000/api/v1/test | grep 'Failed requests'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design comprehensive backup and disaster recovery strategy for multi-cloud deployment",
      "steps": [
        {
          "name": "Analyze critical data and recovery requirements",
          "type": "prompt",
          "prompt": "Map all critical data stores, databases, and configuration files. Define RPO (Recovery Point Objective) and RTO (Recovery Time Objective) for each tier of service.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create backup automation scripts",
          "type": "prompt",
          "prompt": "Design scripts for automated backups of: PostgreSQL databases with WAL archiving, Redis snapshots, Elasticsearch indices, and application state. Include encryption and compression.",
          "parameters": {},
          "files": [
            "backup/scripts/database-backup.sh",
            "backup/scripts/redis-backup.sh",
            "backup/scripts/elasticsearch-backup.sh"
          ]
        },
        {
          "name": "Configure cross-region replication",
          "type": "prompt",
          "prompt": "Set up Terraform configuration for S3 cross-region replication, RDS read replicas, and DynamoDB global tables. Include lifecycle policies and cost optimization.",
          "parameters": {},
          "files": [
            "terraform/disaster-recovery/replication.tf"
          ]
        },
        {
          "name": "Design failover orchestration",
          "type": "prompt",
          "prompt": "Create runbooks and automation scripts for failover scenarios: DNS updates, database promotion, cache warming, and service discovery updates. Include rollback procedures.",
          "parameters": {},
          "files": [
            "disaster-recovery/runbooks/regional-failover.md",
            "disaster-recovery/scripts/failover-orchestrator.py"
          ]
        },
        {
          "name": "Validate backup scripts",
          "type": "cli",
          "command": "shellcheck backup/scripts/*.sh && python -m py_compile disaster-recovery/scripts/*.py",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test recovery procedures",
          "type": "cli",
          "command": "bash backup/scripts/database-backup.sh --test-mode --verify && echo 'Backup validation passed'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create advanced Kubernetes network policies and service mesh security",
      "steps": [
        {
          "name": "Analyze service communication patterns",
          "type": "prompt",
          "prompt": "Review microservices architecture to map all inter-service communications, external dependencies, and ingress/egress requirements for zero-trust network design.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Design network policies",
          "type": "prompt",
          "prompt": "Create Kubernetes NetworkPolicies implementing: default deny-all, service-specific ingress rules, namespace isolation, and egress restrictions to external services only.",
          "parameters": {},
          "files": [
            "k8s/network-policies/default-deny.yaml",
            "k8s/network-policies/payment-api-policy.yaml",
            "k8s/network-policies/database-access.yaml"
          ]
        },
        {
          "name": "Implement mTLS with cert-manager",
          "type": "prompt",
          "prompt": "Configure cert-manager for automatic certificate rotation, create Istio DestinationRules for mTLS enforcement, and set up certificate monitoring with expiry alerts.",
          "parameters": {},
          "files": [
            "k8s/cert-manager/cluster-issuer.yaml",
            "istio/security/mtls-strict.yaml"
          ]
        },
        {
          "name": "Create security monitoring",
          "type": "prompt",
          "prompt": "Design Falco rules for runtime security monitoring: detect privilege escalation, unexpected network connections, and file system changes. Include alert routing.",
          "parameters": {},
          "files": [
            "falco/rules/custom-rules.yaml",
            "falco/config/falco.yaml"
          ]
        },
        {
          "name": "Validate network policies",
          "type": "cli",
          "command": "kubectl apply --dry-run=server -f k8s/network-policies/ && echo 'Network policies validated'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test policy enforcement",
          "type": "cli",
          "command": "kubectl run test-pod --image=busybox --rm -it --restart=Never -- wget -qO- --timeout=2 http://payment-api:8080 2>&1 | grep -E 'timeout|refused'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Design comprehensive compliance automation for SOC2 and GDPR requirements",
      "steps": [
        {
          "name": "Map compliance requirements to controls",
          "type": "prompt",
          "prompt": "Create mapping of SOC2 Trust Service Criteria and GDPR articles to technical controls: encryption, access logs, data retention, and privacy by design implementations.",
          "parameters": {},
          "files": [
            "compliance/mappings/soc2-controls.yaml",
            "compliance/mappings/gdpr-requirements.yaml"
          ]
        },
        {
          "name": "Implement audit logging pipeline",
          "type": "prompt",
          "prompt": "Design centralized audit logging with: structured logs from all services, immutable storage, automated PII detection and redaction, and retention policies per data classification.",
          "parameters": {},
          "files": [
            "logging/audit/fluentd-config.yaml",
            "logging/audit/pii-detection-rules.json"
          ]
        },
        {
          "name": "Create compliance validation tests",
          "type": "prompt",
          "prompt": "Develop InSpec profiles to validate: encryption at rest and in transit, access control policies, backup procedures, and data retention compliance.",
          "parameters": {},
          "files": [
            "compliance/inspec/controls/encryption.rb",
            "compliance/inspec/controls/access-control.rb",
            "compliance/inspec/controls/data-retention.rb"
          ]
        },
        {
          "name": "Generate compliance reports",
          "type": "prompt",
          "prompt": "Create automated reporting templates for: monthly compliance status, audit evidence collection, risk assessment matrices, and remediation tracking.",
          "parameters": {},
          "files": [
            "compliance/reports/templates/monthly-soc2.md",
            "compliance/reports/scripts/evidence-collector.py"
          ]
        },
        {
          "name": "Run compliance checks",
          "type": "cli",
          "command": "inspec exec compliance/inspec --reporter json:compliance-results.json cli",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate audit log integrity",
          "type": "cli",
          "command": "sha256sum /var/log/audit/*.log > audit-hashes.txt && gpg --detach-sign audit-hashes.txt",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create edge computing deployment strategy with IoT device management",
      "steps": [
        {
          "name": "Design edge architecture",
          "type": "prompt",
          "prompt": "Plan edge computing architecture with: K3s clusters at edge locations, MQTT brokers for IoT communication, local data processing, and cloud synchronization strategies.",
          "parameters": {},
          "files": [
            "edge/architecture/edge-deployment.md",
            "edge/k3s/cluster-config.yaml"
          ]
        },
        {
          "name": "Configure IoT device provisioning",
          "type": "prompt",
          "prompt": "Create device provisioning workflow using AWS IoT Core: certificate generation, thing registration, policy attachment, and OTA update capabilities.",
          "parameters": {},
          "files": [
            "iot/provisioning/device-template.json",
            "iot/scripts/bulk-provisioning.py"
          ]
        },
        {
          "name": "Implement edge data pipeline",
          "type": "prompt",
          "prompt": "Design local data processing pipeline with: Apache NiFi for data flow, TimescaleDB for time-series storage, and selective sync to cloud based on bandwidth availability.",
          "parameters": {},
          "files": [
            "edge/nifi/data-flow-template.xml",
            "edge/timescale/schema.sql"
          ]
        },
        {
          "name": "Create offline operation mode",
          "type": "prompt",
          "prompt": "Implement resilient edge operations for network outages: local caching, store-and-forward messaging, conflict resolution for data sync, and autonomous decision making.",
          "parameters": {},
          "files": [
            "edge/resilience/offline-handler.py",
            "edge/resilience/sync-queue.yaml"
          ]
        },
        {
          "name": "Validate edge deployment",
          "type": "cli",
          "command": "k3s kubectl apply --dry-run=client -f edge/k3s/ && echo 'Edge configs valid'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test IoT provisioning",
          "type": "cli",
          "command": "python iot/scripts/bulk-provisioning.py --test --devices 10 --dry-run",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create CircleCI pipeline for Node.js microservice",
      "steps": [
        {
          "name": "Design CircleCI config",
          "type": "prompt",
          "prompt": "Create CircleCI configuration for Node.js app with: test job using Jest, build Docker image, and push to registry. Use orbs for simplification.",
          "parameters": {},
          "files": [
            ".circleci/config.yml"
          ]
        },
        {
          "name": "Validate CircleCI config",
          "type": "cli",
          "command": "circleci config validate .circleci/config.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test locally",
          "type": "cli",
          "command": "circleci local execute --job test",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up basic Kubernetes service with health checks",
      "steps": [
        {
          "name": "Create service manifest",
          "type": "prompt",
          "prompt": "Write Kubernetes Service and Deployment for a web API with readiness and liveness probes on /health endpoint.",
          "parameters": {},
          "files": [
            "k8s/api-service.yaml"
          ]
        },
        {
          "name": "Deploy to cluster",
          "type": "cli",
          "command": "kubectl apply -f k8s/api-service.yaml --dry-run=client",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure AWS S3 bucket with lifecycle policies",
      "steps": [
        {
          "name": "Create S3 bucket config",
          "type": "prompt",
          "prompt": "Design Terraform for S3 bucket with: versioning enabled, lifecycle rule to move to Glacier after 90 days, and server-side encryption.",
          "parameters": {},
          "files": [
            "terraform/s3-archive.tf"
          ]
        },
        {
          "name": "Validate configuration",
          "type": "cli",
          "command": "terraform fmt terraform/s3-archive.tf && terraform validate",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate plan",
          "type": "cli",
          "command": "terraform plan -out=s3.tfplan",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create Docker multi-stage build for Go application",
      "steps": [
        {
          "name": "Design Dockerfile",
          "type": "prompt",
          "prompt": "Create multi-stage Dockerfile for Go app: builder stage with dependencies, final stage with minimal Alpine image and non-root user.",
          "parameters": {},
          "files": [
            "Dockerfile"
          ]
        },
        {
          "name": "Build and scan image",
          "type": "cli",
          "command": "docker build -t go-app:latest . && docker scout cves go-app:latest",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up Prometheus monitoring for Kubernetes pod",
      "steps": [
        {
          "name": "Create ServiceMonitor",
          "type": "prompt",
          "prompt": "Write Prometheus ServiceMonitor to scrape metrics from pods with label app=api on port 8080/metrics endpoint.",
          "parameters": {},
          "files": [
            "monitoring/service-monitor.yaml"
          ]
        },
        {
          "name": "Configure alerts",
          "type": "prompt",
          "prompt": "Create PrometheusRule for high error rate alert when rate of 5xx errors exceeds 1% over 5 minutes.",
          "parameters": {},
          "files": [
            "monitoring/alerts.yaml"
          ]
        },
        {
          "name": "Apply monitoring config",
          "type": "cli",
          "command": "kubectl apply -f monitoring/ --dry-run=client",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure GitHub Actions for security scanning",
      "steps": [
        {
          "name": "Create security workflow",
          "type": "prompt",
          "prompt": "Design GitHub Actions workflow that runs on PR: dependency scanning with Dependabot, SAST with CodeQL, and secret scanning.",
          "parameters": {},
          "files": [
            ".github/workflows/security.yml"
          ]
        },
        {
          "name": "Test workflow syntax",
          "type": "cli",
          "command": "actionlint .github/workflows/security.yml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure nginx reverse proxy",
      "steps": [
        {
          "name": "Create nginx config",
          "type": "prompt",
          "prompt": "Write nginx configuration for reverse proxy to backend services with health checks and load balancing.",
          "parameters": {},
          "files": [
            "nginx/nginx.conf"
          ]
        },
        {
          "name": "Test configuration",
          "type": "cli",
          "command": "nginx -t -c nginx/nginx.conf",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up PostgreSQL backup script",
      "steps": [
        {
          "name": "Create backup script",
          "type": "prompt",
          "prompt": "Write bash script to backup PostgreSQL database with compression and S3 upload. Include retention policy.",
          "parameters": {},
          "files": [
            "scripts/pg-backup.sh"
          ]
        },
        {
          "name": "Test backup",
          "type": "cli",
          "command": "bash scripts/pg-backup.sh --dry-run",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create Helm chart for microservice",
      "steps": [
        {
          "name": "Initialize Helm chart",
          "type": "cli",
          "command": "helm create microservice-chart",
          "parameters": {},
          "files": []
        },
        {
          "name": "Customize values",
          "type": "prompt",
          "prompt": "Update Helm chart values.yaml with resource limits, health checks, and ingress configuration.",
          "parameters": {},
          "files": [
            "microservice-chart/values.yaml"
          ]
        },
        {
          "name": "Lint chart",
          "type": "cli",
          "command": "helm lint microservice-chart/",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure log rotation with logrotate",
      "steps": [
        {
          "name": "Create logrotate config",
          "type": "prompt",
          "prompt": "Write logrotate configuration for application logs: daily rotation, 7 days retention, compression.",
          "parameters": {},
          "files": [
            "/etc/logrotate.d/app-logs"
          ]
        },
        {
          "name": "Test rotation",
          "type": "cli",
          "command": "logrotate -d /etc/logrotate.d/app-logs",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update the Vue.js client application to add a new dashboard feature",
      "steps": [
        {
          "name": "Analyze existing Vue components",
          "type": "prompt",
          "prompt": "Review the Code/client/src/components directory to understand the existing component structure and identify where to add the new dashboard component.",
          "parameters": {},
          "files": [
            "Code/client/src/components/"
          ]
        },
        {
          "name": "Create dashboard component",
          "type": "prompt",
          "prompt": "Create a new Vue component for the dashboard that displays metrics from the backend API. Use the existing component patterns found in the codebase.",
          "parameters": {},
          "files": [
            "Code/client/src/components/Dashboard.vue"
          ]
        },
        {
          "name": "Update router configuration",
          "type": "prompt",
          "prompt": "Add the new dashboard route to the Vue router configuration to make it accessible from the navigation.",
          "parameters": {},
          "files": [
            "Code/client/src/router/index.js"
          ]
        },
        {
          "name": "Run client tests",
          "type": "cli",
          "command": "cd Code/client && npm test",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Terraform to deploy the application to AWS EKS",
      "steps": [
        {
          "name": "Review EKS module",
          "type": "prompt",
          "prompt": "Examine terraform/modules/aws/eks to understand the existing EKS configuration and identify necessary updates for the application deployment.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/eks/"
          ]
        },
        {
          "name": "Update EKS node group",
          "type": "prompt",
          "prompt": "Modify the EKS node group configuration to support the application's resource requirements based on the k8s manifests.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/eks/main.tf"
          ]
        },
        {
          "name": "Validate Terraform changes",
          "type": "cli",
          "command": "cd terraform/modules/aws/eks && terraform fmt && terraform validate",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up Consul service mesh for the Node.js backend",
      "steps": [
        {
          "name": "Review Consul configuration",
          "type": "prompt",
          "prompt": "Examine terraform/modules/consul/k8s-client to understand how Consul is configured for Kubernetes.",
          "parameters": {},
          "files": [
            "terraform/modules/consul/k8s-client/main.tf"
          ]
        },
        {
          "name": "Create service definition",
          "type": "prompt",
          "prompt": "Create a Consul service definition for the Node.js backend server based on the existing patterns in the codebase.",
          "parameters": {},
          "files": [
            "k8s/consul/backend-service.yaml"
          ]
        },
        {
          "name": "Update backend deployment",
          "type": "prompt",
          "prompt": "Modify the backend Kubernetes deployment to include Consul Connect sidecar annotations.",
          "parameters": {},
          "files": [
            "k8s/deployments/backend-deployment.yaml"
          ]
        },
        {
          "name": "Validate Consul configuration",
          "type": "cli",
          "command": "consul validate k8s/consul/backend-service.yaml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create monitoring dashboards for the application using existing Grafana setup",
      "steps": [
        {
          "name": "Review monitoring configuration",
          "type": "prompt",
          "prompt": "Examine monitoring/grafana and monitoring/prometheus directories to understand the existing monitoring setup.",
          "parameters": {},
          "files": [
            "monitoring/grafana/",
            "monitoring/prometheus/"
          ]
        },
        {
          "name": "Create application dashboard",
          "type": "prompt",
          "prompt": "Design a Grafana dashboard JSON for the Node.js application metrics using the existing dashboard patterns in monitoring/grafana/dashboards.",
          "parameters": {},
          "files": [
            "monitoring/grafana/dashboards/application-metrics.json"
          ]
        },
        {
          "name": "Add Prometheus scrape config",
          "type": "prompt",
          "prompt": "Update the Prometheus configuration to scrape metrics from the Node.js backend service.",
          "parameters": {},
          "files": [
            "monitoring/prometheus/config/prometheus.yml"
          ]
        },
        {
          "name": "Validate monitoring configuration",
          "type": "cli",
          "command": "promtool check config monitoring/prometheus/config/prometheus.yml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update CircleCI pipeline for the full-stack application",
      "steps": [
        {
          "name": "Review existing CircleCI config",
          "type": "prompt",
          "prompt": "Examine .circleci/config.yml to understand the current CI pipeline structure.",
          "parameters": {},
          "files": [
            ".circleci/config.yml"
          ]
        },
        {
          "name": "Add frontend build job",
          "type": "prompt",
          "prompt": "Add a new job to the CircleCI pipeline to build and test the Vue.js frontend application.",
          "parameters": {},
          "files": [
            ".circleci/config.yml"
          ]
        },
        {
          "name": "Add backend build job",
          "type": "prompt",
          "prompt": "Add a job to build the Node.js backend, run tests, and create a Docker image.",
          "parameters": {},
          "files": [
            ".circleci/config.yml"
          ]
        },
        {
          "name": "Validate CircleCI configuration",
          "type": "cli",
          "command": "circleci config validate .circleci/config.yml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Add health check endpoint to Node.js server",
      "steps": [
        {
          "name": "Add health route",
          "type": "prompt",
          "prompt": "Add a /health endpoint to Code/server/routes/index.js that returns server status and version.",
          "parameters": {},
          "files": [
            "Code/server/routes/index.js"
          ]
        },
        {
          "name": "Test endpoint",
          "type": "cli",
          "command": "cd Code/server && npm start & sleep 3 && curl localhost:3000/health",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update Makefile to add new deployment target",
      "steps": [
        {
          "name": "Add staging target",
          "type": "prompt",
          "prompt": "Add a 'deploy-staging' target to the Makefile that deploys to the staging environment.",
          "parameters": {},
          "files": [
            "Makefile"
          ]
        },
        {
          "name": "Test make target",
          "type": "cli",
          "command": "make deploy-staging --dry-run",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create Kubernetes ConfigMap for app configuration",
      "steps": [
        {
          "name": "Create ConfigMap",
          "type": "prompt",
          "prompt": "Create a ConfigMap in k8s/configmaps/ for application environment variables based on .env.template.",
          "parameters": {},
          "files": [
            "k8s/configmaps/app-config.yaml",
            ".env.template"
          ]
        },
        {
          "name": "Apply ConfigMap",
          "type": "cli",
          "command": "kubectl apply -f k8s/configmaps/app-config.yaml --dry-run=client",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Add authentication middleware to the Node.js backend API",
      "steps": [
        {
          "name": "Review existing Express app structure",
          "type": "prompt",
          "prompt": "Examine the Express application in app.js to understand the current middleware setup and API endpoints.",
          "parameters": {},
          "files": [
            "complex-demo/Code/server/src/app.js"
          ]
        },
        {
          "name": "Create JWT authentication middleware",
          "type": "prompt",
          "prompt": "Create a new authentication middleware file that validates JWT tokens for the API endpoints, following the patterns in app.js.",
          "parameters": {},
          "files": [
            "complex-demo/Code/server/src/middleware/auth.js"
          ]
        },
        {
          "name": "Update app.js to use authentication",
          "type": "prompt",
          "prompt": "Modify app.js to import and use the authentication middleware for the /api/getAllProducts endpoint.",
          "parameters": {},
          "files": [
            "complex-demo/Code/server/src/app.js"
          ]
        },
        {
          "name": "Test authentication",
          "type": "cli",
          "command": "cd complex-demo/Code/server && npm test -- --grep 'authentication'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update Kubernetes deployment to use ConfigMap for environment variables",
      "steps": [
        {
          "name": "Review current deployment configuration",
          "type": "prompt",
          "prompt": "Examine the backend deployment.yaml to understand the current environment variable configuration.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/deployment.yaml"
          ]
        },
        {
          "name": "Create ConfigMap for backend configuration",
          "type": "prompt",
          "prompt": "Create a ConfigMap that contains the environment variables currently hardcoded in the deployment.yaml file.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/configmap.yaml"
          ]
        },
        {
          "name": "Update deployment to use ConfigMap",
          "type": "prompt",
          "prompt": "Modify deployment.yaml to use envFrom to load environment variables from the ConfigMap instead of hardcoded values.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/deployment.yaml"
          ]
        },
        {
          "name": "Validate Kubernetes manifests",
          "type": "cli",
          "command": "kubectl apply --dry-run=client -f complex-demo/k8s/envs/dev/backend/",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Terraform AWS VPC module for production environment",
      "steps": [
        {
          "name": "Review VPC module structure",
          "type": "prompt",
          "prompt": "Examine the AWS VPC module to understand the configurable variables and outputs.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/modules/aws/vpc/main.tf",
            "complex-demo/terraform/modules/aws/vpc/variables.tf"
          ]
        },
        {
          "name": "Create production VPC configuration",
          "type": "prompt",
          "prompt": "Create a production environment Terraform configuration that uses the VPC module with production-appropriate CIDR blocks and subnet configurations.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/envs/prod/us-west-2/vpc.tf"
          ]
        },
        {
          "name": "Configure VPC outputs",
          "type": "prompt",
          "prompt": "Update the outputs.tf file to expose necessary VPC information for other modules.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/modules/aws/vpc/outputs.tf"
          ]
        },
        {
          "name": "Validate Terraform configuration",
          "type": "cli",
          "command": "cd complex-demo/terraform/modules/aws/vpc && terraform fmt && terraform validate",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Add new API endpoint to Vue.js REST service client",
      "steps": [
        {
          "name": "Review REST services implementation",
          "type": "prompt",
          "prompt": "Examine RestServices.js to understand how API calls are structured in the Vue.js application.",
          "parameters": {},
          "files": [
            "complex-demo/Code/client/src/services/RestServices.js"
          ]
        },
        {
          "name": "Add new service method",
          "type": "prompt",
          "prompt": "Add a new method to RestServices.js for fetching user profile data from a new /api/getUserProfile endpoint.",
          "parameters": {},
          "files": [
            "complex-demo/Code/client/src/services/RestServices.js"
          ]
        },
        {
          "name": "Update Vue component to use new service",
          "type": "prompt",
          "prompt": "Modify the Main.vue component to call the new getUserProfile service method and display the results.",
          "parameters": {},
          "files": [
            "complex-demo/Code/client/src/components/Main.vue"
          ]
        },
        {
          "name": "Test the frontend changes",
          "type": "cli",
          "command": "cd complex-demo/Code/client && npm run test:unit",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Prometheus monitoring for the backend service",
      "steps": [
        {
          "name": "Review Prometheus configuration",
          "type": "prompt",
          "prompt": "Examine the existing Prometheus configuration to understand the scrape configs and service discovery setup.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/aws/observability/prometheus.yaml"
          ]
        },
        {
          "name": "Add backend service monitoring",
          "type": "prompt",
          "prompt": "Update prometheus.yaml to add a new scrape config for the backend service using Kubernetes service discovery.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/aws/observability/prometheus.yaml"
          ]
        },
        {
          "name": "Create ServiceMonitor for backend",
          "type": "prompt",
          "prompt": "Create a ServiceMonitor resource that tells Prometheus how to scrape metrics from the backend service.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/servicemonitor.yaml"
          ]
        },
        {
          "name": "Apply monitoring configuration",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/aws/observability/prometheus.yaml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Add CORS configuration to Express app",
      "steps": [
        {
          "name": "Update CORS settings",
          "type": "prompt",
          "prompt": "Modify the CORS configuration in app.js to allow specific origins instead of all origins.",
          "parameters": {},
          "files": [
            "complex-demo/Code/server/src/app.js"
          ]
        },
        {
          "name": "Test CORS",
          "type": "cli",
          "command": "cd complex-demo/Code/server && npm start & sleep 3 && curl -H 'Origin: http://localhost:8080' -I http://localhost:3001/status",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update Vue router for new route",
      "steps": [
        {
          "name": "Add dashboard route",
          "type": "prompt",
          "prompt": "Add a new route for /dashboard in the Vue router configuration that loads a Dashboard component.",
          "parameters": {},
          "files": [
            "complex-demo/Code/client/src/router/index.js"
          ]
        },
        {
          "name": "Verify routing",
          "type": "cli",
          "command": "cd complex-demo/Code/client && npm run serve",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure S3 bucket in Terraform",
      "steps": [
        {
          "name": "Update S3 module",
          "type": "prompt",
          "prompt": "Add versioning and lifecycle rules to the S3 bucket configuration in the Terraform module.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/modules/aws/s3/main.tf"
          ]
        },
        {
          "name": "Validate changes",
          "type": "cli",
          "command": "cd complex-demo/terraform/modules/aws/s3 && terraform fmt && terraform validate",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update backend deployment resources",
      "steps": [
        {
          "name": "Increase resource limits",
          "type": "prompt",
          "prompt": "Update the backend deployment to increase memory limits to 1Gi and CPU limits to 1000m for production workloads.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/deployment.yaml"
          ]
        },
        {
          "name": "Apply changes",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/backend/deployment.yaml --dry-run=client",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Optimize Docker build for Node.js backend with multi-stage caching",
      "steps": [
        {
          "name": "Review current Dockerfile",
          "type": "prompt",
          "prompt": "Analyze the existing Dockerfile to identify opportunities for build optimization and layer caching improvements.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/Code/server/Dockerfile"
          ]
        },
        {
          "name": "Update Dockerfile with build cache",
          "type": "prompt",
          "prompt": "Modify the Dockerfile to use Docker BuildKit cache mounts for npm modules and add .dockerignore optimizations.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/Code/server/Dockerfile",
            "/Users/computer/complex-demo/Code/server/.dockerignore"
          ]
        },
        {
          "name": "Test optimized build",
          "type": "cli",
          "command": "cd /Users/computer/complex-demo/Code/server && DOCKER_BUILDKIT=1 docker build --progress=plain -t backend:optimized .",
          "parameters": {},
          "files": []
        },
        {
          "name": "Compare build times",
          "type": "cli",
          "command": "docker history backend:optimized --human --format \"table {{.CreatedBy}}\\t{{.Size}}\"",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure CircleCI for multi-stage deployment pipeline",
      "steps": [
        {
          "name": "Review CircleCI configuration",
          "type": "prompt",
          "prompt": "Examine the current CircleCI config to understand the existing workflow structure and job definitions.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/.circleci/config.yml"
          ]
        },
        {
          "name": "Add staging deployment job",
          "type": "prompt",
          "prompt": "Update config.yml to add a staging deployment job that runs after tests pass, with manual approval for production.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/.circleci/config.yml"
          ]
        },
        {
          "name": "Configure environment variables",
          "type": "prompt",
          "prompt": "Add CircleCI context references for AWS, GCP, and Azure credentials in the deployment jobs.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/.circleci/config.yml"
          ]
        },
        {
          "name": "Validate CircleCI config",
          "type": "cli",
          "command": "circleci config validate /Users/computer/complex-demo/.circleci/config.yml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up Ansible playbook for Node.js application deployment",
      "steps": [
        {
          "name": "Review existing Ansible playbooks",
          "type": "prompt",
          "prompt": "Examine the day0-provisioning playbook to understand the Ansible patterns and variable usage.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/ansible/playbooks/day0-provisioning.yml"
          ]
        },
        {
          "name": "Create application deployment playbook",
          "type": "prompt",
          "prompt": "Create a new Ansible playbook for deploying the Node.js backend application with health checks and rollback capability.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/ansible/playbooks/deploy-backend-app.yml"
          ]
        },
        {
          "name": "Add Node exporter task",
          "type": "prompt",
          "prompt": "Include the install-node-exporter task in the deployment playbook for monitoring integration.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/ansible/playbooks/deploy-backend-app.yml",
            "/Users/computer/complex-demo/ansible/tasks/install-node-exporter.yml"
          ]
        },
        {
          "name": "Test playbook syntax",
          "type": "cli",
          "command": "ansible-playbook --syntax-check /Users/computer/complex-demo/ansible/playbooks/deploy-backend-app.yml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Nexus repository for npm package caching",
      "steps": [
        {
          "name": "Review Nexus configuration script",
          "type": "prompt",
          "prompt": "Examine the configure-nexus.sh script to understand how Nexus repositories are set up.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/scripts/configure-nexus.sh"
          ]
        },
        {
          "name": "Update script for npm proxy",
          "type": "prompt",
          "prompt": "Modify configure-nexus.sh to add npm proxy repository configuration pointing to npmjs.org.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/scripts/configure-nexus.sh"
          ]
        },
        {
          "name": "Configure npm client",
          "type": "prompt",
          "prompt": "Update the backend package.json to include npmrc configuration for using Nexus as npm registry.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/Code/server/package.json",
            "/Users/computer/complex-demo/Code/server/.npmrc"
          ]
        },
        {
          "name": "Test Nexus npm repository",
          "type": "cli",
          "command": "cd /Users/computer/complex-demo/Code/server && npm config set registry http://nexus:8081/repository/npm-proxy/ && npm install --dry-run",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement security hardening for Kubernetes deployments",
      "steps": [
        {
          "name": "Review security hardening script",
          "type": "prompt",
          "prompt": "Examine the security-hardening.sh script to understand the security measures being applied.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/scripts/security-hardening.sh"
          ]
        },
        {
          "name": "Update backend deployment security",
          "type": "prompt",
          "prompt": "Modify the backend deployment.yaml to add security context with runAsNonRoot, readOnlyRootFilesystem, and capabilities drop.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/k8s/envs/dev/backend/deployment.yaml"
          ]
        },
        {
          "name": "Add network policy",
          "type": "prompt",
          "prompt": "Create a NetworkPolicy for the backend namespace to restrict ingress traffic only from frontend pods.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/k8s/envs/dev/backend/network-policy.yaml"
          ]
        },
        {
          "name": "Apply security policies",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/envs/dev/backend/network-policy.yaml --dry-run=client",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up comprehensive monitoring with Datadog integration",
      "steps": [
        {
          "name": "Review Datadog deployment script",
          "type": "prompt",
          "prompt": "Examine the deploy-datadog-multicloud.sh script to understand the multi-cloud Datadog deployment approach.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/scripts/deploy-datadog-multicloud.sh"
          ]
        },
        {
          "name": "Update Datadog EKS configuration",
          "type": "prompt",
          "prompt": "Modify datadog-aws-eks.yaml to enable APM, logs collection, and process monitoring for the backend service.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/k8s/envs/dev/monitoring/datadog-aws-eks.yaml"
          ]
        },
        {
          "name": "Configure ServiceMonitor",
          "type": "prompt",
          "prompt": "Update the Datadog ServiceMonitor to scrape custom metrics from the Node.js backend /metrics endpoint.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/monitoring/datadog-servicemonitor.yaml"
          ]
        },
        {
          "name": "Deploy monitoring stack",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/deploy-datadog-multicloud.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update Makefile with new deployment target",
      "steps": [
        {
          "name": "Add production target",
          "type": "prompt",
          "prompt": "Add a new 'deploy-prod' target to the Makefile that calls deploy.sh with production parameters.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/Makefile"
          ]
        },
        {
          "name": "Test new target",
          "type": "cli",
          "command": "make deploy-prod --dry-run",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure JFrog image pull secret",
      "steps": [
        {
          "name": "Update pull secret",
          "type": "prompt",
          "prompt": "Update the JFrog pull secret YAML with base64 encoded Docker config for authentication.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/k8s/secrets/jfrog-pull-secret.yaml"
          ]
        },
        {
          "name": "Apply secret",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/secrets/jfrog-pull-secret.yaml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Update frontend Docker build",
      "steps": [
        {
          "name": "Optimize frontend Dockerfile",
          "type": "prompt",
          "prompt": "Update the frontend Dockerfile to use nginx:alpine for smaller production image size.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/Code/client/Dockerfile"
          ]
        },
        {
          "name": "Build optimized image",
          "type": "cli",
          "command": "cd /Users/computer/complex-demo/Code/client && docker build -t frontend:slim .",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure New Relic monitoring",
      "steps": [
        {
          "name": "Update New Relic config",
          "type": "prompt",
          "prompt": "Update the New Relic integration YAML with proper license key and app name configuration.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/k8s/envs/dev/monitoring/newrelic-integration.yaml"
          ]
        },
        {
          "name": "Deploy New Relic",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/k8s/envs/dev/monitoring/deploy-newrelic.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Add Terraform backend configuration",
      "steps": [
        {
          "name": "Update backend config",
          "type": "prompt",
          "prompt": "Modify the backend configuration JSON to use the correct S3 bucket and DynamoDB table for state locking.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/terraform/bootstrap/generated/backend-config.json"
          ]
        },
        {
          "name": "Initialize backend",
          "type": "cli",
          "command": "cd /Users/computer/complex-demo/terraform && terraform init -backend-config=bootstrap/generated/backend-config.json",
          "parameters": {},
          "files": []
        }
      ]
    }
  ],
  "level_2": [
    {
      "goal": "Deploy a highly available RDS PostgreSQL cluster with read replicas and comprehensive monitoring",
      "steps": [
        {
          "name": "Analyze VPC and security requirements",
          "type": "integration",
          "integration": "aws",
          "method": "ec2.DescribeVpcs",
          "parameters": {
            "Filters": [
              {
                "Name": "tag:Environment",
                "Values": [
                  "dev"
                ]
              }
            ]
          },
          "files": []
        },
        {
          "name": "Create RDS subnet group",
          "type": "prompt",
          "prompt": "Create Terraform configuration for RDS subnet group spanning multiple availability zones using existing private subnets. Include proper tags and descriptions.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/rds/subnet_group.tf"
          ]
        },
        {
          "name": "Configure RDS parameter group",
          "type": "prompt",
          "prompt": "Create custom RDS parameter group for PostgreSQL 14 with optimized settings for production workloads including connection pooling, query optimization, and logging parameters.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/rds/parameter_group.tf"
          ]
        },
        {
          "name": "Write RDS instance configuration",
          "type": "prompt",
          "prompt": "Create Terraform for RDS PostgreSQL with: db.r5.xlarge instance, 500GB gp3 storage with encryption, multi-AZ deployment, automated backups with 30-day retention, and performance insights enabled.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/rds/main.tf"
          ]
        },
        {
          "name": "Configure read replicas",
          "type": "prompt",
          "prompt": "Add Terraform configuration for 2 read replicas in different availability zones with dedicated parameter groups and automatic minor version upgrades enabled.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/rds/read_replicas.tf"
          ]
        },
        {
          "name": "Apply Terraform configuration",
          "type": "cli",
          "command": "terraform apply -auto-approve",
          "parameters": {
            "working_directory": "terraform/envs/dev/us-east-2/"
          },
          "files": []
        },
        {
          "name": "Verify RDS instance creation",
          "type": "integration",
          "integration": "aws",
          "method": "rds.describe_db_instances",
          "parameters": {
            "DBInstanceIdentifier": "devops-demo-postgres-primary"
          },
          "files": []
        },
        {
          "name": "Test database connectivity",
          "type": "cli",
          "command": "psql -h $(terraform output -raw rds_endpoint) -U postgres -d postgres -c 'SELECT version();'",
          "parameters": {
            "working_directory": "terraform/envs/dev/us-east-2/"
          },
          "files": []
        },
        {
          "name": "Configure CloudWatch alarms",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.PutMetricAlarm",
          "parameters": {
            "AlarmName": "rds-high-cpu-usage",
            "ComparisonOperator": "GreaterThanThreshold",
            "EvaluationPeriods": 2,
            "MetricName": "CPUUtilization",
            "Namespace": "AWS/RDS",
            "Period": 300,
            "Statistic": "Average",
            "Threshold": 80.0,
            "ActionsEnabled": true
          },
          "files": []
        },
        {
          "name": "Verify read replica sync status",
          "type": "integration",
          "integration": "aws",
          "method": "rds.describe_db_instances",
          "parameters": {
            "Filters": [
              {
                "Name": "db-instance-id",
                "Values": [
                  "devops-demo-postgres-replica-*"
                ]
              }
            ]
          },
          "files": []
        },
        {
          "name": "Setup automated backups verification",
          "type": "integration",
          "integration": "aws",
          "method": "rds.describe_db_snapshots",
          "parameters": {
            "DBInstanceIdentifier": "devops-demo-postgres-primary",
            "SnapshotType": "automated"
          },
          "files": []
        },
        {
          "name": "Configure Datadog RDS monitoring",
          "type": "integration",
          "integration": "datadog",
          "method": "MetricsApi.query_scalar_data",
          "parameters": {
            "body": {
              "queries": [
                {
                  "data_source": "metrics",
                  "query": "avg:aws.rds.cpuutilization{dbinstanceidentifier:devops-demo-postgres-primary}",
                  "name": "rds_cpu"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Perform failover test",
          "type": "cli",
          "command": "aws rds failover-db-cluster --db-cluster-identifier devops-demo-postgres-cluster",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate complete RDS setup",
          "type": "prompt",
          "prompt": "Verify the RDS cluster is fully operational with primary instance, read replicas, automated backups, monitoring alarms, and proper security group configurations.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy and configure a production-ready Kubernetes cluster with service mesh and observability",
      "steps": [
        {
          "name": "Verify EKS cluster status",
          "type": "cli",
          "command": "aws eks describe-cluster --name devops-demo-eks --query cluster.status",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update kubeconfig for cluster access",
          "type": "cli",
          "command": "aws eks update-kubeconfig --name devops-demo-eks --region us-east-2",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Istio service mesh",
          "type": "cli",
          "command": "istioctl install --set profile=production -y",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify Istio components",
          "type": "integration",
          "integration": "kubernetes",
          "method": "CoreV1Api.list_namespaced_pod",
          "parameters": {
            "namespace": "istio-system",
            "label_selector": "app=istiod"
          },
          "files": []
        },
        {
          "name": "Deploy application workloads",
          "type": "cli",
          "command": "kubectl apply -f k8s/envs/dev/ --recursive",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify deployments are running",
          "type": "integration",
          "integration": "kubernetes",
          "method": "AppsV1Api.list_namespaced_deployment",
          "parameters": {
            "namespace": "default",
            "label_selector": "app=devops-demo"
          },
          "files": []
        },
        {
          "name": "Configure horizontal pod autoscaling",
          "type": "cli",
          "command": "kubectl autoscale deployment frontend --cpu-percent=70 --min=3 --max=10",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Prometheus monitoring stack",
          "type": "cli",
          "command": "helm install prometheus prometheus-community/kube-prometheus-stack -f k8s/envs/dev/monitoring/prometheus-values.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify Prometheus is collecting metrics",
          "type": "integration",
          "integration": "prometheus",
          "method": "query",
          "parameters": {
            "query": "up{job=\"kubernetes-nodes\"}",
            "time": "now"
          },
          "files": []
        },
        {
          "name": "Configure Istio traffic management",
          "type": "cli",
          "command": "kubectl apply -f k8s/envs/dev/microservices/istio-policies.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test service mesh traffic routing",
          "type": "cli",
          "command": "kubectl exec -it $(kubectl get pod -l app=frontend -o jsonpath='{.items[0].metadata.name}') -- curl -I backend-service:8080/health",
          "parameters": {},
          "files": []
        },
        {
          "name": "Setup distributed tracing",
          "type": "cli",
          "command": "kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/addons/jaeger.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure log aggregation",
          "type": "cli",
          "command": "kubectl apply -f k8s/envs/dev/logging/fluent-bit-daemonset.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify complete cluster setup",
          "type": "integration",
          "integration": "kubernetes",
          "method": "CoreV1Api.list_namespaced_pod",
          "parameters": {
            "namespace": "default"
          },
          "files": []
        },
        {
          "name": "Run chaos engineering test",
          "type": "cli",
          "command": "kubectl delete pod $(kubectl get pod -l app=backend -o jsonpath='{.items[0].metadata.name}') --grace-period=0",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate self-healing",
          "type": "prompt",
          "prompt": "Monitor the cluster to ensure the deleted pod is automatically recreated and traffic continues to flow without interruption through the service mesh.",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement comprehensive CI/CD pipeline with security scanning and automated deployments",
      "steps": [
        {
          "name": "Configure Jenkins instance",
          "type": "cli",
          "command": "terraform output jenkins_url",
          "parameters": {
            "working_directory": "terraform/envs/dev/us-east-2/"
          },
          "files": []
        },
        {
          "name": "Setup Jenkins credentials",
          "type": "prompt",
          "prompt": "Configure Jenkins with credentials for AWS, Docker registry, Kubernetes cluster, and source code repository access using Jenkins credentials management.",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create multibranch pipeline",
          "type": "prompt",
          "prompt": "Create a Jenkins multibranch pipeline configuration that discovers branches from the Git repository and automatically creates build jobs for each branch with different deployment strategies.",
          "parameters": {},
          "files": [
            "ci-cd/jenkins/pipelines/multibranch-pipeline.groovy"
          ]
        },
        {
          "name": "Configure build stage with caching",
          "type": "prompt",
          "prompt": "Implement build stage in Jenkinsfile with Docker layer caching, dependency caching, and parallel builds for frontend and backend components.",
          "parameters": {},
          "files": [
            "Jenkinsfile"
          ]
        },
        {
          "name": "Add security scanning stages",
          "type": "prompt",
          "prompt": "Add pipeline stages for SAST (SonarQube), dependency scanning (OWASP), container scanning (Trivy), and infrastructure as code scanning (Checkov).",
          "parameters": {},
          "files": [
            "Jenkinsfile"
          ]
        },
        {
          "name": "Deploy pipeline to Jenkins",
          "type": "cli",
          "command": "jenkins-cli create-job devops-demo-pipeline < ci-cd/jenkins/pipelines/config.xml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Trigger initial pipeline run",
          "type": "integration",
          "integration": "jenkins",
          "method": "build_job",
          "parameters": {
            "name": "devops-demo-pipeline",
            "parameters": {
              "BRANCH": "main"
            }
          },
          "files": []
        },
        {
          "name": "Monitor build progress",
          "type": "integration",
          "integration": "jenkins",
          "method": "get_build_info",
          "parameters": {
            "name": "devops-demo-pipeline",
            "number": "lastBuild"
          },
          "files": []
        },
        {
          "name": "Verify security scan results",
          "type": "cli",
          "command": "curl -s http://sonarqube.local/api/measures/component?component=devops-demo&metricKeys=security_rating,vulnerabilities",
          "parameters": {},
          "files": []
        },
        {
          "name": "Push images to registry",
          "type": "cli",
          "command": "docker push $(terraform output ecr_repository_url):latest",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy to Kubernetes staging",
          "type": "integration",
          "integration": "kubernetes",
          "method": "AppsV1Api.patch_namespaced_deployment",
          "parameters": {
            "name": "frontend",
            "namespace": "staging",
            "body": {
              "spec": {
                "template": {
                  "spec": {
                    "containers": [
                      {
                        "name": "frontend",
                        "image": "${ECR_REPO}:${BUILD_NUMBER}"
                      }
                    ]
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Run integration tests",
          "type": "cli",
          "command": "npm run test:integration -- --reporter json --output test-results.json",
          "parameters": {
            "working_directory": "Code/server"
          },
          "files": []
        },
        {
          "name": "Promote to production",
          "type": "integration",
          "integration": "argocd",
          "method": "sync_application",
          "parameters": {
            "name": "devops-demo-prod",
            "revision": "main",
            "prune": true
          },
          "files": []
        },
        {
          "name": "Verify production deployment",
          "type": "cli",
          "command": "kubectl rollout status deployment/frontend -n production --timeout=600s",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure deployment notifications",
          "type": "integration",
          "integration": "datadog",
          "method": "events.create_event",
          "parameters": {
            "title": "Production Deployment Complete",
            "text": "DevOps Demo application deployed to production",
            "tags": [
              "deployment",
              "production",
              "ci-cd"
            ]
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Perform comprehensive AWS cost optimization analysis and implement savings recommendations",
      "steps": [
        {
          "name": "Analyze current AWS costs",
          "type": "integration",
          "integration": "aws",
          "method": "ce.get_cost_and_usage",
          "parameters": {
            "TimePeriod": {
              "Start": "2024-01-01",
              "End": "2024-01-31"
            },
            "Granularity": "MONTHLY",
            "Metrics": [
              "UNBLENDED_COST"
            ],
            "GroupBy": [
              {
                "Type": "DIMENSION",
                "Key": "SERVICE"
              }
            ]
          },
          "files": []
        },
        {
          "name": "Identify unused EC2 instances",
          "type": "integration",
          "integration": "aws",
          "method": "ec2.DescribeInstances",
          "parameters": {
            "Filters": [
              {
                "Name": "instance-state-name",
                "Values": [
                  "running"
                ]
              }
            ]
          },
          "files": []
        },
        {
          "name": "Check EC2 utilization metrics",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.get_metric_statistics",
          "parameters": {
            "Namespace": "AWS/EC2",
            "MetricName": "CPUUtilization",
            "StartTime": "2024-01-01T00:00:00Z",
            "EndTime": "2024-01-31T23:59:59Z",
            "Period": 86400,
            "Statistics": [
              "Average"
            ]
          },
          "files": []
        },
        {
          "name": "Analyze EBS volume usage",
          "type": "cli",
          "command": "aws ec2 describe-volumes --filters Name=status,Values=available --query 'Volumes[*].[VolumeId,Size,VolumeType,CreateTime]' --output table",
          "parameters": {},
          "files": []
        },
        {
          "name": "Review RDS instance sizing",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.get_metric_statistics",
          "parameters": {
            "Namespace": "AWS/RDS",
            "MetricName": "CPUUtilization",
            "Dimensions": [
              {
                "Name": "DBInstanceIdentifier",
                "Value": "production-db"
              }
            ],
            "StartTime": "2024-01-01T00:00:00Z",
            "EndTime": "2024-01-31T23:59:59Z",
            "Period": 3600,
            "Statistics": [
              "Average",
              "Maximum"
            ]
          },
          "files": []
        },
        {
          "name": "Generate cost optimization report",
          "type": "prompt",
          "prompt": "Analyze the collected metrics and create a comprehensive cost optimization report identifying: underutilized instances, unattached volumes, oversized RDS instances, and potential savings from Reserved Instances.",
          "parameters": {},
          "files": [
            "reports/cost-optimization-report.md"
          ]
        },
        {
          "name": "Calculate Reserved Instance recommendations",
          "type": "integration",
          "integration": "aws",
          "method": "ce.get_reservation_purchase_recommendation",
          "parameters": {
            "Service": "EC2",
            "TermInYears": "ONE_YEAR",
            "PaymentOption": "PARTIAL_UPFRONT"
          },
          "files": []
        },
        {
          "name": "Identify S3 lifecycle opportunities",
          "type": "cli",
          "command": "aws s3api list-buckets --query 'Buckets[*].Name' | xargs -I {} aws s3api get-bucket-lifecycle-configuration --bucket {}",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check for idle load balancers",
          "type": "integration",
          "integration": "aws",
          "method": "elbv2.describe_load_balancers",
          "parameters": {},
          "files": []
        },
        {
          "name": "Analyze data transfer costs",
          "type": "integration",
          "integration": "aws",
          "method": "ce.get_cost_and_usage",
          "parameters": {
            "TimePeriod": {
              "Start": "2024-01-01",
              "End": "2024-01-31"
            },
            "Granularity": "DAILY",
            "Metrics": [
              "UNBLENDED_COST"
            ],
            "Filter": {
              "Dimensions": {
                "Key": "USAGE_TYPE_GROUP",
                "Values": [
                  "EC2: Data Transfer"
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Create savings plan recommendations",
          "type": "prompt",
          "prompt": "Based on usage patterns, create specific recommendations for Savings Plans that would provide the best discount rates for the workload profile.",
          "parameters": {},
          "files": [
            "reports/savings-plan-recommendations.md"
          ]
        },
        {
          "name": "Implement tagging strategy",
          "type": "prompt",
          "prompt": "Design and document a comprehensive tagging strategy for cost allocation, including cost centers, projects, environments, and owners.",
          "parameters": {},
          "files": [
            "policies/tagging-strategy.md"
          ]
        },
        {
          "name": "Configure cost anomaly detection",
          "type": "integration",
          "integration": "aws",
          "method": "ce.create_anomaly_monitor",
          "parameters": {
            "AnomalyMonitor": {
              "MonitorName": "DevOps-Cost-Monitor",
              "MonitorType": "DIMENSIONAL",
              "MonitorDimension": "SERVICE"
            }
          },
          "files": []
        },
        {
          "name": "Set up budget alerts",
          "type": "cli",
          "command": "aws budgets create-budget --account-id $(aws sts get-caller-identity --query Account --output text) --budget file://budgets/monthly-budget.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate optimization impact",
          "type": "prompt",
          "prompt": "Calculate the total potential monthly savings from all identified optimization opportunities and create an implementation priority matrix based on effort vs. savings.",
          "parameters": {},
          "files": [
            "reports/optimization-impact-analysis.md"
          ]
        }
      ]
    },
    {
      "goal": "Implement zero-trust security architecture with AWS IAM, service mesh policies, and network segmentation",
      "steps": [
        {
          "name": "Analyze current security posture",
          "type": "integration",
          "integration": "aws",
          "method": "iam.get_account_authorization_details",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create IAM roles with minimal permissions",
          "type": "prompt",
          "prompt": "Design IAM roles following principle of least privilege for each service. Create separate roles for frontend, backend, and data processing services with only required permissions.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/iam/service-roles.tf"
          ]
        },
        {
          "name": "Implement AWS PrivateLink endpoints",
          "type": "prompt",
          "prompt": "Configure VPC endpoints for AWS services (S3, DynamoDB, ECR, etc.) to ensure traffic doesn't traverse the internet. Include endpoint policies restricting access.",
          "parameters": {},
          "files": [
            "terraform/modules/aws/vpc/endpoints.tf"
          ]
        },
        {
          "name": "Deploy AWS WAF rules",
          "type": "integration",
          "integration": "aws",
          "method": "wafv2.create_web_acl",
          "parameters": {
            "Name": "zero-trust-waf",
            "Scope": "REGIONAL",
            "DefaultAction": {
              "Block": {}
            },
            "Rules": [
              {
                "Name": "RateLimitRule",
                "Priority": 1,
                "Statement": {
                  "RateBasedStatement": {
                    "Limit": 2000,
                    "AggregateKeyType": "IP"
                  }
                },
                "Action": {
                  "Block": {}
                }
              }
            ]
          },
          "files": []
        },
        {
          "name": "Configure Istio authorization policies",
          "type": "prompt",
          "prompt": "Create Istio AuthorizationPolicy resources implementing zero-trust between services. Define explicit allow rules for service-to-service communication with JWT validation.",
          "parameters": {},
          "files": [
            "k8s/service-mesh/authorization-policies.yaml"
          ]
        },
        {
          "name": "Apply network policies",
          "type": "cli",
          "command": "kubectl apply -f k8s/network-policies/ --recursive",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy secrets management",
          "type": "integration",
          "integration": "aws",
          "method": "secretsmanager.create_secret",
          "parameters": {
            "Name": "app/database/credentials",
            "SecretString": "{\"username\":\"dbuser\",\"password\":\"${GENERATED_PASSWORD}\"}",
            "KmsKeyId": "alias/aws/secretsmanager"
          },
          "files": []
        },
        {
          "name": "Enable AWS GuardDuty",
          "type": "integration",
          "integration": "aws",
          "method": "guardduty.create_detector",
          "parameters": {
            "Enable": true,
            "FindingPublishingFrequency": "FIFTEEN_MINUTES",
            "DataSources": {
              "S3Logs": {
                "Enable": true
              },
              "Kubernetes": {
                "AuditLogs": {
                  "Enable": true
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure mTLS between services",
          "type": "cli",
          "command": "istioctl install --set values.global.mtls.auto=true --set values.global.mtls.enabled=true",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test service authorization",
          "type": "cli",
          "command": "kubectl exec -it $(kubectl get pod -l app=test-client -o jsonpath='{.items[0].metadata.name}') -- curl -I https://backend-service:8443/api/health",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify network segmentation",
          "type": "integration",
          "integration": "aws",
          "method": "ec2.DescribeSecurityGroups",
          "parameters": {
            "Filters": [
              {
                "Name": "vpc-id",
                "Values": [
                  "${VPC_ID}"
                ]
              }
            ]
          },
          "files": []
        },
        {
          "name": "Run security compliance scan",
          "type": "cli",
          "command": "prowler aws --compliance cis_level2_aws",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure security monitoring",
          "type": "integration",
          "integration": "datadog",
          "method": "SecurityMonitoringApi.create_security_monitoring_rule",
          "parameters": {
            "body": {
              "name": "Unauthorized Access Attempts",
              "query": "source:aws.guardduty @threatIntelligenceDetail.threatNames:*",
              "isEnabled": true,
              "cases": [
                {
                  "status": "high",
                  "condition": "above",
                  "threshold": 1
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Document security architecture",
          "type": "prompt",
          "prompt": "Create comprehensive documentation of the zero-trust architecture including network diagrams, authentication flows, and incident response procedures.",
          "parameters": {},
          "files": [
            "docs/zero-trust-architecture.md"
          ]
        }
      ]
    },
    {
      "goal": "Set up multi-cloud disaster recovery with automated failover between AWS and Azure",
      "steps": [
        {
          "name": "Assess current infrastructure",
          "type": "integration",
          "integration": "aws",
          "method": "ec2.DescribeInstances",
          "parameters": {
            "Filters": [
              {
                "Name": "tag:Environment",
                "Values": [
                  "production"
                ]
              }
            ]
          },
          "files": []
        },
        {
          "name": "Create Azure resource group",
          "type": "integration",
          "integration": "azure",
          "method": "resource_groups.create_or_update",
          "parameters": {
            "resource_group_name": "dr-failover-rg",
            "parameters": {
              "location": "eastus2",
              "tags": {
                "Purpose": "DisasterRecovery",
                "Primary": "AWS"
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure cross-cloud VPN",
          "type": "prompt",
          "prompt": "Set up site-to-site VPN between AWS VPC and Azure VNet for secure cross-cloud communication. Configure BGP for dynamic routing and redundant tunnels.",
          "parameters": {},
          "files": [
            "terraform/multi-cloud/vpn-connection.tf"
          ]
        },
        {
          "name": "Deploy Azure Kubernetes Service",
          "type": "integration",
          "integration": "azure",
          "method": "managed_clusters.begin_create_or_update",
          "parameters": {
            "resource_group_name": "dr-failover-rg",
            "resource_name": "dr-aks-cluster",
            "parameters": {
              "location": "eastus2",
              "dnsPrefix": "dr-aks",
              "agentPoolProfiles": [
                {
                  "name": "nodepool1",
                  "count": 3,
                  "vmSize": "Standard_D4s_v3",
                  "mode": "System"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Set up database replication",
          "type": "cli",
          "command": "aws rds create-db-instance-read-replica --db-instance-identifier dr-replica --source-db-instance-identifier production-db --publicly-accessible",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure DNS failover",
          "type": "integration",
          "integration": "aws",
          "method": "route53.create_health_check",
          "parameters": {
            "CallerReference": "dr-health-check-1",
            "HealthCheckConfig": {
              "Type": "HTTPS",
              "ResourcePath": "/health",
              "FullyQualifiedDomainName": "api.example.com",
              "Port": 443,
              "RequestInterval": 30,
              "FailureThreshold": 3
            }
          },
          "files": []
        },
        {
          "name": "Deploy applications to Azure",
          "type": "cli",
          "command": "kubectl apply -f k8s/dr-manifests/ --context=dr-aks-cluster",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure data synchronization",
          "type": "prompt",
          "prompt": "Implement bi-directional data synchronization between AWS S3 and Azure Blob Storage using Azure Data Factory and AWS DataSync for static assets and backups.",
          "parameters": {},
          "files": [
            "scripts/data-sync-setup.sh"
          ]
        },
        {
          "name": "Test failover procedure",
          "type": "cli",
          "command": "python3 scripts/dr-failover-test.py --source aws --target azure --dry-run",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify Azure deployment health",
          "type": "integration",
          "integration": "azure",
          "method": "deployments.get",
          "parameters": {
            "resource_group_name": "dr-failover-rg",
            "deployment_name": "dr-deployment"
          },
          "files": []
        },
        {
          "name": "Configure monitoring across clouds",
          "type": "integration",
          "integration": "datadog",
          "method": "DashboardsApi.create_dashboard",
          "parameters": {
            "body": {
              "title": "Multi-Cloud DR Status",
              "widgets": [
                {
                  "definition": {
                    "type": "query_value",
                    "requests": [
                      {
                        "q": "avg:aws.rds.replica_lag{*}",
                        "aggregator": "avg"
                      }
                    ]
                  }
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Execute live failover",
          "type": "cli",
          "command": "aws route53 change-resource-record-sets --hosted-zone-id Z123456 --change-batch file://route53-failover.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate failover success",
          "type": "prompt",
          "prompt": "Verify all services are running in Azure, data is synchronized, and traffic is properly routed. Document any issues and rollback procedures.",
          "parameters": {},
          "files": [
            "reports/dr-test-results.md"
          ]
        }
      ]
    },
    {
      "goal": "Build comprehensive data pipeline with real-time streaming and batch processing",
      "steps": [
        {
          "name": "Design data architecture",
          "type": "prompt",
          "prompt": "Create architecture design for hybrid data pipeline supporting both real-time streaming (Kafka/Kinesis) and batch processing (Spark/EMR) with data lake storage.",
          "parameters": {},
          "files": [
            "docs/data-pipeline-architecture.md"
          ]
        },
        {
          "name": "Deploy Kafka cluster",
          "type": "prompt",
          "prompt": "Configure a production-ready Kafka cluster with 3 brokers, ZooKeeper ensemble, and topic configurations for different data streams with appropriate retention policies.",
          "parameters": {},
          "files": [
            "k8s/kafka/kafka-cluster.yaml",
            "k8s/kafka/topics.yaml"
          ]
        },
        {
          "name": "Create Kinesis data streams",
          "type": "integration",
          "integration": "aws",
          "method": "kinesis.create_stream",
          "parameters": {
            "StreamName": "real-time-events",
            "ShardCount": 10,
            "StreamModeDetails": {
              "StreamMode": "ON_DEMAND"
            }
          },
          "files": []
        },
        {
          "name": "Set up S3 data lake",
          "type": "integration",
          "integration": "aws",
          "method": "s3.CreateBucket",
          "parameters": {
            "Bucket": "company-data-lake-prod",
            "CreateBucketConfiguration": {
              "LocationConstraint": "us-east-2"
            }
          },
          "files": []
        },
        {
          "name": "Configure data lake partitioning",
          "type": "cli",
          "command": "aws s3api put-bucket-lifecycle-configuration --bucket company-data-lake-prod --lifecycle-configuration file://s3-lifecycle-policy.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy stream processing jobs",
          "type": "prompt",
          "prompt": "Create Flink/Spark Streaming jobs for real-time data transformation, aggregation, and enrichment. Include windowing functions and state management.",
          "parameters": {},
          "files": [
            "streaming-jobs/event-processor/src/main/scala/EventProcessor.scala"
          ]
        },
        {
          "name": "Set up EMR cluster",
          "type": "integration",
          "integration": "aws",
          "method": "emr.run_job_flow",
          "parameters": {
            "Name": "batch-processing-cluster",
            "ReleaseLabel": "emr-6.10.0",
            "Instances": {
              "InstanceGroups": [
                {
                  "Name": "Master",
                  "Market": "ON_DEMAND",
                  "InstanceRole": "MASTER",
                  "InstanceType": "m5.xlarge",
                  "InstanceCount": 1
                },
                {
                  "Name": "Worker",
                  "Market": "SPOT",
                  "InstanceRole": "CORE",
                  "InstanceType": "m5.2xlarge",
                  "InstanceCount": 5
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Create batch ETL jobs",
          "type": "prompt",
          "prompt": "Develop PySpark ETL jobs for daily batch processing including data validation, deduplication, transformation, and loading into data warehouse.",
          "parameters": {},
          "files": [
            "batch-jobs/daily-etl/main.py",
            "batch-jobs/daily-etl/transformations.py"
          ]
        },
        {
          "name": "Configure data catalog",
          "type": "integration",
          "integration": "aws",
          "method": "glue.create_database",
          "parameters": {
            "DatabaseInput": {
              "Name": "data_lake_catalog",
              "Description": "Central metadata catalog for data lake"
            }
          },
          "files": []
        },
        {
          "name": "Set up data quality monitoring",
          "type": "prompt",
          "prompt": "Implement data quality checks using Great Expectations or Deequ. Create validation rules for schema compliance, null checks, and business logic validation.",
          "parameters": {},
          "files": [
            "data-quality/expectations/event_data_suite.json"
          ]
        },
        {
          "name": "Deploy Apache Airflow",
          "type": "cli",
          "command": "helm install airflow apache-airflow/airflow -f airflow-values.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create orchestration DAGs",
          "type": "prompt",
          "prompt": "Write Airflow DAGs for orchestrating the entire data pipeline including dependencies between streaming and batch jobs, data quality checks, and alerting.",
          "parameters": {},
          "files": [
            "airflow/dags/data_pipeline_dag.py"
          ]
        },
        {
          "name": "Test end-to-end pipeline",
          "type": "cli",
          "command": "python3 tests/pipeline_integration_test.py --source kafka --sink s3 --validate",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor pipeline metrics",
          "type": "integration",
          "integration": "datadog",
          "method": "MetricsApi.submit_metrics",
          "parameters": {
            "body": {
              "series": [
                {
                  "metric": "data.pipeline.throughput",
                  "points": [
                    [
                      1234567890,
                      1000
                    ]
                  ],
                  "type": "rate",
                  "tags": [
                    "pipeline:streaming",
                    "env:prod"
                  ]
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Validate data completeness",
          "type": "prompt",
          "prompt": "Run data reconciliation checks comparing source system counts with data lake records. Generate completeness report and investigate any discrepancies.",
          "parameters": {},
          "files": [
            "reports/data-completeness-report.md"
          ]
        }
      ]
    },
    {
      "goal": "Implement chaos engineering experiments to validate system resilience",
      "steps": [
        {
          "name": "Define steady state",
          "type": "prompt",
          "prompt": "Document system steady state metrics including response times, error rates, and throughput. Define acceptable thresholds for each metric during chaos experiments.",
          "parameters": {},
          "files": [
            "chaos-engineering/steady-state-hypothesis.md"
          ]
        },
        {
          "name": "Deploy Chaos Mesh",
          "type": "cli",
          "command": "helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=chaos-testing --create-namespace",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create pod failure experiments",
          "type": "prompt",
          "prompt": "Design Chaos Mesh experiments to randomly kill pods in production namespace. Target 30% of pods with proper selectors and duration limits.",
          "parameters": {},
          "files": [
            "chaos-engineering/experiments/pod-failure.yaml"
          ]
        },
        {
          "name": "Implement network chaos",
          "type": "prompt",
          "prompt": "Create network chaos experiments including latency injection (100-500ms), packet loss (10-30%), and bandwidth limitations to test service resilience.",
          "parameters": {},
          "files": [
            "chaos-engineering/experiments/network-chaos.yaml"
          ]
        },
        {
          "name": "Configure stress testing",
          "type": "prompt",
          "prompt": "Set up CPU and memory stress experiments targeting specific services. Configure 80% CPU usage and memory pressure to test resource handling.",
          "parameters": {},
          "files": [
            "chaos-engineering/experiments/resource-stress.yaml"
          ]
        },
        {
          "name": "Execute pod deletion experiment",
          "type": "cli",
          "command": "kubectl apply -f chaos-engineering/experiments/pod-failure.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor system behavior",
          "type": "integration",
          "integration": "datadog",
          "method": "MetricsApi.query_scalar_data",
          "parameters": {
            "body": {
              "queries": [
                {
                  "data_source": "metrics",
                  "query": "avg:kubernetes.cpu.usage{kube_namespace:production} by {pod_name}",
                  "name": "cpu_during_chaos"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Test database failover",
          "type": "integration",
          "integration": "aws",
          "method": "rds.failover_db_cluster",
          "parameters": {
            "DBClusterIdentifier": "production-aurora-cluster",
            "TargetDBInstanceIdentifier": "production-aurora-instance-2"
          },
          "files": []
        },
        {
          "name": "Simulate zone failure",
          "type": "cli",
          "command": "kubectl cordon $(kubectl get nodes -l failure-domain.beta.kubernetes.io/zone=us-east-2a -o name)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify auto-scaling",
          "type": "integration",
          "integration": "kubernetes",
          "method": "AppsV1Api.list_namespaced_deployment",
          "parameters": {
            "namespace": "production",
            "label_selector": "app=frontend"
          },
          "files": []
        },
        {
          "name": "Run traffic surge test",
          "type": "cli",
          "command": "k6 run --vus 1000 --duration 5m chaos-engineering/load-tests/surge-test.js",
          "parameters": {},
          "files": []
        },
        {
          "name": "Analyze experiment results",
          "type": "prompt",
          "prompt": "Analyze metrics collected during chaos experiments. Compare with steady state hypothesis and identify any deviations or unexpected behaviors.",
          "parameters": {},
          "files": [
            "chaos-engineering/reports/experiment-analysis.md"
          ]
        },
        {
          "name": "Clean up experiments",
          "type": "cli",
          "command": "kubectl delete chaosengine --all -n production",
          "parameters": {},
          "files": []
        },
        {
          "name": "Document improvements",
          "type": "prompt",
          "prompt": "Based on chaos experiment findings, document required system improvements, updated runbooks, and new monitoring alerts to enhance resilience.",
          "parameters": {},
          "files": [
            "chaos-engineering/improvement-plan.md"
          ]
        }
      ]
    },
    {
      "goal": "Create enterprise-grade monitoring and alerting system with SLO-based alerts",
      "steps": [
        {
          "name": "Define SLIs and SLOs",
          "type": "prompt",
          "prompt": "Define Service Level Indicators (availability, latency, error rate) and corresponding SLOs (99.9% availability, p99 latency < 200ms) for all critical services.",
          "parameters": {},
          "files": [
            "monitoring/slo-definitions.yaml"
          ]
        },
        {
          "name": "Deploy Prometheus Operator",
          "type": "cli",
          "command": "helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -f monitoring/prometheus-values.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure service monitors",
          "type": "prompt",
          "prompt": "Create ServiceMonitor CRDs for all microservices to enable automatic Prometheus scraping with appropriate labels and endpoints.",
          "parameters": {},
          "files": [
            "monitoring/service-monitors/"
          ]
        },
        {
          "name": "Set up recording rules",
          "type": "prompt",
          "prompt": "Define Prometheus recording rules for pre-computing complex queries, SLI calculations, and aggregations to optimize query performance.",
          "parameters": {},
          "files": [
            "monitoring/prometheus-rules/recording-rules.yaml"
          ]
        },
        {
          "name": "Create SLO-based alerts",
          "type": "prompt",
          "prompt": "Implement multi-window, multi-burn-rate alerts for SLOs using the error budget approach. Include both fast-burn and slow-burn alert conditions.",
          "parameters": {},
          "files": [
            "monitoring/prometheus-rules/slo-alerts.yaml"
          ]
        },
        {
          "name": "Deploy Thanos for HA",
          "type": "cli",
          "command": "helm install thanos bitnami/thanos -f monitoring/thanos-values.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure Grafana dashboards",
          "type": "integration",
          "integration": "grafana",
          "method": "dashboard.create_dashboard",
          "parameters": {
            "dashboard": {
              "title": "SLO Overview",
              "panels": [
                {
                  "title": "Service Availability",
                  "targets": [
                    {
                      "expr": "sum(rate(http_requests_total{status!~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))"
                    }
                  ]
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Set up alert routing",
          "type": "prompt",
          "prompt": "Configure AlertManager with routing rules based on severity, team ownership, and time-based escalations. Include PagerDuty and Slack integrations.",
          "parameters": {},
          "files": [
            "monitoring/alertmanager-config.yaml"
          ]
        },
        {
          "name": "Deploy custom exporters",
          "type": "prompt",
          "prompt": "Create custom Prometheus exporters for business metrics (orders per minute, payment success rate) and deploy them as Kubernetes deployments.",
          "parameters": {},
          "files": [
            "monitoring/custom-exporters/business-metrics-exporter/"
          ]
        },
        {
          "name": "Configure log-based alerts",
          "type": "integration",
          "integration": "elasticsearch",
          "method": "watcher.put_watch",
          "parameters": {
            "id": "error-spike-alert",
            "body": {
              "trigger": {
                "schedule": {
                  "interval": "1m"
                }
              },
              "input": {
                "search": {
                  "request": {
                    "indices": [
                      "logs-*"
                    ],
                    "body": {
                      "query": {
                        "match": {
                          "level": "ERROR"
                        }
                      }
                    }
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Test alert notifications",
          "type": "cli",
          "command": "amtool alert add alertname=test severity=critical service=api --alertmanager.url=http://alertmanager:9093",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create runbook automation",
          "type": "prompt",
          "prompt": "Develop automated runbooks triggered by specific alerts for common issues like pod restarts, disk cleanup, and cache clearing.",
          "parameters": {},
          "files": [
            "monitoring/runbooks/automated-remediation.yaml"
          ]
        },
        {
          "name": "Set up SLO reporting",
          "type": "integration",
          "integration": "grafana",
          "method": "dashboard.create_dashboard",
          "parameters": {
            "dashboard": {
              "title": "Monthly SLO Report",
              "panels": [
                {
                  "title": "Error Budget Burn Rate",
                  "targets": [
                    {
                      "expr": "(1 - slo_availability) / (1 - 0.999) * 100"
                    }
                  ]
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Validate monitoring coverage",
          "type": "prompt",
          "prompt": "Audit all services to ensure complete monitoring coverage. Verify each service has metrics, logs, traces, and appropriate alerts configured.",
          "parameters": {},
          "files": [
            "monitoring/coverage-report.md"
          ]
        }
      ]
    },
    {
      "goal": "Implement GitLab CI/CD pipeline with security scanning and compliance checks",
      "steps": [
        {
          "name": "Create GitLab CI configuration",
          "type": "prompt",
          "prompt": "Design comprehensive .gitlab-ci.yml with stages for build, test, security scanning, compliance checks, and deployment across multiple environments.",
          "parameters": {},
          "files": [
            ".gitlab-ci.yml"
          ]
        },
        {
          "name": "Configure container registry",
          "type": "integration",
          "integration": "gitlab",
          "method": "projects.update",
          "parameters": {
            "id": "${PROJECT_ID}",
            "container_registry_enabled": true,
            "container_registry_access_level": "enabled"
          },
          "files": []
        },
        {
          "name": "Set up SAST scanning",
          "type": "prompt",
          "prompt": "Configure GitLab SAST with custom rulesets for detecting security vulnerabilities in multiple languages. Include severity thresholds for pipeline failures.",
          "parameters": {},
          "files": [
            ".gitlab/sast-config.yml"
          ]
        },
        {
          "name": "Implement dependency scanning",
          "type": "prompt",
          "prompt": "Set up dependency scanning for vulnerabilities in third-party libraries. Configure auto-remediation for patch-level updates and notifications for major vulnerabilities.",
          "parameters": {},
          "files": [
            ".gitlab/dependency-scanning.yml"
          ]
        },
        {
          "name": "Configure license compliance",
          "type": "prompt",
          "prompt": "Implement license compliance scanning to detect and block prohibited licenses. Define approved license list and exception approval workflow.",
          "parameters": {},
          "files": [
            ".gitlab/license-compliance.yml"
          ]
        },
        {
          "name": "Validate CI configuration",
          "type": "integration",
          "integration": "gitlab",
          "method": "ci_lint.validate",
          "parameters": {
            "content": "$(cat .gitlab-ci.yml)"
          },
          "files": []
        },
        {
          "name": "Create deployment environments",
          "type": "integration",
          "integration": "gitlab",
          "method": "environments.create",
          "parameters": {
            "project_id": "${PROJECT_ID}",
            "name": "production",
            "external_url": "https://api.example.com",
            "tier": "production"
          },
          "files": []
        },
        {
          "name": "Set up protected branches",
          "type": "integration",
          "integration": "gitlab",
          "method": "protected_branches.protect",
          "parameters": {
            "id": "${PROJECT_ID}",
            "name": "main",
            "push_access_level": 40,
            "merge_access_level": 30,
            "allow_force_push": false
          },
          "files": []
        },
        {
          "name": "Configure merge request approvals",
          "type": "integration",
          "integration": "gitlab",
          "method": "merge_request_approvals.set_configuration",
          "parameters": {
            "id": "${PROJECT_ID}",
            "approvals_before_merge": 2,
            "reset_approvals_on_push": true,
            "disable_overriding_approvers_per_merge_request": true
          },
          "files": []
        },
        {
          "name": "Implement code quality gates",
          "type": "prompt",
          "prompt": "Configure code quality thresholds that block merge requests if coverage drops below 80%, cyclomatic complexity exceeds limits, or code smells increase.",
          "parameters": {},
          "files": [
            ".gitlab/code-quality.yml"
          ]
        },
        {
          "name": "Set up performance testing",
          "type": "prompt",
          "prompt": "Add performance testing stage using k6 or JMeter. Define performance baselines and fail pipeline if response times increase by more than 20%.",
          "parameters": {},
          "files": [
            "performance-tests/baseline-test.js"
          ]
        },
        {
          "name": "Configure artifact management",
          "type": "prompt",
          "prompt": "Set up artifact retention policies, dependency proxy for Docker images, and package registry for internal libraries with semantic versioning.",
          "parameters": {},
          "files": [
            ".gitlab/artifact-config.yml"
          ]
        },
        {
          "name": "Test pipeline execution",
          "type": "cli",
          "command": "gitlab-runner exec docker test-job",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create compliance report",
          "type": "prompt",
          "prompt": "Generate compliance report dashboard showing security scan results, license compliance status, and code quality metrics across all projects.",
          "parameters": {},
          "files": [
            "compliance/dashboard-config.json"
          ]
        },
        {
          "name": "Document CI/CD workflows",
          "type": "prompt",
          "prompt": "Create comprehensive documentation covering pipeline stages, approval workflows, rollback procedures, and troubleshooting guides for developers.",
          "parameters": {},
          "files": [
            "docs/cicd-guide.md"
          ]
        }
      ]
    },
    {
      "goal": "Deploy and manage multi-cluster service mesh with Istio across hybrid cloud environments",
      "steps": [
        {
          "name": "Assess cluster environments",
          "type": "integration",
          "integration": "kubernetes",
          "method": "CoreV1Api.list_node",
          "parameters": {},
          "files": []
        },
        {
          "name": "Install Istio control plane",
          "type": "cli",
          "command": "istioctl install --set values.pilot.env.EXTERNAL_ISTIOD=true --set values.global.meshID=mesh1 --set values.global.multiCluster.clusterName=cluster1 --set values.global.network=network1",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure multi-cluster setup",
          "type": "prompt",
          "prompt": "Create Istio multi-cluster configuration connecting EKS, GKE, and on-premise clusters. Set up east-west gateways and cross-cluster service discovery.",
          "parameters": {},
          "files": [
            "istio/multicluster-config.yaml"
          ]
        },
        {
          "name": "Create cluster secrets",
          "type": "cli",
          "command": "istioctl x create-remote-secret --context=cluster2 --name=cluster2 | kubectl apply -f - --context=cluster1",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy east-west gateway",
          "type": "cli",
          "command": "kubectl apply -f istio/eastwest-gateway.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure service discovery",
          "type": "integration",
          "integration": "kubernetes",
          "method": "CoreV1Api.create_namespaced_service",
          "parameters": {
            "namespace": "istio-system",
            "body": {
              "metadata": {
                "name": "cross-cluster-gateway"
              },
              "spec": {
                "type": "LoadBalancer",
                "selector": {
                  "istio": "eastwestgateway"
                },
                "ports": [
                  {
                    "port": 15443,
                    "targetPort": 15443
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Apply traffic management policies",
          "type": "cli",
          "command": "kubectl apply -f istio/traffic-policies/ --recursive",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure observability",
          "type": "prompt",
          "prompt": "Set up distributed tracing across clusters with Jaeger, configure Prometheus federation for metrics aggregation, and implement Kiali for service mesh visualization.",
          "parameters": {},
          "files": [
            "istio/observability/"
          ]
        },
        {
          "name": "Test cross-cluster communication",
          "type": "cli",
          "command": "kubectl exec -it $(kubectl get pod -l app=test-client -o jsonpath='{.items[0].metadata.name}') --context=cluster1 -- curl http://remote-service.default.cluster2.local:8080",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify service mesh configuration",
          "type": "cli",
          "command": "istioctl proxy-config cluster $(kubectl get pod -l app=productpage -o jsonpath='{.items[0].metadata.name}') --context=cluster1",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure security policies",
          "type": "integration",
          "integration": "kubernetes",
          "method": "CustomObjectsApi.create_namespaced_custom_object",
          "parameters": {
            "group": "security.istio.io",
            "version": "v1beta1",
            "namespace": "default",
            "plural": "authorizationpolicies",
            "body": {
              "metadata": {
                "name": "cross-cluster-auth"
              },
              "spec": {
                "selector": {
                  "matchLabels": {
                    "app": "productpage"
                  }
                },
                "rules": [
                  {
                    "from": [
                      {
                        "source": {
                          "principals": [
                            "cluster.local/ns/default/sa/bookinfo-productpage"
                          ]
                        }
                      }
                    ]
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Test failover scenarios",
          "type": "cli",
          "command": "kubectl scale deployment productpage-v1 --replicas=0 --context=cluster1",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor service mesh health",
          "type": "integration",
          "integration": "prometheus",
          "method": "query",
          "parameters": {
            "query": "istio_request_duration_milliseconds_bucket{destination_service_name=\"productpage\"}"
          },
          "files": []
        },
        {
          "name": "Validate multi-cluster setup",
          "type": "prompt",
          "prompt": "Verify all clusters are connected, services are discoverable across clusters, and traffic policies are properly enforced. Document the final topology.",
          "parameters": {},
          "files": [
            "docs/multicluster-service-mesh.md"
          ]
        }
      ]
    },
    {
      "goal": "Implement serverless data processing pipeline with AWS Lambda, Kinesis, and Athena",
      "steps": [
        {
          "name": "Create Kinesis data streams",
          "type": "integration",
          "integration": "aws",
          "method": "kinesis.create_stream",
          "parameters": {
            "StreamName": "real-time-analytics",
            "ShardCount": 5,
            "StreamModeDetails": {
              "StreamMode": "PROVISIONED"
            }
          },
          "files": []
        },
        {
          "name": "Deploy Lambda processors",
          "type": "integration",
          "integration": "aws",
          "method": "lambda.CreateFunction",
          "parameters": {
            "FunctionName": "kinesis-processor",
            "Runtime": "python3.9",
            "Role": "arn:aws:iam::123456789012:role/lambda-kinesis-role",
            "Handler": "handler.process_records",
            "Code": {
              "S3Bucket": "lambda-code-bucket",
              "S3Key": "kinesis-processor.zip"
            },
            "Environment": {
              "Variables": {
                "OUTPUT_BUCKET": "processed-data-bucket"
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure event source mapping",
          "type": "integration",
          "integration": "aws",
          "method": "lambda.create_event_source_mapping",
          "parameters": {
            "FunctionName": "kinesis-processor",
            "EventSourceArn": "arn:aws:kinesis:us-east-2:123456789012:stream/real-time-analytics",
            "StartingPosition": "LATEST",
            "ParallelizationFactor": 10,
            "MaximumBatchingWindowInSeconds": 5
          },
          "files": []
        },
        {
          "name": "Set up Kinesis Firehose",
          "type": "prompt",
          "prompt": "Configure Kinesis Firehose delivery stream to batch and compress data, transform records using Lambda, and deliver to S3 with proper partitioning for Athena queries.",
          "parameters": {},
          "files": [
            "terraform/kinesis/firehose-delivery-stream.tf"
          ]
        },
        {
          "name": "Create Glue data catalog",
          "type": "integration",
          "integration": "aws",
          "method": "glue.create_table",
          "parameters": {
            "DatabaseName": "analytics_db",
            "TableInput": {
              "Name": "processed_events",
              "StorageDescriptor": {
                "Columns": [
                  {
                    "Name": "event_id",
                    "Type": "string"
                  },
                  {
                    "Name": "timestamp",
                    "Type": "timestamp"
                  },
                  {
                    "Name": "user_id",
                    "Type": "string"
                  },
                  {
                    "Name": "event_data",
                    "Type": "string"
                  }
                ],
                "Location": "s3://processed-data-bucket/events/",
                "InputFormat": "org.apache.hadoop.mapred.TextInputFormat",
                "OutputFormat": "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",
                "SerdeInfo": {
                  "SerializationLibrary": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure Athena workgroup",
          "type": "cli",
          "command": "aws athena create-work-group --name analytics-workgroup --configuration ResultConfigurationUpdates={OutputLocation=s3://athena-query-results/} --description 'Workgroup for analytics queries'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy data quality Lambda",
          "type": "prompt",
          "prompt": "Create Lambda function for data quality validation including schema validation, anomaly detection, and alerting for data quality issues.",
          "parameters": {},
          "files": [
            "lambda-functions/data-quality-validator/"
          ]
        },
        {
          "name": "Test data ingestion",
          "type": "cli",
          "command": "aws kinesis put-records --stream-name real-time-analytics --records file://test-data/sample-events.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor Lambda performance",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.get_metric_statistics",
          "parameters": {
            "Namespace": "AWS/Lambda",
            "MetricName": "Duration",
            "Dimensions": [
              {
                "Name": "FunctionName",
                "Value": "kinesis-processor"
              }
            ],
            "StartTime": "2024-01-15T00:00:00Z",
            "EndTime": "2024-01-15T01:00:00Z",
            "Period": 300,
            "Statistics": [
              "Average",
              "Maximum"
            ]
          },
          "files": []
        },
        {
          "name": "Query processed data",
          "type": "cli",
          "command": "aws athena start-query-execution --query-string 'SELECT COUNT(*) FROM analytics_db.processed_events WHERE timestamp > current_timestamp - interval 1 hour' --work-group analytics-workgroup",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure auto-scaling",
          "type": "integration",
          "integration": "aws",
          "method": "application-autoscaling.put_scaling_policy",
          "parameters": {
            "ServiceNamespace": "lambda",
            "ResourceId": "function:kinesis-processor:provisioned-concurrency:BLUE",
            "ScalableDimension": "lambda:function:ProvisionedConcurrency",
            "PolicyType": "TargetTrackingScaling",
            "TargetTrackingScalingPolicyConfiguration": {
              "TargetValue": 0.7,
              "PredefinedMetricSpecification": {
                "PredefinedMetricType": "LambdaProvisionedConcurrencyUtilization"
              }
            }
          },
          "files": []
        },
        {
          "name": "Set up cost monitoring",
          "type": "integration",
          "integration": "aws",
          "method": "ce.create_cost_category_definition",
          "parameters": {
            "Name": "ServerlessDataPipeline",
            "Rules": [
              {
                "Value": "DataProcessing",
                "Rule": {
                  "Dimensions": {
                    "Key": "SERVICE",
                    "Values": [
                      "Lambda",
                      "Kinesis",
                      "Athena"
                    ]
                  }
                }
              }
            ]
          },
          "files": []
        },
        {
          "name": "Validate end-to-end pipeline",
          "type": "prompt",
          "prompt": "Run end-to-end tests verifying data flows from Kinesis through Lambda processing to S3 storage and is queryable via Athena. Document performance metrics and costs.",
          "parameters": {},
          "files": [
            "reports/pipeline-validation.md"
          ]
        }
      ]
    },
    {
      "goal": "Build zero-downtime database migration system with CDC and gradual cutover",
      "steps": [
        {
          "name": "Assess source database",
          "type": "integration",
          "integration": "aws",
          "method": "rds.describe_db_instances",
          "parameters": {
            "DBInstanceIdentifier": "legacy-mysql-db"
          },
          "files": []
        },
        {
          "name": "Create target Aurora cluster",
          "type": "prompt",
          "prompt": "Provision Aurora PostgreSQL cluster with appropriate instance sizes, parameter groups optimized for the workload, and encryption enabled.",
          "parameters": {},
          "files": [
            "terraform/database-migration/aurora-target.tf"
          ]
        },
        {
          "name": "Set up DMS replication instance",
          "type": "integration",
          "integration": "aws",
          "method": "dms.create_replication_instance",
          "parameters": {
            "ReplicationInstanceIdentifier": "migration-instance",
            "ReplicationInstanceClass": "dms.r5.xlarge",
            "VpcSecurityGroupIds": [
              "sg-migration"
            ],
            "MultiAZ": true
          },
          "files": []
        },
        {
          "name": "Configure source endpoint",
          "type": "integration",
          "integration": "aws",
          "method": "dms.create_endpoint",
          "parameters": {
            "EndpointIdentifier": "mysql-source",
            "EndpointType": "source",
            "EngineName": "mysql",
            "ServerName": "legacy-mysql-db.region.rds.amazonaws.com",
            "Port": 3306,
            "DatabaseName": "production"
          },
          "files": []
        },
        {
          "name": "Configure target endpoint",
          "type": "integration",
          "integration": "aws",
          "method": "dms.create_endpoint",
          "parameters": {
            "EndpointIdentifier": "aurora-target",
            "EndpointType": "target",
            "EngineName": "aurora-postgresql",
            "ServerName": "new-aurora-cluster.region.rds.amazonaws.com",
            "Port": 5432,
            "DatabaseName": "production"
          },
          "files": []
        },
        {
          "name": "Create migration task",
          "type": "integration",
          "integration": "aws",
          "method": "dms.create_replication_task",
          "parameters": {
            "ReplicationTaskIdentifier": "full-load-and-cdc",
            "SourceEndpointArn": "${SOURCE_ENDPOINT_ARN}",
            "TargetEndpointArn": "${TARGET_ENDPOINT_ARN}",
            "ReplicationInstanceArn": "${REPLICATION_INSTANCE_ARN}",
            "MigrationType": "full-load-and-cdc",
            "TableMappings": "{\"rules\":[{\"rule-type\":\"selection\",\"rule-id\":\"1\",\"rule-name\":\"1\",\"object-locator\":{\"schema-name\":\"%\",\"table-name\":\"%\"},\"rule-action\":\"include\"}]}"
          },
          "files": []
        },
        {
          "name": "Start replication task",
          "type": "cli",
          "command": "aws dms start-replication-task --replication-task-arn ${TASK_ARN} --start-replication-task-type start-replication",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor replication lag",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.get_metric_statistics",
          "parameters": {
            "Namespace": "AWS/DMS",
            "MetricName": "CDCLatencySource",
            "Dimensions": [
              {
                "Name": "ReplicationTaskIdentifier",
                "Value": "full-load-and-cdc"
              }
            ],
            "StartTime": "2024-01-15T00:00:00Z",
            "EndTime": "2024-01-15T01:00:00Z",
            "Period": 60,
            "Statistics": [
              "Average"
            ]
          },
          "files": []
        },
        {
          "name": "Implement dual-write pattern",
          "type": "prompt",
          "prompt": "Modify application to write to both databases during migration. Implement feature flags to control read source and handle consistency between databases.",
          "parameters": {},
          "files": [
            "application/database/dual-write-adapter.py"
          ]
        },
        {
          "name": "Validate data consistency",
          "type": "cli",
          "command": "python scripts/migration/data-validator.py --source mysql://legacy-db --target postgresql://new-db --sample-size 10000",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure traffic shifting",
          "type": "integration",
          "integration": "aws",
          "method": "route53.change_resource_record_sets",
          "parameters": {
            "HostedZoneId": "Z123456",
            "ChangeBatch": {
              "Changes": [
                {
                  "Action": "UPSERT",
                  "ResourceRecordSet": {
                    "Name": "db.internal.example.com",
                    "Type": "CNAME",
                    "SetIdentifier": "Primary",
                    "Weight": 90,
                    "TTL": 60,
                    "ResourceRecords": [
                      {
                        "Value": "legacy-mysql-db.region.rds.amazonaws.com"
                      }
                    ]
                  }
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Test application with new database",
          "type": "cli",
          "command": "npm run test:integration -- --database-url postgresql://new-aurora-cluster/production",
          "parameters": {},
          "files": []
        },
        {
          "name": "Complete cutover",
          "type": "prompt",
          "prompt": "Execute final cutover by updating Route53 weights to 100% new database, stopping replication task, and updating application configuration. Monitor for any issues.",
          "parameters": {},
          "files": [
            "runbooks/database-cutover.md"
          ]
        },
        {
          "name": "Verify migration success",
          "type": "integration",
          "integration": "datadog",
          "method": "MetricsApi.query_scalar_data",
          "parameters": {
            "body": {
              "queries": [
                {
                  "data_source": "metrics",
                  "query": "avg:database.query.time{db:aurora-postgresql}",
                  "name": "query_performance"
                }
              ]
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Implement progressive canary deployment with automated rollback based on metrics",
      "steps": [
        {
          "name": "Set up Flagger operator",
          "type": "cli",
          "command": "helm install flagger flagger/flagger --namespace=istio-system --set meshProvider=istio --set metricsServer=http://prometheus:9090",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create canary resource",
          "type": "prompt",
          "prompt": "Define Flagger Canary resource with progressive traffic shifting (10% -> 50% -> 100%), success criteria based on request success rate and latency, and automated rollback triggers.",
          "parameters": {},
          "files": [
            "k8s/canary/frontend-canary.yaml"
          ]
        },
        {
          "name": "Configure custom metrics",
          "type": "integration",
          "integration": "prometheus",
          "method": "create_rule",
          "parameters": {
            "groups": [
              {
                "name": "canary_metrics",
                "rules": [
                  {
                    "record": "service:request_success_rate",
                    "expr": "sum(rate(istio_request_duration_milliseconds_bucket{response_code!~\"5..\"}[1m])) by (destination_service_name) / sum(rate(istio_request_duration_milliseconds_bucket[1m])) by (destination_service_name)"
                  }
                ]
              }
            ]
          },
          "files": []
        },
        {
          "name": "Deploy canary configuration",
          "type": "cli",
          "command": "kubectl apply -f k8s/canary/frontend-canary.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create load testing job",
          "type": "integration",
          "integration": "kubernetes",
          "method": "BatchV1Api.create_namespaced_job",
          "parameters": {
            "namespace": "default",
            "body": {
              "metadata": {
                "name": "load-test-canary"
              },
              "spec": {
                "template": {
                  "spec": {
                    "containers": [
                      {
                        "name": "load-test",
                        "image": "loadimpact/k6",
                        "args": [
                          "run",
                          "--vus",
                          "50",
                          "--duration",
                          "10m",
                          "/scripts/canary-test.js"
                        ]
                      }
                    ],
                    "restartPolicy": "Never"
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Trigger canary deployment",
          "type": "cli",
          "command": "kubectl set image deployment/frontend frontend=frontend:v2.0 -n production",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor canary progress",
          "type": "cli",
          "command": "kubectl describe canary frontend -n production",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check traffic distribution",
          "type": "integration",
          "integration": "prometheus",
          "method": "query",
          "parameters": {
            "query": "sum(rate(istio_request_total[1m])) by (destination_version)"
          },
          "files": []
        },
        {
          "name": "Verify success metrics",
          "type": "integration",
          "integration": "datadog",
          "method": "MetricsApi.query_scalar_data",
          "parameters": {
            "body": {
              "queries": [
                {
                  "data_source": "metrics",
                  "query": "avg:trace.http.request.errors{service:frontend,version:v2.0}",
                  "name": "error_rate"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Test rollback trigger",
          "type": "cli",
          "command": "kubectl exec -it $(kubectl get pod -l app=chaos-injector -o jsonpath='{.items[0].metadata.name}') -- inject-errors --service frontend --error-rate 0.3",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor rollback execution",
          "type": "integration",
          "integration": "kubernetes",
          "method": "CoreV1Api.list_namespaced_event",
          "parameters": {
            "namespace": "production",
            "field_selector": "involvedObject.name=frontend"
          },
          "files": []
        },
        {
          "name": "Verify service health post-deployment",
          "type": "cli",
          "command": "kubectl run curl-test --image=curlimages/curl --rm -it --restart=Never -- curl -s http://frontend.production:8080/health",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate deployment report",
          "type": "prompt",
          "prompt": "Analyze canary deployment metrics, document the traffic shifting pattern, success/failure criteria evaluation, and any rollbacks that occurred.",
          "parameters": {},
          "files": [
            "reports/canary-deployment-analysis.md"
          ]
        },
        {
          "name": "Update deployment runbook",
          "type": "prompt",
          "prompt": "Document the canary deployment process, metrics thresholds, rollback procedures, and troubleshooting steps for future deployments.",
          "parameters": {},
          "files": [
            "runbooks/canary-deployment.md"
          ]
        }
      ]
    },
    {
      "goal": "Create multi-cloud cost optimization platform with automated resource rightsizing",
      "steps": [
        {
          "name": "Collect AWS cost data",
          "type": "integration",
          "integration": "aws",
          "method": "ce.get_cost_and_usage_with_resources",
          "parameters": {
            "TimePeriod": {
              "Start": "2024-01-01",
              "End": "2024-01-31"
            },
            "Granularity": "DAILY",
            "Metrics": [
              "UnblendedCost",
              "UsageQuantity"
            ],
            "GroupBy": [
              {
                "Type": "DIMENSION",
                "Key": "SERVICE"
              },
              {
                "Type": "DIMENSION",
                "Key": "RESOURCE_ID"
              }
            ]
          },
          "files": []
        },
        {
          "name": "Analyze GCP billing data",
          "type": "integration",
          "integration": "gcp",
          "method": "cloudbilling.services.skus.list",
          "parameters": {
            "parent": "services/6F81-5844-456A"
          },
          "files": []
        },
        {
          "name": "Query Azure cost management",
          "type": "integration",
          "integration": "azure",
          "method": "cost_management.query.usage",
          "parameters": {
            "scope": "/subscriptions/{subscription-id}",
            "parameters": {
              "type": "Usage",
              "timeframe": "MonthToDate",
              "dataset": {
                "granularity": "Daily",
                "aggregation": {
                  "totalCost": {
                    "name": "Cost",
                    "function": "Sum"
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Deploy cost analysis Lambda",
          "type": "integration",
          "integration": "aws",
          "method": "lambda.CreateFunction",
          "parameters": {
            "FunctionName": "cost-optimizer",
            "Runtime": "python3.9",
            "Role": "arn:aws:iam::123456789012:role/cost-optimizer-role",
            "Handler": "optimizer.analyze_costs",
            "Code": {
              "S3Bucket": "lambda-code",
              "S3Key": "cost-optimizer.zip"
            },
            "Timeout": 900,
            "MemorySize": 1024
          },
          "files": []
        },
        {
          "name": "Analyze EC2 utilization patterns",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.get_metric_statistics",
          "parameters": {
            "Namespace": "AWS/EC2",
            "MetricName": "CPUUtilization",
            "Dimensions": [
              {
                "Name": "InstanceId",
                "Value": "i-1234567890abcdef0"
              }
            ],
            "StartTime": "2024-01-01T00:00:00Z",
            "EndTime": "2024-01-31T23:59:59Z",
            "Period": 3600,
            "Statistics": [
              "Average",
              "Maximum"
            ]
          },
          "files": []
        },
        {
          "name": "Create rightsizing recommendations",
          "type": "prompt",
          "prompt": "Analyze utilization data and generate rightsizing recommendations for EC2 instances, RDS databases, and container resources. Include potential cost savings calculations.",
          "parameters": {},
          "files": [
            "recommendations/rightsizing-report.json"
          ]
        },
        {
          "name": "Implement automated rightsizing",
          "type": "prompt",
          "prompt": "Create automation scripts that can safely resize resources during maintenance windows based on utilization patterns and business rules.",
          "parameters": {},
          "files": [
            "automation/rightsizing/"
          ]
        },
        {
          "name": "Configure Savings Plans recommendations",
          "type": "integration",
          "integration": "aws",
          "method": "ce.get_savings_plans_purchase_recommendation",
          "parameters": {
            "SavingsPlansType": "COMPUTE_SP",
            "TermInYears": "ONE_YEAR",
            "PaymentOption": "NO_UPFRONT",
            "LookbackPeriodInDays": "SIXTY_DAYS"
          },
          "files": []
        },
        {
          "name": "Set up automated tagging",
          "type": "integration",
          "integration": "aws",
          "method": "lambda.CreateFunction",
          "parameters": {
            "FunctionName": "auto-tagger",
            "Runtime": "python3.9",
            "Role": "arn:aws:iam::123456789012:role/auto-tagger-role",
            "Handler": "tagger.tag_resources",
            "Code": {
              "S3Bucket": "lambda-code",
              "S3Key": "auto-tagger.zip"
            }
          },
          "files": []
        },
        {
          "name": "Create cost anomaly alerts",
          "type": "integration",
          "integration": "aws",
          "method": "ce.create_anomaly_subscription",
          "parameters": {
            "AnomalySubscription": {
              "SubscriptionName": "CostAnomalyAlerts",
              "Threshold": 100.0,
              "Frequency": "DAILY",
              "MonitorArnList": [
                "${COST_MONITOR_ARN}"
              ],
              "Subscribers": [
                {
                  "Address": "cost-team@example.com",
                  "Type": "EMAIL"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Deploy cost dashboard",
          "type": "integration",
          "integration": "grafana",
          "method": "dashboard.create_dashboard",
          "parameters": {
            "dashboard": {
              "title": "Multi-Cloud Cost Optimization",
              "panels": [
                {
                  "title": "Monthly Spend Trend",
                  "targets": [
                    {
                      "expr": "sum(aws_cost_daily) by (service)"
                    }
                  ]
                },
                {
                  "title": "Rightsizing Opportunities",
                  "targets": [
                    {
                      "expr": "cost_optimization_savings_potential"
                    }
                  ]
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Test automated rightsizing",
          "type": "cli",
          "command": "python automation/rightsizing/test_resize.py --instance-id i-test --target-type t3.small --dry-run",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate cost savings",
          "type": "prompt",
          "prompt": "Compare costs before and after optimization implementations. Calculate actual savings achieved and ROI of the optimization platform.",
          "parameters": {},
          "files": [
            "reports/cost-savings-validation.md"
          ]
        }
      ]
    },
    {
      "goal": "Build high-performance caching layer with Redis cluster and cache warming strategies",
      "steps": [
        {
          "name": "Deploy Redis cluster",
          "type": "integration",
          "integration": "aws",
          "method": "elasticache.create_cache_cluster",
          "parameters": {
            "CacheClusterId": "prod-redis-cluster",
            "CacheNodeType": "cache.r6g.xlarge",
            "Engine": "redis",
            "NumCacheNodes": 3,
            "CacheSubnetGroupName": "redis-subnet-group",
            "SecurityGroupIds": [
              "sg-redis"
            ],
            "SnapshotRetentionLimit": 7
          },
          "files": []
        },
        {
          "name": "Configure Redis Sentinel",
          "type": "prompt",
          "prompt": "Set up Redis Sentinel for high availability with automatic failover. Configure quorum settings, down-after-milliseconds, and failover timeout parameters.",
          "parameters": {},
          "files": [
            "redis/sentinel.conf"
          ]
        },
        {
          "name": "Implement cache client library",
          "type": "prompt",
          "prompt": "Create cache client library with connection pooling, circuit breaker pattern, retry logic, and support for cache-aside, write-through, and write-behind patterns.",
          "parameters": {},
          "files": [
            "libraries/cache-client/"
          ]
        },
        {
          "name": "Deploy cache warming Lambda",
          "type": "integration",
          "integration": "aws",
          "method": "lambda.CreateFunction",
          "parameters": {
            "FunctionName": "cache-warmer",
            "Runtime": "python3.9",
            "Role": "arn:aws:iam::123456789012:role/cache-warmer-role",
            "Handler": "warmer.warm_cache",
            "Code": {
              "S3Bucket": "lambda-code",
              "S3Key": "cache-warmer.zip"
            },
            "Environment": {
              "Variables": {
                "REDIS_ENDPOINT": "prod-redis-cluster.cache.amazonaws.com",
                "DATABASE_ENDPOINT": "aurora-cluster.rds.amazonaws.com"
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure cache warming schedule",
          "type": "integration",
          "integration": "aws",
          "method": "events.put_rule",
          "parameters": {
            "Name": "cache-warming-schedule",
            "ScheduleExpression": "rate(30 minutes)",
            "State": "ENABLED",
            "Targets": [
              {
                "Arn": "arn:aws:lambda:us-east-2:123456789012:function:cache-warmer",
                "Id": "1"
              }
            ]
          },
          "files": []
        },
        {
          "name": "Test Redis connectivity",
          "type": "cli",
          "command": "redis-cli -h prod-redis-cluster.cache.amazonaws.com -p 6379 ping",
          "parameters": {},
          "files": []
        },
        {
          "name": "Load test cache performance",
          "type": "cli",
          "command": "redis-benchmark -h prod-redis-cluster.cache.amazonaws.com -p 6379 -c 100 -n 100000 -d 1024",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor cache hit rates",
          "type": "integration",
          "integration": "datadog",
          "method": "MetricsApi.query_scalar_data",
          "parameters": {
            "body": {
              "queries": [
                {
                  "data_source": "metrics",
                  "query": "avg:redis.cache.hit_rate{cluster:prod-redis-cluster}",
                  "name": "cache_hit_rate"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Implement cache invalidation",
          "type": "prompt",
          "prompt": "Create cache invalidation service that handles cache consistency across the cluster using pub/sub patterns and supports targeted and bulk invalidation.",
          "parameters": {},
          "files": [
            "services/cache-invalidator/"
          ]
        },
        {
          "name": "Configure memory optimization",
          "type": "cli",
          "command": "redis-cli -h prod-redis-cluster.cache.amazonaws.com CONFIG SET maxmemory-policy allkeys-lru",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test failover scenario",
          "type": "cli",
          "command": "redis-cli -h prod-redis-cluster.cache.amazonaws.com SENTINEL failover mymaster",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor memory usage",
          "type": "integration",
          "integration": "aws",
          "method": "cloudwatch.get_metric_statistics",
          "parameters": {
            "Namespace": "AWS/ElastiCache",
            "MetricName": "DatabaseMemoryUsagePercentage",
            "Dimensions": [
              {
                "Name": "CacheClusterId",
                "Value": "prod-redis-cluster"
              }
            ],
            "StartTime": "2024-01-15T00:00:00Z",
            "EndTime": "2024-01-15T01:00:00Z",
            "Period": 300,
            "Statistics": [
              "Average",
              "Maximum"
            ]
          },
          "files": []
        },
        {
          "name": "Optimize cache warming strategy",
          "type": "prompt",
          "prompt": "Analyze cache access patterns and optimize warming strategy to pre-load frequently accessed data during off-peak hours. Document cache eviction patterns.",
          "parameters": {},
          "files": [
            "reports/cache-optimization.md"
          ]
        },
        {
          "name": "Validate performance improvements",
          "type": "prompt",
          "prompt": "Compare application performance metrics before and after cache implementation. Document response time improvements and database load reduction.",
          "parameters": {},
          "files": [
            "reports/cache-performance-impact.md"
          ]
        }
      ]
    },
    {
      "goal": "Deploy and verify Step Functions state machine for automated disaster recovery workflow",
      "steps": [
        {
          "name": "Design disaster recovery state machine",
          "type": "prompt",
          "prompt": "Create a Step Functions workflow that orchestrates: health checks, automated failover decisions, DNS updates, and notification sending. Include parallel execution and error handling.",
          "parameters": {},
          "files": [
            "stepfunctions/disaster-recovery/dr-workflow.json"
          ]
        },
        {
          "name": "Deploy state machine to AWS",
          "type": "integration",
          "integration_name": "CreateStateMachine",
          "integration_params": {
            "adapter_name": "aws",
            "method": "stepfunctions.CreateStateMachine",
            "parameters": {
              "name": "disaster-recovery-orchestrator",
              "definition": "file://stepfunctions/disaster-recovery/dr-workflow.json",
              "roleArn": "arn:aws:iam::123456789012:role/StepFunctionsRole"
            }
          },
          "files": []
        },
        {
          "name": "Execute test disaster recovery scenario",
          "type": "integration",
          "integration_name": "StartSyncExecution",
          "integration_params": {
            "adapter_name": "aws",
            "method": "stepfunctions.StartSyncExecution",
            "parameters": {
              "stateMachineArn": "arn:aws:states:us-west-2:123456789012:stateMachine:disaster-recovery-orchestrator",
              "input": "{\"region\": \"us-west-2\", \"simulateFailure\": true}"
            }
          },
          "files": []
        },
        {
          "name": "Verify health check Lambda execution",
          "type": "cli",
          "command": "aws logs tail /aws/lambda/dr-health-check --since 5m --follow",
          "parameters": {},
          "files": []
        },
        {
          "name": "Confirm DNS failover occurred",
          "type": "cli",
          "command": "dig api.example.com +short && aws route53 list-resource-record-sets --hosted-zone-id Z123456 --query \"ResourceRecordSets[?Name=='api.example.com.']\"",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check CloudWatch metrics for execution",
          "type": "integration",
          "integration_name": "GetMetricStatistics",
          "integration_params": {
            "adapter_name": "aws",
            "method": "cloudwatch.GetMetricStatistics",
            "parameters": {
              "Namespace": "AWS/States",
              "MetricName": "ExecutionsFailed",
              "StartTime": "2024-01-01T00:00:00Z",
              "EndTime": "2024-01-01T01:00:00Z",
              "Period": 300,
              "Statistics": [
                "Sum"
              ]
            }
          },
          "files": []
        },
        {
          "name": "Verify notifications were sent",
          "type": "cli",
          "command": "aws sns list-subscriptions-by-topic --topic-arn arn:aws:sns:us-west-2:123456789012:dr-notifications --query 'Subscriptions[].Endpoint'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy GitHub Actions runner on ECS Fargate and verify CI/CD pipeline execution",
      "steps": [
        {
          "name": "Create self-hosted runner configuration",
          "type": "prompt",
          "prompt": "Design ECS task definition for GitHub Actions self-hosted runner with Docker-in-Docker support, proper IAM roles, and secure token management via Secrets Manager.",
          "parameters": {},
          "files": [
            "ecs/github-runner/task-definition.json"
          ]
        },
        {
          "name": "Deploy runner to ECS cluster",
          "type": "integration",
          "integration_name": "RunTask",
          "integration_params": {
            "adapter_name": "aws",
            "method": "ecs.RunTask",
            "parameters": {
              "cluster": "devops-cluster",
              "taskDefinition": "github-runner:latest",
              "launchType": "FARGATE",
              "count": 2,
              "networkConfiguration": {
                "awsvpcConfiguration": {
                  "subnets": [
                    "subnet-123",
                    "subnet-456"
                  ],
                  "securityGroups": [
                    "sg-runner"
                  ],
                  "assignPublicIp": "DISABLED"
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Verify runners registered with GitHub",
          "type": "integration",
          "integration_name": "ListSelfHostedRunnersForRepo",
          "integration_params": {
            "adapter_name": "github",
            "method": "actions.ListSelfHostedRunnersForRepo",
            "parameters": {
              "owner": "example-org",
              "repo": "devops-demo"
            }
          },
          "files": []
        },
        {
          "name": "Trigger workflow on self-hosted runner",
          "type": "integration",
          "integration_name": "CreateWorkflowDispatch",
          "integration_params": {
            "adapter_name": "github",
            "method": "actions.CreateWorkflowDispatch",
            "parameters": {
              "owner": "example-org",
              "repo": "devops-demo",
              "workflow_id": "ci-cd.yml",
              "ref": "main",
              "inputs": {
                "runner_type": "self-hosted"
              }
            }
          },
          "files": []
        },
        {
          "name": "Monitor runner task health",
          "type": "integration",
          "integration_name": "ListTasks",
          "integration_params": {
            "adapter_name": "aws",
            "method": "ecs.ListTasks",
            "parameters": {
              "cluster": "devops-cluster",
              "family": "github-runner",
              "desiredStatus": "RUNNING"
            }
          },
          "files": []
        },
        {
          "name": "Check workflow execution logs",
          "type": "cli",
          "command": "gh run list --workflow=ci-cd.yml --limit 1 --json status,conclusion,databaseId | jq -r '.[0]' && gh run view $(gh run list --workflow=ci-cd.yml --limit 1 --json databaseId -q '.[0].databaseId') --log",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy Azure AKS cluster with GitOps using Flux and verify application deployment",
      "steps": [
        {
          "name": "Create AKS cluster configuration",
          "type": "prompt",
          "prompt": "Design AKS cluster with 3 node pools (system, general, gpu), Azure AD integration, and Container Insights enabled. Configure for production workloads with availability zones.",
          "parameters": {},
          "files": [
            "terraform/azure/aks-cluster.tf"
          ]
        },
        {
          "name": "Deploy AKS cluster",
          "type": "cli",
          "command": "terraform apply -auto-approve -var-file=environments/production.tfvars",
          "parameters": {
            "working_directory": "terraform/azure/"
          },
          "files": []
        },
        {
          "name": "Install Flux v2 on cluster",
          "type": "cli",
          "command": "flux bootstrap github --owner=example-org --repository=gitops-configs --branch=main --path=./clusters/production --personal",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure Flux to sync applications",
          "type": "prompt",
          "prompt": "Create Flux Kustomization resources to sync application manifests from git repository. Include image automation for continuous deployment and notification to Slack.",
          "parameters": {},
          "files": [
            "clusters/production/apps/kustomization.yaml"
          ]
        },
        {
          "name": "Verify Flux components are running",
          "type": "cli",
          "command": "flux check --components && kubectl get pods -n flux-system",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy sample application via GitOps",
          "type": "cli",
          "command": "git add clusters/production/apps/ && git commit -m 'Deploy demo app' && git push origin main",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor Flux reconciliation",
          "type": "cli",
          "command": "flux get kustomizations --watch",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify application deployment",
          "type": "cli",
          "command": "kubectl get deployments,services,ingress -n demo-app && kubectl rollout status deployment/demo-api -n demo-app",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check application endpoint health",
          "type": "cli",
          "command": "curl -f https://demo.example.com/health || exit 1",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement ECS blue-green deployment with automated canary testing",
      "steps": [
        {
          "name": "Create ECS service with blue-green configuration",
          "type": "prompt",
          "prompt": "Configure ECS service for blue-green deployments using CodeDeploy. Set up target groups, load balancer rules, and deployment configuration with 10% traffic shifting.",
          "parameters": {},
          "files": [
            "terraform/modules/ecs/blue-green-service.tf"
          ]
        },
        {
          "name": "Deploy initial blue environment",
          "type": "integration",
          "integration_name": "CreateService",
          "integration_params": {
            "adapter_name": "aws",
            "method": "ecs.CreateService",
            "parameters": {
              "cluster": "production-cluster",
              "serviceName": "api-service-blue",
              "taskDefinition": "api:blue",
              "desiredCount": 4,
              "deploymentController": {
                "type": "CODE_DEPLOY"
              }
            }
          },
          "files": []
        },
        {
          "name": "Create canary test Lambda",
          "type": "prompt",
          "prompt": "Implement Lambda function that performs synthetic monitoring during deployments: API health checks, performance tests, and error rate monitoring. Return pass/fail for CodeDeploy hooks.",
          "parameters": {},
          "files": [
            "lambda/deployment-validation/canary-tests.py"
          ]
        },
        {
          "name": "Update task definition for green deployment",
          "type": "integration",
          "integration_name": "RegisterTaskDefinition",
          "integration_params": {
            "adapter_name": "aws",
            "method": "ecs.RegisterTaskDefinition",
            "parameters": {
              "family": "api",
              "taskRoleArn": "arn:aws:iam::123456789012:role/ecsTaskRole",
              "containerDefinitions": [
                {
                  "name": "api",
                  "image": "123456789012.dkr.ecr.us-west-2.amazonaws.com/api:green",
                  "memory": 2048,
                  "cpu": 1024
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Start blue-green deployment",
          "type": "cli",
          "command": "aws deploy create-deployment --application-name ecs-api-app --deployment-group-name production-api --revision '{\"revisionType\": \"AppSpecContent\", \"appSpecContent\": {\"content\": \"{...}\"}}' --description 'Deploy green version'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor canary traffic shift",
          "type": "cli",
          "command": "watch -n 5 'aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-west-2:123456789012:targetgroup/api-green/abc123'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify application metrics during deployment",
          "type": "integration",
          "integration_name": "Query",
          "integration_params": {
            "adapter_name": "datadog",
            "method": "metrics.Query",
            "parameters": {
              "query": "avg:ecs.service.running{service:api-service-blue} by {container_name}",
              "from": "now-30m",
              "to": "now"
            }
          },
          "files": []
        },
        {
          "name": "Validate green environment is healthy",
          "type": "cli",
          "command": "for i in {1..10}; do curl -s https://api-green.example.com/health | jq '.status' | grep -q 'healthy' || exit 1; sleep 3; done",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy multi-region disaster recovery setup with automated failover testing",
      "steps": [
        {
          "name": "Design multi-region architecture",
          "type": "prompt",
          "prompt": "Create Terraform modules for deploying identical infrastructure in us-west-2 (primary) and us-east-1 (standby). Include RDS cross-region read replicas, S3 bucket replication, and Route53 health checks.",
          "parameters": {},
          "files": [
            "terraform/modules/multi-region/main.tf"
          ]
        },
        {
          "name": "Deploy primary region infrastructure",
          "type": "cli",
          "command": "terraform workspace select us-west-2 && terraform apply -auto-approve -var='region=us-west-2' -var='role=primary'",
          "parameters": {
            "working_directory": "terraform/environments/production/"
          },
          "files": []
        },
        {
          "name": "Deploy standby region infrastructure",
          "type": "cli",
          "command": "terraform workspace select us-east-1 && terraform apply -auto-approve -var='region=us-east-1' -var='role=standby'",
          "parameters": {
            "working_directory": "terraform/environments/production/"
          },
          "files": []
        },
        {
          "name": "Configure RDS read replica promotion Lambda",
          "type": "prompt",
          "prompt": "Create Lambda function that monitors primary RDS health and automatically promotes read replica to master during outages. Include SNS notifications and CloudWatch alarms.",
          "parameters": {},
          "files": [
            "lambda/dr-automation/rds-failover.py"
          ]
        },
        {
          "name": "Set up Route53 failover routing",
          "type": "integration",
          "integration_name": "ChangeResourceRecordSets",
          "integration_params": {
            "adapter_name": "aws",
            "method": "route53.ChangeResourceRecordSets",
            "parameters": {
              "HostedZoneId": "Z123456789",
              "ChangeBatch": {
                "Changes": [
                  {
                    "Action": "UPSERT",
                    "ResourceRecordSet": {
                      "Name": "api.example.com",
                      "Type": "A",
                      "SetIdentifier": "Primary",
                      "Failover": "PRIMARY",
                      "HealthCheckId": "health-check-primary"
                    }
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Execute chaos engineering test",
          "type": "cli",
          "command": "chaos-mesh apply -f chaos/regional-failure.yaml && sleep 30",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify automatic failover occurred",
          "type": "cli",
          "command": "dig api.example.com +short | grep -q '$(aws ec2 describe-instances --region us-east-1 --filters Name=tag:Name,Values=api-standby --query \"Reservations[0].Instances[0].PublicIpAddress\" --output text)'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check application availability during failover",
          "type": "cli",
          "command": "for i in {1..60}; do curl -w '%{http_code}\\n' -o /dev/null -s https://api.example.com/health || true; sleep 1; done | grep -c 200",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate data consistency after failover",
          "type": "cli",
          "command": "aws rds describe-db-instances --region us-east-1 --db-instance-identifier api-db-replica --query 'DBInstances[0].DBInstanceStatus' | grep -q 'available'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement progressive canary deployment for serverless application with automated rollback",
      "steps": [
        {
          "name": "Create Lambda function with alias configuration",
          "type": "prompt",
          "prompt": "Design Lambda function with staging and production aliases. Configure AWS CodeDeploy for canary deployments with CloudWatch alarms for automatic rollback on errors.",
          "parameters": {},
          "files": [
            "lambda/api-gateway/handler.py",
            "lambda/api-gateway/deployment-config.json"
          ]
        },
        {
          "name": "Deploy Lambda function with initial version",
          "type": "integration",
          "integration_name": "CreateFunction",
          "integration_params": {
            "adapter_name": "aws",
            "method": "lambda.CreateFunction",
            "parameters": {
              "FunctionName": "api-handler",
              "Runtime": "python3.9",
              "Handler": "handler.main",
              "Code": {
                "S3Bucket": "deployment-artifacts",
                "S3Key": "lambda/api-handler-v1.zip"
              },
              "Publish": true
            }
          },
          "files": []
        },
        {
          "name": "Create production alias pointing to version",
          "type": "integration",
          "integration_name": "CreateAlias",
          "integration_params": {
            "adapter_name": "aws",
            "method": "lambda.CreateAlias",
            "parameters": {
              "FunctionName": "api-handler",
              "Name": "production",
              "FunctionVersion": "1",
              "Description": "Production environment"
            }
          },
          "files": []
        },
        {
          "name": "Configure CloudWatch alarms for rollback",
          "type": "integration",
          "integration_name": "PutMetricAlarm",
          "integration_params": {
            "adapter_name": "aws",
            "method": "cloudwatch.PutMetricAlarm",
            "parameters": {
              "AlarmName": "lambda-api-handler-errors",
              "MetricName": "Errors",
              "Namespace": "AWS/Lambda",
              "Statistic": "Sum",
              "Period": 60,
              "Threshold": 10,
              "ComparisonOperator": "GreaterThanThreshold"
            }
          },
          "files": []
        },
        {
          "name": "Deploy new version with canary configuration",
          "type": "cli",
          "command": "aws lambda update-function-code --function-name api-handler --s3-bucket deployment-artifacts --s3-key lambda/api-handler-v2.zip --publish",
          "parameters": {},
          "files": []
        },
        {
          "name": "Start canary deployment with 10% traffic",
          "type": "cli",
          "command": "aws codedeploy create-deployment --application-name lambda-api-app --deployment-group-name production --deployment-config-name CodeDeployDefault.LambdaCanary10PercentEvery5Minutes --revision '{\"revisionType\": \"S3\", \"s3Location\": {\"bucket\": \"deployment-artifacts\", \"key\": \"codedeploy/appspec.yaml\"}}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor canary metrics",
          "type": "integration",
          "integration_name": "GetMetricData",
          "integration_params": {
            "adapter_name": "aws",
            "method": "cloudwatch.GetMetricData",
            "parameters": {
              "MetricDataQueries": [
                {
                  "Id": "m1",
                  "MetricStat": {
                    "Metric": {
                      "Namespace": "AWS/Lambda",
                      "MetricName": "Duration",
                      "Dimensions": [
                        {
                          "Name": "FunctionName",
                          "Value": "api-handler"
                        }
                      ]
                    },
                    "Period": 300,
                    "Stat": "Average"
                  }
                }
              ],
              "StartTime": "2024-01-01T00:00:00Z",
              "EndTime": "2024-01-01T01:00:00Z"
            }
          },
          "files": []
        },
        {
          "name": "Verify canary deployment progress",
          "type": "cli",
          "command": "aws codedeploy get-deployment --deployment-id $(aws codedeploy list-deployments --application-name lambda-api-app --deployment-group-name production --query 'deployments[0]' --output text) --query 'deploymentInfo.status' --output text",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test both versions are serving traffic",
          "type": "cli",
          "command": "for i in {1..20}; do curl -s https://api.example.com/version | jq -r '.version'; done | sort | uniq -c",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy Grafana monitoring stack with automated dashboard provisioning and alert routing",
      "steps": [
        {
          "name": "Create Grafana folder structure",
          "type": "integration",
          "integration_name": "create_folder",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "folder.create_folder",
            "parameters": {
              "title": "Production Microservices",
              "uid": "prod-microservices"
            }
          },
          "files": []
        },
        {
          "name": "Add Prometheus datasource",
          "type": "integration",
          "integration_name": "add_datasource",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "datasource.add_datasource",
            "parameters": {
              "name": "Prometheus-Prod",
              "type": "prometheus",
              "url": "http://prometheus:9090",
              "access": "proxy",
              "isDefault": true
            }
          },
          "files": []
        },
        {
          "name": "Import microservices dashboard",
          "type": "integration",
          "integration_name": "create_dashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": "file://grafana/dashboards/microservices-overview.json",
              "folderId": "prod-microservices",
              "overwrite": true
            }
          },
          "files": []
        },
        {
          "name": "Create alert notification channel",
          "type": "integration",
          "integration_name": "create_contactpoint",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "alerting.create_contactpoint",
            "parameters": {
              "name": "ops-team-slack",
              "type": "slack",
              "settings": {
                "url": "https://hooks.slack.com/services/xxx/yyy/zzz",
                "channel": "#alerts-production"
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure alert rules for SLOs",
          "type": "integration",
          "integration_name": "create_alertrule",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "alerting.create_alertrule",
            "parameters": {
              "uid": "slo-availability",
              "title": "Service Availability SLO",
              "condition": "avg()",
              "data": [
                {
                  "refId": "A",
                  "queryType": "prometheus",
                  "expr": "sum(rate(http_requests_total{status!~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) < 0.999"
                }
              ],
              "folderUID": "prod-microservices"
            }
          },
          "files": []
        },
        {
          "name": "Set dashboard permissions",
          "type": "integration",
          "integration_name": "update_permissions_by_uid",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.update_permissions_by_uid",
            "parameters": {
              "uid": "microservices-overview",
              "permissions": [
                {
                  "role": "Viewer",
                  "permission": 1
                },
                {
                  "teamId": 2,
                  "permission": 2
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Test dashboard data queries",
          "type": "integration",
          "integration_name": "smartquery",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "datasource.smartquery",
            "parameters": {
              "datasourceId": 1,
              "targets": [
                {
                  "expr": "sum(rate(http_requests_total[5m])) by (service)",
                  "refId": "A"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Verify alerts are active",
          "type": "cli",
          "command": "curl -s http://localhost:3000/api/alertmanager/grafana/api/v2/alerts | jq '.[] | select(.labels.alertname==\"Service Availability SLO\")'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute Jenkins security pipeline with automated remediation and compliance reporting",
      "steps": [
        {
          "name": "Create security scan job",
          "type": "integration",
          "integration_name": "create_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.create_job",
            "parameters": {
              "name": "security-compliance-scan",
              "config_xml": "file://jenkins/jobs/security-scan-config.xml"
            }
          },
          "files": []
        },
        {
          "name": "Configure job credentials",
          "type": "integration",
          "integration_name": "create_credential",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "credential.create_credential",
            "parameters": {
              "credential_type": "usernamePassword",
              "credential_id": "sonarqube-token",
              "username": "admin",
              "password": "${SONAR_TOKEN}",
              "description": "SonarQube authentication"
            }
          },
          "files": []
        },
        {
          "name": "Trigger security scan build",
          "type": "integration",
          "integration_name": "build_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.build_job",
            "parameters": {
              "name": "security-compliance-scan",
              "parameters": {
                "BRANCH": "main",
                "SCAN_TYPE": "full",
                "FAIL_ON_HIGH": "true"
              }
            }
          },
          "files": []
        },
        {
          "name": "Monitor build progress",
          "type": "integration",
          "integration_name": "get_build_info",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "build.get_build_info",
            "parameters": {
              "name": "security-compliance-scan",
              "number": "lastBuild"
            }
          },
          "files": []
        },
        {
          "name": "Retrieve security scan results",
          "type": "integration",
          "integration_name": "get_build_console_output",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "build.get_build_console_output",
            "parameters": {
              "name": "security-compliance-scan",
              "number": "lastBuild"
            }
          },
          "files": []
        },
        {
          "name": "Check quality gate status",
          "type": "cli",
          "command": "curl -s -u admin:${SONAR_TOKEN} 'http://sonarqube:9000/api/qualitygates/project_status?projectKey=microservices' | jq -r '.projectStatus.status'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate compliance report",
          "type": "cli",
          "command": "jenkins-cli get-job security-compliance-scan | xmllint --xpath '//lastBuild/artifacts/relativePath[contains(text(),\"compliance-report\")]' - | xargs -I {} curl -O http://jenkins:8080/job/security-compliance-scan/lastBuild/artifact/{}",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create remediation tasks if needed",
          "type": "cli",
          "command": "if [ $(jq '.vulnerabilities.high' security-scan-results.json) -gt 0 ]; then gh issue create --title 'High severity vulnerabilities found' --body \"$(jq '.vulnerabilities' security-scan-results.json)\"; fi",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement Elasticsearch observability pipeline with real-time anomaly detection",
      "steps": [
        {
          "name": "Create application log index template",
          "type": "integration",
          "integration_name": "indices.put_index_template",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "indices.put_index_template",
            "parameters": {
              "name": "logs-app",
              "index_patterns": [
                "logs-app-*"
              ],
              "template": {
                "settings": {
                  "number_of_shards": 3,
                  "number_of_replicas": 1,
                  "index.lifecycle.name": "logs-policy"
                },
                "mappings": {
                  "properties": {
                    "@timestamp": {
                      "type": "date"
                    },
                    "level": {
                      "type": "keyword"
                    },
                    "service": {
                      "type": "keyword"
                    },
                    "message": {
                      "type": "text"
                    }
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Configure data stream",
          "type": "integration",
          "integration_name": "indices.create_data_stream",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "indices.create_data_stream",
            "parameters": {
              "name": "logs-app-production"
            }
          },
          "files": []
        },
        {
          "name": "Set up ML anomaly detection job",
          "type": "cli",
          "command": "curl -X PUT 'localhost:9200/_ml/anomaly_detectors/app-error-rate' -H 'Content-Type: application/json' -d '{\"analysis_config\": {\"bucket_span\": \"5m\", \"detectors\": [{\"function\": \"high_count\", \"partition_field_name\": \"service\", \"detector_description\": \"High error rate by service\"}]}, \"data_description\": {\"time_field\": \"@timestamp\"}}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Start real-time log ingestion test",
          "type": "cli",
          "command": "for i in {1..100}; do echo '{\"@timestamp\": \"'$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)'\", \"level\": \"ERROR\", \"service\": \"payment-api\", \"message\": \"Payment processing failed\"}' | curl -X POST 'localhost:9200/logs-app-production/_doc' -H 'Content-Type: application/json' -d @-; sleep 0.1; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Execute complex search query",
          "type": "integration",
          "integration_name": "msearch",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "msearch",
            "parameters": {
              "body": [
                {
                  "index": "logs-app-*"
                },
                {
                  "query": {
                    "bool": {
                      "must": [
                        {
                          "term": {
                            "level": "ERROR"
                          }
                        },
                        {
                          "range": {
                            "@timestamp": {
                              "gte": "now-1h"
                            }
                          }
                        }
                      ]
                    }
                  },
                  "aggs": {
                    "errors_by_service": {
                      "terms": {
                        "field": "service",
                        "size": 10
                      }
                    }
                  }
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Check index lifecycle status",
          "type": "integration",
          "integration_name": "indices.explain_data_lifecycle",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "indices.explain_data_lifecycle",
            "parameters": {
              "index": "logs-app-production"
            }
          },
          "files": []
        },
        {
          "name": "Verify ML job detected anomalies",
          "type": "cli",
          "command": "curl -X GET 'localhost:9200/_ml/anomaly_detectors/app-error-rate/results/records?start=now-1h&end=now' | jq '.records[] | select(.record_score > 75)'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate observability report",
          "type": "integration",
          "integration_name": "search",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "search",
            "parameters": {
              "index": "logs-app-*",
              "body": {
                "size": 0,
                "aggs": {
                  "service_stats": {
                    "terms": {
                      "field": "service"
                    },
                    "aggs": {
                      "error_rate": {
                        "filters": {
                          "filters": {
                            "errors": {
                              "term": {
                                "level": "ERROR"
                              }
                            },
                            "total": {
                              "match_all": {}
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Orchestrate multi-stage Jenkins pipeline for microservices deployment with canary analysis",
      "steps": [
        {
          "name": "Create microservices deployment pipeline",
          "type": "prompt",
          "prompt": "Design Jenkins pipeline that builds multiple microservices in parallel, runs integration tests, deploys to staging, performs canary deployment to production with automated rollback.",
          "parameters": {},
          "files": [
            "jenkins/pipelines/microservices-deploy.Jenkinsfile"
          ]
        },
        {
          "name": "Configure pipeline job in Jenkins",
          "type": "integration",
          "integration_name": "create_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.create_job",
            "parameters": {
              "name": "microservices-deployment",
              "config_xml": "<flow-definition plugin=\"workflow-job\"><definition class=\"org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition\"><scm class=\"hudson.plugins.git.GitSCM\"><configVersion>2</configVersion><userRemoteConfigs><hudson.plugins.git.UserRemoteConfig><url>https://github.com/example/microservices.git</url></hudson.plugins.git.UserRemoteConfig></userRemoteConfigs><branches><hudson.plugins.git.BranchSpec><name>*/main</name></hudson.plugins.git.BranchSpec></branches></scm><scriptPath>jenkins/pipelines/microservices-deploy.Jenkinsfile</scriptPath></definition></flow-definition>"
            }
          },
          "files": []
        },
        {
          "name": "Trigger deployment pipeline",
          "type": "integration",
          "integration_name": "build_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.build_job",
            "parameters": {
              "name": "microservices-deployment",
              "parameters": {
                "DEPLOY_ENV": "staging",
                "SERVICES": "auth-api,payment-api,notification-api",
                "CANARY_PERCENTAGE": "10"
              }
            }
          },
          "files": []
        },
        {
          "name": "Monitor build stages progress",
          "type": "integration",
          "integration_name": "get_build_stages",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "build.get_build_stages",
            "parameters": {
              "name": "microservices-deployment",
              "number": "lastBuild"
            }
          },
          "files": []
        },
        {
          "name": "Run canary analysis in Grafana",
          "type": "integration",
          "integration_name": "smartquery",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "datasource.smartquery",
            "parameters": {
              "datasourceId": 1,
              "targets": [
                {
                  "expr": "sum(rate(http_requests_total{version=\"canary\"}[5m])) / sum(rate(http_requests_total[5m]))",
                  "refId": "canary_traffic"
                },
                {
                  "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{version=\"canary\"}[5m]))",
                  "refId": "canary_latency"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Check canary deployment health",
          "type": "cli",
          "command": "kubectl get deployments -l version=canary -o json | jq '.items[] | {name: .metadata.name, ready: .status.readyReplicas, desired: .spec.replicas}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Decide on canary promotion",
          "type": "cli",
          "command": "ERROR_RATE=$(curl -s 'http://prometheus:9090/api/v1/query?query=sum(rate(http_requests_total{status=~\"5..\",version=\"canary\"}[5m]))/sum(rate(http_requests_total{version=\"canary\"}[5m]))' | jq -r '.data.result[0].value[1]'); if (( $(echo \"$ERROR_RATE < 0.01\" | bc -l) )); then echo 'Canary healthy, promoting'; else echo 'Canary unhealthy, rolling back' && exit 1; fi",
          "parameters": {},
          "files": []
        },
        {
          "name": "Complete deployment or rollback",
          "type": "integration",
          "integration_name": "build_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.build_job",
            "parameters": {
              "name": "microservices-deployment",
              "parameters": {
                "ACTION": "promote-canary",
                "CANARY_BUILD": "${BUILD_NUMBER}"
              }
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy Grafana team-based access control with automated onboarding",
      "steps": [
        {
          "name": "Create organizational teams",
          "type": "integration",
          "integration_name": "add_team",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "teams.add_team",
            "parameters": {
              "name": "Platform Engineering",
              "email": "platform@example.com"
            }
          },
          "files": []
        },
        {
          "name": "Add team members",
          "type": "integration",
          "integration_name": "add_team_member",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "teams.add_team_member",
            "parameters": {
              "team_id": 1,
              "user_id": 5
            }
          },
          "files": []
        },
        {
          "name": "Create team-specific folders",
          "type": "integration",
          "integration_name": "create_folder",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "folder.create_folder",
            "parameters": {
              "title": "Platform Engineering Dashboards",
              "uid": "platform-eng"
            }
          },
          "files": []
        },
        {
          "name": "Set folder permissions for team",
          "type": "integration",
          "integration_name": "update_folder_permissions",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "folder.update_folder_permissions",
            "parameters": {
              "uid": "platform-eng",
              "permissions": [
                {
                  "teamId": 1,
                  "permission": 2
                },
                {
                  "role": "Viewer",
                  "permission": 1
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Import team dashboard",
          "type": "cli",
          "command": "curl -X POST 'http://admin:admin@localhost:3000/api/dashboards/db' -H 'Content-Type: application/json' -d '{\"dashboard\": {\"title\": \"Platform Metrics\", \"panels\": []}, \"folderId\": 1, \"overwrite\": true}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure team preferences",
          "type": "integration",
          "integration_name": "update_team_preferences",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "teams.update_team_preferences",
            "parameters": {
              "team_id": 1,
              "theme": "dark",
              "homeDashboardId": 10,
              "timezone": "utc"
            }
          },
          "files": []
        },
        {
          "name": "Verify team access",
          "type": "integration",
          "integration_name": "get_team_members",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "teams.get_team_members",
            "parameters": {
              "team_id": 1
            }
          },
          "files": []
        },
        {
          "name": "Test dashboard access with team member",
          "type": "cli",
          "command": "curl -H 'Authorization: Bearer ${TEAM_MEMBER_TOKEN}' 'http://localhost:3000/api/dashboards/uid/platform-metrics' | jq -r '.dashboard.title'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute chaos engineering experiments with automated recovery validation",
      "steps": [
        {
          "name": "Install Litmus chaos operator",
          "type": "cli",
          "command": "kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v2.14.0.yaml && kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=operator -n litmus --timeout=120s",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy chaos experiments",
          "type": "cli",
          "command": "kubectl apply -f chaos/experiments/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Start pod delete chaos experiment",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: pod-delete-chaos\n  namespace: default\nspec:\n  appinfo:\n    appns: production\n    applabel: 'app=payment-api'\n  chaosServiceAccount: litmus-admin\n  experiments:\n  - name: pod-delete\n    spec:\n      components:\n        env:\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: FORCE\n          value: 'true'\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor application health during chaos",
          "type": "cli",
          "command": "for i in {1..12}; do echo \"Check $i:\"; curl -s -o /dev/null -w '%{http_code}' http://payment-api:8080/health; echo; kubectl get pods -l app=payment-api --no-headers | wc -l; sleep 5; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Query Grafana for service metrics",
          "type": "integration",
          "integration_name": "smartquery",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "datasource.smartquery",
            "parameters": {
              "datasourceId": 1,
              "targets": [
                {
                  "expr": "up{job=\"payment-api\"}",
                  "refId": "availability"
                },
                {
                  "expr": "rate(http_requests_total{job=\"payment-api\",status!~\"5..\"}[1m])",
                  "refId": "success_rate"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Verify auto-scaling responded",
          "type": "cli",
          "command": "kubectl get hpa payment-api-hpa -o json | jq '{current: .status.currentReplicas, desired: .status.desiredReplicas, metrics: .status.currentMetrics}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check chaos experiment results",
          "type": "cli",
          "command": "kubectl get chaosresult pod-delete-chaos-pod-delete -o json | jq '.status.experimentStatus'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate chaos report",
          "type": "cli",
          "command": "kubectl logs -l chaosUID=$(kubectl get chaosengine pod-delete-chaos -o json | jq -r '.metadata.uid') --tail=100 | grep -E 'verdict|probe|summary' > chaos-report.txt && cat chaos-report.txt",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy Consul service mesh with zero-downtime migration of microservices",
      "steps": [
        {
          "name": "Register services with Consul",
          "type": "integration",
          "integration_name": "RegisterService",
          "integration_params": {
            "adapter_name": "consul",
            "method": "agent.service.register",
            "parameters": {
              "name": "payment-api",
              "port": 8080,
              "tags": [
                "production",
                "v2.0"
              ],
              "check": {
                "http": "http://localhost:8080/health",
                "interval": "10s",
                "timeout": "5s"
              },
              "connect": {
                "sidecar_service": {}
              }
            }
          },
          "files": []
        },
        {
          "name": "Verify service registration",
          "type": "integration",
          "integration_name": "ListServices",
          "integration_params": {
            "adapter_name": "consul",
            "method": "catalog.services",
            "parameters": {
              "filter": "ServiceName == payment-api"
            }
          },
          "files": []
        },
        {
          "name": "Create service intentions",
          "type": "cli",
          "command": "consul intention create -allow auth-api payment-api && consul intention create -allow payment-api notification-api",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Envoy sidecars",
          "type": "cli",
          "command": "kubectl apply -f consul/k8s/consul-inject-deployment.yaml && kubectl rollout status deployment/payment-api",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check service health",
          "type": "integration",
          "integration_name": "ServiceHealth",
          "integration_params": {
            "adapter_name": "consul",
            "method": "health.service",
            "parameters": {
              "service": "payment-api",
              "passing": true
            }
          },
          "files": []
        },
        {
          "name": "Test service mesh communication",
          "type": "cli",
          "command": "kubectl exec deployment/auth-api -c auth-api -- curl -s http://payment-api.service.consul:8080/api/health | jq '.mesh_enabled'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor service mesh metrics",
          "type": "cli",
          "command": "consul monitor -log-level=debug | grep -E 'payment-api.*intention' &",
          "parameters": {},
          "files": []
        },
        {
          "name": "Perform traffic shifting",
          "type": "cli",
          "command": "consul config write - <<EOF\nKind: service-splitter\nName: payment-api\nSplits:\n  - Weight: 90\n    ServiceSubset: v1\n  - Weight: 10\n    ServiceSubset: v2\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify zero-downtime migration",
          "type": "cli",
          "command": "while true; do curl -s -o /dev/null -w '%{http_code}' http://payment-api:8080/health || echo 'Request failed'; sleep 0.5; done | head -20 | grep -c 200",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Provision GCP infrastructure with automated scaling based on custom metrics",
      "steps": [
        {
          "name": "Create GCP instance template",
          "type": "integration",
          "integration_name": "CreateInstanceTemplate",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instanceTemplates.insert",
            "parameters": {
              "project": "devops-demo-prod",
              "body": {
                "name": "app-server-template-v2",
                "properties": {
                  "machineType": "n2-standard-4",
                  "disks": [
                    {
                      "boot": true,
                      "initializeParams": {
                        "sourceImage": "projects/ubuntu-os-cloud/global/images/family/ubuntu-2204-lts"
                      }
                    }
                  ],
                  "networkInterfaces": [
                    {
                      "network": "global/networks/prod-vpc",
                      "subnetwork": "regions/us-central1/subnetworks/app-subnet"
                    }
                  ],
                  "metadata": {
                    "items": [
                      {
                        "key": "startup-script",
                        "value": "#!/bin/bash\ncurl -sSO https://dl.google.com/cloudagents/install-monitoring-agent.sh\nsudo bash install-monitoring-agent.sh"
                      }
                    ]
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Create managed instance group",
          "type": "cli",
          "command": "gcloud compute instance-groups managed create app-server-mig --template=app-server-template-v2 --size=3 --zone=us-central1-a --health-check=app-health-check --initial-delay=300",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure autoscaling with custom metrics",
          "type": "cli",
          "command": "gcloud compute instance-groups managed set-autoscaling app-server-mig --zone=us-central1-a --max-num-replicas=10 --min-num-replicas=3 --custom-metric-utilization metric=custom.googleapis.com/application/request_queue_depth,utilization-target=100",
          "parameters": {},
          "files": []
        },
        {
          "name": "List running instances",
          "type": "integration",
          "integration_name": "ListInstances",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instances.list",
            "parameters": {
              "project": "devops-demo-prod",
              "zone": "us-central1-a",
              "filter": "name:app-server-mig-*"
            }
          },
          "files": []
        },
        {
          "name": "Generate load to trigger scaling",
          "type": "cli",
          "command": "hey -n 10000 -c 100 -q 100 http://$(gcloud compute forwarding-rules describe app-lb-forwarding-rule --global --format='value(IPAddress)')/api/stress",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor autoscaling events",
          "type": "cli",
          "command": "gcloud logging read 'resource.type=\"gce_instance_group\" AND jsonPayload.event_subtype=\"compute.autoscalers.update\"' --limit=10 --format=json | jq '.[].jsonPayload'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify new instances are healthy",
          "type": "integration",
          "integration_name": "GetInstanceHealth",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instances.get",
            "parameters": {
              "project": "devops-demo-prod",
              "zone": "us-central1-a",
              "instance": "app-server-mig-xyz"
            }
          },
          "files": []
        },
        {
          "name": "Check custom metric values",
          "type": "cli",
          "command": "gcloud monitoring time-series list --filter='metric.type=\"custom.googleapis.com/application/request_queue_depth\"' --format='table(metric.labels.instance_name,points[0].value.int64_value,points[0].interval.end_time)'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement data pipeline with Airflow including real-time monitoring and alerting",
      "steps": [
        {
          "name": "Deploy Airflow DAG",
          "type": "cli",
          "command": "cp airflow/dags/data_pipeline_dag.py $AIRFLOW_HOME/dags/ && airflow dags unpause data_pipeline",
          "parameters": {},
          "files": []
        },
        {
          "name": "Trigger pipeline execution",
          "type": "cli",
          "command": "airflow dags trigger data_pipeline --conf '{\"source_date\": \"2024-01-15\", \"batch_size\": 10000}'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor DAG execution",
          "type": "cli",
          "command": "airflow dags state data_pipeline $(date +%Y-%m-%d) && airflow tasks states-for-dag-run data_pipeline data_pipeline__$(date +%Y-%m-%d)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check data quality metrics",
          "type": "cli",
          "command": "airflow tasks test data_pipeline data_quality_check $(date +%Y-%m-%d) 2>&1 | grep -E 'Quality score:|Records processed:'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Query processed data in BigQuery",
          "type": "cli",
          "command": "bq query --use_legacy_sql=false 'SELECT COUNT(*) as total_records, AVG(processing_time) as avg_time, MAX(created_at) as latest_record FROM `project.dataset.processed_data` WHERE DATE(created_at) = CURRENT_DATE()'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Set up Airflow alerts",
          "type": "cli",
          "command": "airflow connections add 'slack_alerts' --conn-type 'http' --conn-host 'https://hooks.slack.com' --conn-password '/services/XXX/YYY/ZZZ'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test SLA violation handling",
          "type": "cli",
          "command": "airflow dags test data_pipeline --execution-date $(date -d '1 hour ago' +%Y-%m-%dT%H:%M:%S)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate pipeline performance report",
          "type": "cli",
          "command": "airflow task_instances list --dag-id data_pipeline --state success --start-date $(date -d '7 days ago' +%Y-%m-%d) | awk '{print $4, $5, $6}' | sort | uniq -c",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy service mesh observability with distributed tracing and custom dashboards",
      "steps": [
        {
          "name": "Install Jaeger tracing",
          "type": "cli",
          "command": "kubectl create namespace observability && helm install jaeger jaegertracing/jaeger --namespace observability --set collector.service.type=LoadBalancer",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure service mesh for tracing",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: envoy-tracing-config\ndata:\n  envoy.yaml: |\n    tracing:\n      http:\n        name: envoy.tracers.zipkin\n        typed_config:\n          \"@type\": type.googleapis.com/envoy.config.trace.v3.ZipkinConfig\n          collector_cluster: jaeger-collector\n          collector_endpoint: \"/api/v2/spans\"\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Query service dependencies",
          "type": "integration",
          "integration_name": "GetServiceConnections",
          "integration_params": {
            "adapter_name": "consul",
            "method": "catalog.service",
            "parameters": {
              "service": "payment-api",
              "dc": "dc1"
            }
          },
          "files": []
        },
        {
          "name": "Create Grafana tracing dashboard",
          "type": "integration",
          "integration_name": "CreateDashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": {
                "title": "Service Mesh Tracing",
                "panels": [
                  {
                    "title": "Request Flow",
                    "type": "jaeger",
                    "targets": [
                      {
                        "query": "service=\"payment-api\" AND operation=\"POST /api/payment\""
                      }
                    ]
                  }
                ]
              },
              "folderId": 0
            }
          },
          "files": []
        },
        {
          "name": "Generate trace data",
          "type": "cli",
          "command": "for i in {1..50}; do curl -H 'X-B3-TraceId: $(openssl rand -hex 16)' -H 'X-B3-SpanId: $(openssl rand -hex 8)' -H 'X-B3-Sampled: 1' http://api-gateway/api/v1/payment -d '{\"amount\": 100}' &; done; wait",
          "parameters": {},
          "files": []
        },
        {
          "name": "Query trace metrics",
          "type": "cli",
          "command": "curl -s 'http://jaeger-query:16686/api/traces?service=payment-api&limit=20' | jq '.data[].spans | group_by(.operationName) | map({operation: .[0].operationName, count: length, avg_duration: (map(.duration) | add/length)})'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check service mesh health metrics",
          "type": "integration",
          "integration_name": "ServiceHealthStats",
          "integration_params": {
            "adapter_name": "consul",
            "method": "health.service",
            "parameters": {
              "service": "payment-api",
              "passing": true,
              "near": "_agent"
            }
          },
          "files": []
        },
        {
          "name": "Analyze trace anomalies",
          "type": "cli",
          "command": "curl -s 'http://jaeger-query:16686/api/traces?service=payment-api&tags={\"error\":\"true\"}' | jq '.data[] | {traceID, duration: (.spans | map(.duration) | add), errorCount: (.spans | map(select(.tags[]?.value == \"true\" and .tags[]?.key == \"error\")) | length)}'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement cost optimization with automated GCP resource management",
      "steps": [
        {
          "name": "Analyze current resource usage",
          "type": "cli",
          "command": "gcloud compute instances list --format='table(name,machineType,status,zone)' --filter='status=RUNNING' | tee current-instances.txt",
          "parameters": {},
          "files": []
        },
        {
          "name": "Identify idle instances",
          "type": "integration",
          "integration_name": "ListInstances",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instances.list",
            "parameters": {
              "project": "devops-demo-prod",
              "filter": "status=RUNNING AND labels.environment=development"
            }
          },
          "files": []
        },
        {
          "name": "Create snapshot before optimization",
          "type": "cli",
          "command": "for instance in $(gcloud compute instances list --filter='labels.environment=development' --format='value(name,zone)'); do name=$(echo $instance | cut -d' ' -f1); zone=$(echo $instance | cut -d' ' -f2); gcloud compute disks snapshot ${name} --zone=${zone} --snapshot-names=${name}-backup-$(date +%Y%m%d); done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Stop development instances",
          "type": "integration",
          "integration_name": "StopInstance",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instances.stop",
            "parameters": {
              "project": "devops-demo-prod",
              "zone": "us-central1-a",
              "instance": "dev-server-1"
            }
          },
          "files": []
        },
        {
          "name": "Implement instance scheduling",
          "type": "cli",
          "command": "gcloud scheduler jobs create pubsub start-dev-instances --schedule='0 8 * * MON-FRI' --topic=instance-scheduler --message-body='{\"action\":\"start\",\"tags\":[\"environment:development\"]}' --time-zone='America/New_York'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Resize overprovisioned instances",
          "type": "integration",
          "integration_name": "SetMachineType",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instances.setMachineType",
            "parameters": {
              "project": "devops-demo-prod",
              "zone": "us-central1-a",
              "instance": "prod-server-1",
              "body": {
                "machineType": "zones/us-central1-a/machineTypes/n2-standard-2"
              }
            }
          },
          "files": []
        },
        {
          "name": "Enable committed use discounts",
          "type": "cli",
          "command": "gcloud compute commitments create prod-commitment --plan=12-month --resources=vcpu=100,memory=400GB --region=us-central1",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate cost optimization report",
          "type": "cli",
          "command": "bq query --use_legacy_sql=false 'SELECT service.description, SUM(cost) as total_cost, SUM(CASE WHEN labels.key=\"environment\" AND labels.value=\"development\" THEN cost ELSE 0 END) as dev_cost FROM `billing_dataset.gcp_billing_export_v1` WHERE DATE(_PARTITIONTIME) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY) GROUP BY 1 ORDER BY 2 DESC LIMIT 10'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy API gateway with dynamic routing and real-time configuration updates",
      "steps": [
        {
          "name": "Deploy Kong API Gateway",
          "type": "cli",
          "command": "helm install kong kong/kong --set proxy.type=LoadBalancer --set admin.enabled=true --set admin.type=NodePort --set postgresql.enabled=true --namespace kong --create-namespace",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure upstream services",
          "type": "cli",
          "command": "curl -X POST http://localhost:8001/services -d 'name=payment-service' -d 'url=http://payment-api.default.svc.cluster.local:8080'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create API routes with plugins",
          "type": "cli",
          "command": "curl -X POST http://localhost:8001/services/payment-service/routes -d 'paths[]=/api/v1/payment' -d 'methods[]=POST' -d 'methods[]=GET' && curl -X POST http://localhost:8001/routes/$(curl -s http://localhost:8001/routes | jq -r '.data[0].id')/plugins -d 'name=rate-limiting' -d 'config.minute=100'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Enable authentication plugin",
          "type": "cli",
          "command": "curl -X POST http://localhost:8001/plugins -d 'name=jwt' -d 'config.secret_is_base64=false' -d 'config.claims_to_verify[]=exp'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test API gateway routing",
          "type": "cli",
          "command": "TOKEN=$(jwt encode --secret 'my-secret-key' --alg HS256 '{}'); curl -H \"Authorization: Bearer $TOKEN\" http://$(kubectl get svc kong-proxy -n kong -o jsonpath='{.status.loadBalancer.ingress[0].ip}')/api/v1/payment",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure canary routing",
          "type": "cli",
          "command": "curl -X POST http://localhost:8001/services/payment-service/plugins -d 'name=canary' -d 'config.percentage=20' -d 'config.upstream_host=payment-api-v2.default.svc.cluster.local' -d 'config.upstream_port=8080'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor API gateway metrics",
          "type": "cli",
          "command": "curl -s http://localhost:8001/metrics | grep -E 'kong_http_status|kong_latency_bucket'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update route configuration dynamically",
          "type": "cli",
          "command": "curl -X PATCH http://localhost:8001/routes/$(curl -s http://localhost:8001/routes | jq -r '.data[0].id') -d 'paths[]=/api/v2/payment' -d 'strip_path=true'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify configuration changes",
          "type": "cli",
          "command": "curl -s http://localhost:8001/routes | jq '.data[] | {id, paths, strip_path, service: .service.name}'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute disaster recovery drill with automated failover and validation",
      "steps": [
        {
          "name": "Initiate backup of production data",
          "type": "cli",
          "command": "bash backup/scripts/database-backup.sh --environment prod --destination s3://dr-backups/$(date +%Y%m%d)/ --encrypt",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify backup integrity",
          "type": "cli",
          "command": "aws s3api head-object --bucket dr-backups --key $(date +%Y%m%d)/postgres-backup.sql.gz.enc --query 'Metadata.sha256sum' | xargs -I {} bash -c 'echo {} | sha256sum -c'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Trigger regional failover",
          "type": "integration",
          "integration_name": "StartExecution",
          "integration_params": {
            "adapter_name": "aws",
            "method": "stepfunctions.StartExecution",
            "parameters": {
              "stateMachineArn": "arn:aws:states:us-east-1:123456789012:stateMachine:disaster-recovery",
              "input": "{\"sourceRegion\": \"us-west-2\", \"targetRegion\": \"us-east-1\", \"mode\": \"drill\"}"
            }
          },
          "files": []
        },
        {
          "name": "Promote RDS read replica",
          "type": "cli",
          "command": "aws rds promote-read-replica --db-instance-identifier prod-db-replica-east --backup-retention-period 7",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update DNS for failover",
          "type": "integration",
          "integration_name": "ChangeResourceRecordSets",
          "integration_params": {
            "adapter_name": "aws",
            "method": "route53.ChangeResourceRecordSets",
            "parameters": {
              "HostedZoneId": "Z123456789",
              "ChangeBatch": {
                "Changes": [
                  {
                    "Action": "UPSERT",
                    "ResourceRecordSet": {
                      "Name": "api.example.com",
                      "Type": "A",
                      "AliasTarget": {
                        "HostedZoneId": "Z2FDTNDATAQYW2",
                        "DNSName": "dr-alb-east.elb.amazonaws.com"
                      }
                    }
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Validate application health in DR region",
          "type": "cli",
          "command": "for i in {1..30}; do curl -s https://api.example.com/health | jq -r '.status' | grep -q 'healthy' && echo 'Health check $i: PASS' || echo 'Health check $i: FAIL'; sleep 10; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run smoke tests on DR environment",
          "type": "cli",
          "command": "newman run postman/collections/smoke-tests.json --environment postman/environments/disaster-recovery.json --reporters cli,json --reporter-json-export dr-test-results.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate DR drill report",
          "type": "cli",
          "command": "python disaster-recovery/scripts/generate-report.py --start-time $(date -d '30 minutes ago' +%s) --test-results dr-test-results.json --output dr-drill-report-$(date +%Y%m%d).pdf",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy zero-trust network architecture with continuous security validation",
      "steps": [
        {
          "name": "Deploy network policies",
          "type": "cli",
          "command": "kubectl apply -f k8s/network-policies/ && kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=network-policy-controller --timeout=120s",
          "parameters": {},
          "files": []
        },
        {
          "name": "Enable Istio strict mTLS",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create authorization policies",
          "type": "cli",
          "command": "kubectl apply -f istio/security/authorization-policies/ && kubectl get authorizationpolicies -A",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Falco for runtime security",
          "type": "cli",
          "command": "helm install falco falcosecurity/falco --set falco.grpc.enabled=true --set falco.grpc_output.enabled=true --namespace falco-system --create-namespace",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test network isolation",
          "type": "cli",
          "command": "kubectl run test-isolation --image=nicolaka/netshoot --rm -it --restart=Never -- bash -c 'for svc in payment-api auth-api notification-api; do echo \"Testing $svc:\"; curl -m 2 http://$svc:8080/health 2>&1 | grep -E \"refused|timeout|200\" || echo \"BLOCKED\"; done'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor security events",
          "type": "cli",
          "command": "kubectl logs -n falco-system -l app=falco --tail=50 | grep -E 'Warning|Error|Critical' | jq -R 'fromjson? | select(.priority==\"Warning\" or .priority==\"Error\")'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run compliance validation",
          "type": "cli",
          "command": "kubectl apply -f compliance/validation-job.yaml && kubectl wait --for=condition=complete job/compliance-validator --timeout=600s && kubectl logs job/compliance-validator",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate security posture report",
          "type": "integration",
          "integration_name": "CreateDashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": {
                "title": "Security Posture",
                "panels": [
                  {
                    "title": "Network Policy Violations",
                    "type": "graph",
                    "targets": [
                      {
                        "expr": "sum(rate(network_policy_violations_total[5m])) by (source_pod, destination_pod)"
                      }
                    ]
                  }
                ]
              }
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Implement edge computing infrastructure with real-time data processing",
      "steps": [
        {
          "name": "Deploy K3s edge cluster",
          "type": "cli",
          "command": "curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.25.0+k3s1 sh -s - server --disable traefik --disable servicelb --write-kubeconfig-mode 644",
          "parameters": {},
          "files": []
        },
        {
          "name": "Install edge data processing stack",
          "type": "cli",
          "command": "helm install nifi apache-nifi/nifi --set service.type=ClusterIP --namespace edge-processing --create-namespace && kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=nifi --timeout=300s",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure IoT device connections",
          "type": "cli",
          "command": "mosquitto_passwd -c /etc/mosquitto/passwd iot-gateway && docker run -d -p 1883:1883 -p 9001:9001 -v /etc/mosquitto:/mosquitto/config eclipse-mosquitto",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy TimescaleDB for edge storage",
          "type": "cli",
          "command": "kubectl apply -f edge/timescale/timescaledb-deployment.yaml && kubectl exec -it timescaledb-0 -- psql -U postgres -c 'CREATE EXTENSION IF NOT EXISTS timescaledb;'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Start data ingestion from IoT devices",
          "type": "cli",
          "command": "python edge/scripts/iot-simulator.py --devices 50 --interval 1 --mqtt-host localhost --mqtt-topic sensors/data &",
          "parameters": {},
          "files": []
        },
        {
          "name": "Process real-time data stream",
          "type": "cli",
          "command": "kubectl exec -n edge-processing deployment/nifi -- curl -X POST http://localhost:8080/nifi-api/process-groups/root/processors -H 'Content-Type: application/json' -d @/config/mqtt-processor.json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor edge metrics",
          "type": "cli",
          "command": "curl -s http://localhost:9090/api/v1/query?query='rate(edge_data_processed_total[5m])' | jq '.data.result[0].value[1]' && echo ' messages/sec'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Sync processed data to cloud",
          "type": "integration",
          "integration_name": "PutRecord",
          "integration_params": {
            "adapter_name": "aws",
            "method": "kinesis.PutRecord",
            "parameters": {
              "StreamName": "edge-data-stream",
              "Data": "{\"timestamp\": \"2024-01-15T10:00:00Z\", \"device_count\": 50, \"avg_temperature\": 22.5, \"anomalies\": 2}",
              "PartitionKey": "edge-location-1"
            }
          },
          "files": []
        },
        {
          "name": "Validate edge-to-cloud pipeline",
          "type": "cli",
          "command": "aws kinesis describe-stream --stream-name edge-data-stream --query 'StreamDescription.StreamStatus' | grep -q ACTIVE && echo 'Cloud sync operational'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute automated compliance validation with evidence collection",
      "steps": [
        {
          "name": "Initialize compliance scan",
          "type": "cli",
          "command": "docker run --rm -v $(pwd):/workspace -w /workspace chef/inspec exec compliance/inspec --reporter json:compliance-scan.json html:compliance-report.html",
          "parameters": {},
          "files": []
        },
        {
          "name": "Collect infrastructure evidence",
          "type": "cli",
          "command": "python compliance/scripts/evidence-collector.py --providers aws,gcp,k8s --output evidence/$(date +%Y%m%d)/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Scan for PII in logs",
          "type": "integration",
          "integration_name": "Search",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "search",
            "parameters": {
              "index": "logs-*",
              "body": {
                "query": {
                  "bool": {
                    "should": [
                      {
                        "regexp": {
                          "message": "[0-9]{3}-[0-9]{2}-[0-9]{4}"
                        }
                      },
                      {
                        "regexp": {
                          "message": "[0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9]{4}"
                        }
                      },
                      {
                        "match": {
                          "message": "email"
                        }
                      }
                    ]
                  }
                },
                "size": 100
              }
            }
          },
          "files": []
        },
        {
          "name": "Validate encryption at rest",
          "type": "cli",
          "command": "aws s3api get-bucket-encryption --bucket prod-data-bucket && aws rds describe-db-instances --query 'DBInstances[*].[DBInstanceIdentifier,StorageEncrypted]' --output table",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check access control policies",
          "type": "cli",
          "command": "aws iam get-account-password-policy && kubectl auth can-i --list --namespace=production | grep -E 'create|delete|update' | wc -l",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate compliance dashboard",
          "type": "integration",
          "integration_name": "CreateDashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": {
                "title": "Compliance Status",
                "panels": [
                  {
                    "title": "SOC2 Control Status",
                    "type": "stat",
                    "targets": [
                      {
                        "expr": "compliance_control_status{framework=\"soc2\"}"
                      }
                    ]
                  },
                  {
                    "title": "GDPR Requirements",
                    "type": "table",
                    "targets": [
                      {
                        "expr": "gdpr_requirement_status"
                      }
                    ]
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Archive compliance evidence",
          "type": "cli",
          "command": "tar -czf evidence/compliance-evidence-$(date +%Y%m%d).tar.gz evidence/$(date +%Y%m%d)/ && gpg --encrypt --recipient compliance@example.com evidence/compliance-evidence-$(date +%Y%m%d).tar.gz",
          "parameters": {},
          "files": []
        },
        {
          "name": "Send compliance report",
          "type": "cli",
          "command": "python compliance/scripts/report-sender.py --recipients 'security-team@example.com,audit@example.com' --report compliance-report.html --evidence evidence/compliance-evidence-$(date +%Y%m%d).tar.gz.gpg",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement advanced cost optimization with predictive scaling",
      "steps": [
        {
          "name": "Analyze historical usage patterns",
          "type": "cli",
          "command": "bq query --use_legacy_sql=false 'SELECT EXTRACT(HOUR FROM usage_start_time) as hour, EXTRACT(DAYOFWEEK FROM usage_start_time) as day, AVG(cost) as avg_cost, MAX(cost) as max_cost FROM `billing_dataset.gcp_billing_export_v1` WHERE DATE(_PARTITIONTIME) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY) GROUP BY 1,2 ORDER BY 1,2'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create predictive scaling model",
          "type": "prompt",
          "prompt": "Implement machine learning model using historical metrics to predict resource needs. Use Prophet or ARIMA for time-series forecasting of CPU, memory, and request rates.",
          "parameters": {},
          "files": [
            "ml/cost-optimization/scaling-predictor.py"
          ]
        },
        {
          "name": "Deploy predictive scaler",
          "type": "cli",
          "command": "kubectl apply -f ml/cost-optimization/predictive-scaler-cronjob.yaml && kubectl create configmap scaling-model --from-file=ml/cost-optimization/model.pkl",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure preemptible instances",
          "type": "integration",
          "integration_name": "CreateInstanceTemplate",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instanceTemplates.insert",
            "parameters": {
              "project": "devops-demo-prod",
              "body": {
                "name": "batch-processing-preemptible",
                "properties": {
                  "machineType": "n2-highmem-4",
                  "scheduling": {
                    "preemptible": true,
                    "automaticRestart": false,
                    "onHostMaintenance": "TERMINATE"
                  }
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Implement spot instance fallback",
          "type": "cli",
          "command": "aws ec2 request-spot-fleet --spot-fleet-request-config file://spot-fleet-config.json --query 'SpotFleetRequestId' --output text",
          "parameters": {},
          "files": []
        },
        {
          "name": "Schedule resource optimization",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: resource-optimizer\nspec:\n  schedule: \"0 */4 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: optimizer\n            image: cost-optimizer:latest\n            command: [\"python\", \"/app/optimize.py\"]\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Apply recommendations",
          "type": "cli",
          "command": "python ml/cost-optimization/apply-recommendations.py --dry-run=false --threshold=0.8 --action=scale-down",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor cost savings",
          "type": "integration",
          "integration_name": "Query",
          "integration_params": {
            "adapter_name": "datadog",
            "method": "metrics.Query",
            "parameters": {
              "query": "avg:gcp.billing.cost{service:compute} by {project}.rollup(sum, 86400)",
              "from": "now-7d",
              "to": "now"
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy multi-tenant platform with isolated environments and resource quotas",
      "steps": [
        {
          "name": "Create tenant namespaces",
          "type": "cli",
          "command": "for tenant in alpha beta gamma; do kubectl create namespace tenant-$tenant && kubectl label namespace tenant-$tenant tenant=$tenant environment=production; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Apply resource quotas",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: tenant-quota\n  namespace: tenant-alpha\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"20Gi\"\n    persistentvolumeclaims: \"5\"\n    services.loadbalancers: \"2\"\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure network isolation",
          "type": "cli",
          "command": "kubectl apply -f k8s/multi-tenant/network-policies/ && kubectl get networkpolicies -A | grep tenant",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy tenant-specific ingress",
          "type": "cli",
          "command": "helm install nginx-tenant-alpha ingress-nginx/ingress-nginx --namespace tenant-alpha --set controller.ingressClass=nginx-alpha --set controller.scope.enabled=true --set controller.scope.namespace=tenant-alpha",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create tenant service accounts",
          "type": "cli",
          "command": "for tenant in alpha beta gamma; do kubectl create serviceaccount tenant-admin -n tenant-$tenant && kubectl create rolebinding tenant-admin-binding --clusterrole=edit --serviceaccount=tenant-$tenant:tenant-admin -n tenant-$tenant; done",
          "parameters": {},
          "files": []
        },
        {
          "name": "Set up tenant monitoring",
          "type": "integration",
          "integration_name": "CreateFolder",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "folder.create_folder",
            "parameters": {
              "title": "Tenant Alpha Dashboards",
              "uid": "tenant-alpha"
            }
          },
          "files": []
        },
        {
          "name": "Configure cost allocation",
          "type": "cli",
          "command": "kubectl label nodes node1 node2 node3 tenant-pool=shared && kubectl apply -f k8s/multi-tenant/kubecost-config.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate tenant isolation",
          "type": "cli",
          "command": "kubectl auth can-i --list --as=system:serviceaccount:tenant-alpha:tenant-admin -n tenant-beta | grep -c 'no' | xargs -I {} test {} -gt 10 && echo 'Isolation verified'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Generate tenant usage report",
          "type": "cli",
          "command": "kubectl top pods -A | grep tenant- | awk '{tenant=$1; cpu+=$2; mem+=$3} END {for (t in tenant) print t, cpu[t], mem[t]}' > tenant-usage.txt",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement GitOps-based progressive delivery with automated rollback",
      "steps": [
        {
          "name": "Configure Flux GitOps",
          "type": "cli",
          "command": "flux bootstrap github --owner=example-org --repository=gitops-config --branch=main --path=./clusters/production --personal --private=false",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create Flagger canary resource",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\n  name: payment-api\n  namespace: production\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: payment-api\n  service:\n    port: 8080\n  analysis:\n    interval: 1m\n    threshold: 5\n    maxWeight: 50\n    stepWeight: 10\n    metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99\n      interval: 1m\n    - name: request-duration\n      thresholdRange:\n        max: 500\n      interval: 1m\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Trigger new deployment",
          "type": "cli",
          "command": "kubectl set image deployment/payment-api payment-api=payment-api:v2.0 -n production && kubectl annotate deployment/payment-api flagger.app/canary-revision=\"$(date +%s)\" -n production",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor canary progress",
          "type": "cli",
          "command": "kubectl get canary payment-api -n production -w | grep -E 'Progressing|Succeeded|Failed'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check canary metrics",
          "type": "integration",
          "integration_name": "Query",
          "integration_params": {
            "adapter_name": "datadog",
            "method": "metrics.Query",
            "parameters": {
              "query": "avg:trace.flask.request.duration{service:payment-api,version:canary}",
              "from": "now-10m",
              "to": "now"
            }
          },
          "files": []
        },
        {
          "name": "Simulate failure for rollback",
          "type": "cli",
          "command": "kubectl exec deployment/payment-api -n production -- kill -TERM 1 && sleep 30",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify automatic rollback",
          "type": "cli",
          "command": "kubectl get events -n production --field-selector involvedObject.name=payment-api --sort-by='.lastTimestamp' | grep -i rollback",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update GitOps repository",
          "type": "cli",
          "command": "git clone https://github.com/example-org/gitops-config && cd gitops-config && yq eval '.spec.template.spec.containers[0].image = \"payment-api:v1.9\"' -i clusters/production/payment-api/deployment.yaml && git commit -am 'Rollback payment-api to v1.9' && git push",
          "parameters": {},
          "files": []
        },
        {
          "name": "Confirm GitOps sync",
          "type": "cli",
          "command": "flux get kustomizations --watch | grep 'Applied revision: main'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy CircleCI pipeline with automated testing",
      "steps": [
        {
          "name": "Trigger CircleCI pipeline",
          "type": "integration",
          "integration_name": "trigger_pipeline",
          "integration_params": {
            "adapter_name": "circleci",
            "method": "trigger_pipeline",
            "parameters": {
              "username": "example-org",
              "project": "microservice-api",
              "branch": "main",
              "params": {
                "run_integration_tests": true
              }
            }
          },
          "files": []
        },
        {
          "name": "Get pipeline status",
          "type": "integration",
          "integration_name": "get_pipeline",
          "integration_params": {
            "adapter_name": "circleci",
            "method": "get_pipeline",
            "parameters": {
              "pipeline_id": "123e4567-e89b-12d3-a456-426614174000"
            }
          },
          "files": []
        },
        {
          "name": "Monitor workflow execution",
          "type": "integration",
          "integration_name": "get_pipeline_workflow",
          "integration_params": {
            "adapter_name": "circleci",
            "method": "get_pipeline_workflow",
            "parameters": {
              "pipeline_id": "123e4567-e89b-12d3-a456-426614174000"
            }
          },
          "files": []
        },
        {
          "name": "Verify test results",
          "type": "cli",
          "command": "circleci tests summarize test-results/junit.xml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick Kubernetes deployment with auto-scaling",
      "steps": [
        {
          "name": "Deploy application",
          "type": "cli",
          "command": "kubectl create deployment web-api --image=web-api:v1.0 --replicas=3",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create HPA",
          "type": "cli",
          "command": "kubectl autoscale deployment web-api --cpu-percent=70 --min=3 --max=10",
          "parameters": {},
          "files": []
        },
        {
          "name": "Expose service",
          "type": "cli",
          "command": "kubectl expose deployment web-api --port=80 --target-port=8080 --type=LoadBalancer",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify deployment",
          "type": "cli",
          "command": "kubectl rollout status deployment/web-api && kubectl get svc web-api",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create S3 bucket with CloudFront distribution",
      "steps": [
        {
          "name": "Create S3 bucket",
          "type": "integration",
          "integration_name": "CreateBucket",
          "integration_params": {
            "adapter_name": "aws",
            "method": "s3.CreateBucket",
            "parameters": {
              "Bucket": "static-assets-prod-2024",
              "CreateBucketConfiguration": {
                "LocationConstraint": "us-west-2"
              }
            }
          },
          "files": []
        },
        {
          "name": "Enable static hosting",
          "type": "cli",
          "command": "aws s3 website s3://static-assets-prod-2024 --index-document index.html --error-document error.html",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create CloudFront distribution",
          "type": "cli",
          "command": "aws cloudfront create-distribution --distribution-config file://cloudfront-config.json --query 'Distribution.DomainName'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Upload test file",
          "type": "cli",
          "command": "echo '<h1>Hello CDN</h1>' > index.html && aws s3 cp index.html s3://static-assets-prod-2024/",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy Grafana alert with Slack notification",
      "steps": [
        {
          "name": "Create alert rule",
          "type": "integration",
          "integration_name": "create_alertrule",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "alerting.create_alertrule",
            "parameters": {
              "uid": "cpu-alert",
              "title": "High CPU Usage",
              "condition": "avg()",
              "data": [
                {
                  "refId": "A",
                  "queryType": "prometheus",
                  "expr": "avg(rate(cpu_usage_total[5m])) > 0.8"
                }
              ],
              "noDataState": "NoData",
              "execErrState": "Alerting"
            }
          },
          "files": []
        },
        {
          "name": "Configure Slack contact",
          "type": "integration",
          "integration_name": "create_contactpoint",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "alerting.create_contactpoint",
            "parameters": {
              "name": "slack-alerts",
              "type": "slack",
              "settings": {
                "url": "https://hooks.slack.com/services/XXX/YYY/ZZZ"
              }
            }
          },
          "files": []
        },
        {
          "name": "Test alert",
          "type": "cli",
          "command": "curl -X POST http://localhost:3000/api/v1/eval -H 'Authorization: Bearer $GRAFANA_TOKEN' -d '{\"dashboard\":\"test\",\"panelId\":1}'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick Redis deployment with persistence",
      "steps": [
        {
          "name": "Deploy Redis with Helm",
          "type": "cli",
          "command": "helm install redis bitnami/redis --set auth.enabled=true --set persistence.size=10Gi",
          "parameters": {},
          "files": []
        },
        {
          "name": "Get Redis password",
          "type": "cli",
          "command": "kubectl get secret redis -o jsonpath='{.data.redis-password}' | base64 -d",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test connection",
          "type": "cli",
          "command": "kubectl run redis-test --rm -it --image=redis:alpine --restart=Never -- redis-cli -h redis-master -a $REDIS_PASSWORD ping",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up basic Jenkins job with email notification",
      "steps": [
        {
          "name": "Create Jenkins job",
          "type": "integration",
          "integration_name": "create_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.create_job",
            "parameters": {
              "name": "nightly-backup",
              "config_xml": "<project><builders><shell>tar -czf backup.tar.gz /data</shell></builders></project>"
            }
          },
          "files": []
        },
        {
          "name": "Configure email notification",
          "type": "cli",
          "command": "jenkins-cli groovy = <<< 'Jenkins.instance.getJob(\"nightly-backup\").addPublisher(new hudson.tasks.Mailer(\"ops@example.com\", false, false))'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Trigger build",
          "type": "integration",
          "integration_name": "build_job",
          "integration_params": {
            "adapter_name": "jenkins",
            "method": "job.build_job",
            "parameters": {
              "name": "nightly-backup"
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy Elasticsearch single-node cluster",
      "steps": [
        {
          "name": "Deploy Elasticsearch",
          "type": "cli",
          "command": "docker run -d --name elasticsearch -p 9200:9200 -e 'discovery.type=single-node' -e 'xpack.security.enabled=false' elasticsearch:8.11.0",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create index template",
          "type": "integration",
          "integration_name": "put_index_template",
          "integration_params": {
            "adapter_name": "elasticsearch",
            "method": "indices.put_index_template",
            "parameters": {
              "name": "logs",
              "index_patterns": [
                "logs-*"
              ],
              "template": {
                "settings": {
                  "number_of_shards": 1,
                  "number_of_replicas": 0
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Verify cluster health",
          "type": "cli",
          "command": "curl -s localhost:9200/_cluster/health | jq '.status'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick GCP VM provisioning with startup script",
      "steps": [
        {
          "name": "Create VM instance",
          "type": "integration",
          "integration_name": "insert",
          "integration_params": {
            "adapter_name": "gcp",
            "method": "compute.instances.insert",
            "parameters": {
              "project": "my-project",
              "zone": "us-central1-a",
              "body": {
                "name": "web-server-1",
                "machineType": "zones/us-central1-a/machineTypes/e2-micro",
                "disks": [
                  {
                    "boot": true,
                    "autoDelete": true,
                    "initializeParams": {
                      "sourceImage": "projects/debian-cloud/global/images/family/debian-11"
                    }
                  }
                ],
                "networkInterfaces": [
                  {
                    "network": "global/networks/default"
                  }
                ],
                "metadata": {
                  "items": [
                    {
                      "key": "startup-script",
                      "value": "#!/bin/bash\napt-get update\napt-get install -y nginx\nsystemctl start nginx"
                    }
                  ]
                }
              }
            }
          },
          "files": []
        },
        {
          "name": "Get instance IP",
          "type": "cli",
          "command": "gcloud compute instances describe web-server-1 --zone=us-central1-a --format='get(networkInterfaces[0].accessConfigs[0].natIP)'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify web server",
          "type": "cli",
          "command": "curl -s -o /dev/null -w '%{http_code}' http://$(gcloud compute instances describe web-server-1 --zone=us-central1-a --format='get(networkInterfaces[0].accessConfigs[0].natIP)')",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up Consul key-value store",
      "steps": [
        {
          "name": "Start Consul agent",
          "type": "cli",
          "command": "consul agent -dev -client=0.0.0.0 &",
          "parameters": {},
          "files": []
        },
        {
          "name": "Store configuration",
          "type": "cli",
          "command": "consul kv put config/database/host postgres.example.com && consul kv put config/database/port 5432",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify KV store",
          "type": "integration",
          "integration_name": "kv_get",
          "integration_params": {
            "adapter_name": "consul",
            "method": "kv.get",
            "parameters": {
              "key": "config/database/host"
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy simple Lambda function with API Gateway",
      "steps": [
        {
          "name": "Create Lambda function",
          "type": "integration",
          "integration_name": "CreateFunction",
          "integration_params": {
            "adapter_name": "aws",
            "method": "lambda.CreateFunction",
            "parameters": {
              "FunctionName": "hello-api",
              "Runtime": "nodejs18.x",
              "Handler": "index.handler",
              "Code": {
                "ZipFile": "exports.handler = async (event) => { return { statusCode: 200, body: JSON.stringify('Hello!') }; };"
              },
              "Role": "arn:aws:iam::123456789012:role/lambda-role"
            }
          },
          "files": []
        },
        {
          "name": "Create API Gateway",
          "type": "cli",
          "command": "aws apigatewayv2 create-api --name hello-api --protocol-type HTTP --target arn:aws:lambda:us-east-1:123456789012:function:hello-api",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test endpoint",
          "type": "cli",
          "command": "curl -s $(aws apigatewayv2 get-apis --query \"Items[?Name=='hello-api'].ApiEndpoint\" --output text)",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy MongoDB replica set",
      "steps": [
        {
          "name": "Deploy MongoDB with Helm",
          "type": "cli",
          "command": "helm install mongodb bitnami/mongodb --set architecture=replicaset --set replicaCount=3",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify replica set",
          "type": "cli",
          "command": "kubectl exec mongodb-0 -- mongo --eval 'rs.status()' | jq '.members[].stateStr'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create database user",
          "type": "cli",
          "command": "kubectl exec mongodb-0 -- mongo admin -u root -p $MONGODB_ROOT_PASSWORD --eval 'db.createUser({user:\"app\",pwd:\"pass123\",roles:[{role:\"readWrite\",db:\"myapp\"}]})'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick Datadog monitoring setup",
      "steps": [
        {
          "name": "Deploy Datadog agent",
          "type": "cli",
          "command": "helm install datadog datadog/datadog --set datadog.apiKey=$DD_API_KEY --set datadog.logs.enabled=true",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create custom metric",
          "type": "integration",
          "integration_name": "SubmitMetrics",
          "integration_params": {
            "adapter_name": "datadog",
            "method": "metrics.SubmitMetrics",
            "parameters": {
              "series": [
                {
                  "metric": "custom.app.requests",
                  "points": [
                    [
                      1642521600,
                      100
                    ]
                  ],
                  "type": "gauge",
                  "tags": [
                    "env:prod"
                  ]
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Verify agent status",
          "type": "cli",
          "command": "kubectl exec $(kubectl get pod -l app=datadog -o name | head -1) -- agent status",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Create Azure Storage account with CDN",
      "steps": [
        {
          "name": "Create storage account",
          "type": "cli",
          "command": "az storage account create --name staticcontent2024 --resource-group prod-rg --location eastus --sku Standard_LRS --kind StorageV2",
          "parameters": {},
          "files": []
        },
        {
          "name": "Enable static website",
          "type": "cli",
          "command": "az storage blob service-properties update --account-name staticcontent2024 --static-website --index-document index.html",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create CDN profile",
          "type": "cli",
          "command": "az cdn profile create --name cdn-profile --resource-group prod-rg --sku Standard_Microsoft",
          "parameters": {},
          "files": []
        },
        {
          "name": "Link CDN to storage",
          "type": "cli",
          "command": "az cdn endpoint create --name static-endpoint --profile-name cdn-profile --resource-group prod-rg --origin staticcontent2024.z13.web.core.windows.net",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy GitLab CI runner",
      "steps": [
        {
          "name": "Install GitLab runner",
          "type": "cli",
          "command": "helm install gitlab-runner gitlab/gitlab-runner --set gitlabUrl=https://gitlab.com --set runnerRegistrationToken=$RUNNER_TOKEN",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify runner registration",
          "type": "integration",
          "integration_name": "ListRunners",
          "integration_params": {
            "adapter_name": "gitlab",
            "method": "runners.list",
            "parameters": {
              "status": "online"
            }
          },
          "files": []
        },
        {
          "name": "Test pipeline",
          "type": "cli",
          "command": "git commit --allow-empty -m 'Test runner' && git push",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick EFK stack deployment",
      "steps": [
        {
          "name": "Deploy Elasticsearch",
          "type": "cli",
          "command": "kubectl apply -f https://download.elastic.co/downloads/eck/2.9.0/crds.yaml && kubectl apply -f https://download.elastic.co/downloads/eck/2.9.0/operator.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Kibana",
          "type": "cli",
          "command": "kubectl apply -f - <<EOF\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\n  name: quickstart\nspec:\n  version: 8.11.0\n  count: 1\n  elasticsearchRef:\n    name: quickstart\nEOF",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Fluentd",
          "type": "cli",
          "command": "kubectl apply -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy the complete application stack to multi-cloud Kubernetes clusters",
      "steps": [
        {
          "name": "Deploy infrastructure with Terraform",
          "type": "cli",
          "command": "cd terraform && make init && make plan ENV=prod",
          "parameters": {},
          "files": []
        },
        {
          "name": "Apply Terraform changes",
          "type": "cli",
          "command": "cd terraform && make apply ENV=prod AUTO_APPROVE=true",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure kubectl contexts",
          "type": "cli",
          "command": "aws eks update-kubeconfig --name prod-eks-cluster --region us-west-2 && gcloud container clusters get-credentials prod-gke-cluster --zone us-central1-a",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Consul to clusters",
          "type": "cli",
          "command": "helm install consul hashicorp/consul -f terraform/modules/consul/k8s-client/values.yaml --namespace consul --create-namespace",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy backend application",
          "type": "cli",
          "command": "kubectl apply -f k8s/deployments/backend-deployment.yaml && kubectl apply -f k8s/services/backend-service.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy frontend application",
          "type": "cli",
          "command": "kubectl apply -f k8s/deployments/frontend-deployment.yaml && kubectl apply -f k8s/services/frontend-service.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify deployments",
          "type": "cli",
          "command": "kubectl get deployments -n default && kubectl get services -n default",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check application health",
          "type": "cli",
          "command": "kubectl exec -it deployment/backend -- curl localhost:3000/health",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement end-to-end monitoring for the application using the existing observability stack",
      "steps": [
        {
          "name": "Deploy Prometheus operator",
          "type": "cli",
          "command": "kubectl apply -f monitoring/prometheus/kubernetes/prometheus-operator.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create ServiceMonitor for backend",
          "type": "prompt",
          "prompt": "Create a Prometheus ServiceMonitor to scrape metrics from the Node.js backend service based on patterns in monitoring/prometheus/kubernetes/",
          "parameters": {},
          "files": [
            "monitoring/prometheus/kubernetes/backend-servicemonitor.yaml"
          ]
        },
        {
          "name": "Deploy ServiceMonitor",
          "type": "cli",
          "command": "kubectl apply -f monitoring/prometheus/kubernetes/backend-servicemonitor.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Grafana",
          "type": "cli",
          "command": "helm install grafana grafana/grafana -f monitoring/grafana/values.yaml --namespace monitoring",
          "parameters": {},
          "files": []
        },
        {
          "name": "Import dashboards",
          "type": "integration",
          "integration_name": "create_dashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": "file://monitoring/grafana/dashboards/application-metrics.json",
              "folderId": 0
            }
          },
          "files": []
        },
        {
          "name": "Configure alerts",
          "type": "integration",
          "integration_name": "create_alertrule",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "alerting.create_alertrule",
            "parameters": {
              "uid": "app-error-rate",
              "title": "High Application Error Rate",
              "condition": "avg()",
              "data": [
                {
                  "refId": "A",
                  "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) > 0.05"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Verify metrics collection",
          "type": "cli",
          "command": "kubectl port-forward svc/prometheus 9090:9090 & sleep 5 && curl -s localhost:9090/api/v1/query?query=up | jq '.data.result[].metric.job'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute the full deployment pipeline using the deploy.sh script",
      "steps": [
        {
          "name": "Set up environment variables",
          "type": "cli",
          "command": "cp .env.template .env && source .env",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run deployment script",
          "type": "cli",
          "command": "./deploy.sh --environment prod --clouds aws,gcp --features eks,gke,consul,monitoring",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor deployment progress",
          "type": "cli",
          "command": "tail -f logs/deploy-$(date +%Y%m%d).log | grep -E 'STEP|SUCCESS|ERROR'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify Consul federation",
          "type": "cli",
          "command": "consul members -wan && consul catalog datacenters",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check cross-cloud connectivity",
          "type": "cli",
          "command": "kubectl exec -it deployment/backend --context aws-prod -- curl http://backend.service.consul:3000/health",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run smoke tests",
          "type": "cli",
          "command": "cd Code/tests && npm run integration-tests -- --env=prod",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up GitOps with ArgoCD for continuous deployment",
      "steps": [
        {
          "name": "Deploy ArgoCD using Terraform",
          "type": "cli",
          "command": "cd terraform/modules/k8s/argocd && terraform init && terraform apply -auto-approve",
          "parameters": {},
          "files": []
        },
        {
          "name": "Get ArgoCD admin password",
          "type": "cli",
          "command": "kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create application manifest",
          "type": "prompt",
          "prompt": "Create an ArgoCD Application manifest to deploy the backend and frontend from the Git repository based on patterns in k8s/argocd/",
          "parameters": {},
          "files": [
            "k8s/argocd/application.yaml"
          ]
        },
        {
          "name": "Deploy ArgoCD application",
          "type": "cli",
          "command": "kubectl apply -f k8s/argocd/application.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Sync application",
          "type": "cli",
          "command": "argocd app sync complex-demo-app --revision main",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor sync status",
          "type": "cli",
          "command": "argocd app get complex-demo-app --refresh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify deployment",
          "type": "cli",
          "command": "kubectl get all -l app.kubernetes.io/managed-by=argocd",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement disaster recovery using the existing runbooks",
      "steps": [
        {
          "name": "Review DR runbook",
          "type": "prompt",
          "prompt": "Review runbooks/disaster-recovery.yaml to understand the disaster recovery procedures for this application.",
          "parameters": {},
          "files": [
            "runbooks/disaster-recovery.yaml"
          ]
        },
        {
          "name": "Backup application data",
          "type": "cli",
          "command": "kubectl exec deployment/backend -- npm run backup:dynamodb",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create infrastructure snapshot",
          "type": "cli",
          "command": "cd terraform && terraform state pull > backups/terraform-state-$(date +%Y%m%d).json",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test failover to secondary region",
          "type": "cli",
          "command": "./scripts/disaster-recovery/failover-test.sh --source us-west-2 --target us-east-1 --dry-run",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update DNS for failover",
          "type": "integration",
          "integration_name": "ChangeResourceRecordSets",
          "integration_params": {
            "adapter_name": "aws",
            "method": "route53.ChangeResourceRecordSets",
            "parameters": {
              "HostedZoneId": "Z123456789",
              "ChangeBatch": {
                "Changes": [
                  {
                    "Action": "UPSERT",
                    "ResourceRecordSet": {
                      "Name": "app.example.com",
                      "Type": "A",
                      "SetIdentifier": "Secondary",
                      "Failover": "SECONDARY"
                    }
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Verify application in DR region",
          "type": "cli",
          "command": "kubectl --context aws-dr-east get deployments && curl https://app-dr.example.com/health",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Ansible automation for Day-2 operations",
      "steps": [
        {
          "name": "Review Ansible playbooks",
          "type": "prompt",
          "prompt": "Examine ansible/playbooks directory to understand existing automation patterns for configuration management.",
          "parameters": {},
          "files": [
            "ansible/playbooks/"
          ]
        },
        {
          "name": "Create application configuration playbook",
          "type": "prompt",
          "prompt": "Create an Ansible playbook to configure the Node.js application settings across all environments based on existing playbook patterns.",
          "parameters": {},
          "files": [
            "ansible/playbooks/configure-app.yml"
          ]
        },
        {
          "name": "Deploy Ansible AWX",
          "type": "cli",
          "command": "cd terraform/modules/azure/ansible-controller && terraform apply -auto-approve",
          "parameters": {},
          "files": []
        },
        {
          "name": "Import playbooks to AWX",
          "type": "cli",
          "command": "awx project create --name 'Complex Demo' --scm-type git --scm-url https://github.com/example/complex-demo.git",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create job template",
          "type": "cli",
          "command": "awx job_template create --name 'Configure Application' --project 'Complex Demo' --playbook 'ansible/playbooks/configure-app.yml' --inventory 'Production'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run configuration job",
          "type": "cli",
          "command": "awx job_template launch 'Configure Application' --monitor",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify configuration",
          "type": "cli",
          "command": "ansible -i ansible/inventory/production all -m shell -a 'cat /etc/app/config.json'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement security scanning in the CI/CD pipeline",
      "steps": [
        {
          "name": "Review security configurations",
          "type": "prompt",
          "prompt": "Examine security-enhancements directory to understand the security tools and policies available.",
          "parameters": {},
          "files": [
            "security-enhancements/"
          ]
        },
        {
          "name": "Add SAST to CircleCI",
          "type": "prompt",
          "prompt": "Update .circleci/config.yml to add static application security testing for both frontend and backend code.",
          "parameters": {},
          "files": [
            ".circleci/config.yml"
          ]
        },
        {
          "name": "Configure dependency scanning",
          "type": "cli",
          "command": "cd Code/server && npm audit fix && cd ../client && npm audit fix",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy OPA policies",
          "type": "cli",
          "command": "kubectl apply -f security-enhancements/opa/policies/",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run container scanning",
          "type": "cli",
          "command": "docker scout cves $(kubectl get deployment backend -o jsonpath='{.spec.template.spec.containers[0].image}')",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create security dashboard",
          "type": "integration",
          "integration_name": "create_dashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": {
                "title": "Security Posture",
                "panels": [
                  {
                    "title": "Vulnerability Count",
                    "type": "stat"
                  }
                ]
              }
            }
          },
          "files": []
        },
        {
          "name": "Verify security policies",
          "type": "cli",
          "command": "kubectl auth can-i --list --as=system:serviceaccount:default:backend",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick deployment of backend to local Kubernetes",
      "steps": [
        {
          "name": "Build Docker image",
          "type": "cli",
          "command": "cd Code/server && docker build -t backend:local .",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy to K8s",
          "type": "cli",
          "command": "kubectl create deployment backend --image=backend:local",
          "parameters": {},
          "files": []
        },
        {
          "name": "Expose service",
          "type": "cli",
          "command": "kubectl expose deployment backend --port=3000",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Run Terraform for GCP VPC setup",
      "steps": [
        {
          "name": "Initialize GCP module",
          "type": "cli",
          "command": "cd terraform/modules/gcp/vpc && terraform init",
          "parameters": {},
          "files": []
        },
        {
          "name": "Apply VPC configuration",
          "type": "cli",
          "command": "cd terraform/modules/gcp/vpc && terraform apply -auto-approve",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify VPC creation",
          "type": "cli",
          "command": "gcloud compute networks list --filter='name:prod-vpc'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy monitoring stack using existing scripts",
      "steps": [
        {
          "name": "Run monitoring setup",
          "type": "cli",
          "command": "./scripts/monitoring/setup-prometheus.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Grafana",
          "type": "cli",
          "command": "./scripts/monitoring/setup-grafana.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify monitoring",
          "type": "cli",
          "command": "kubectl get pods -n monitoring",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick Consul deployment check",
      "steps": [
        {
          "name": "Check Consul status",
          "type": "cli",
          "command": "./terraform/scripts/consul-status.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "List Consul members",
          "type": "integration",
          "integration_name": "ListAgentMembers",
          "integration_params": {
            "adapter_name": "consul",
            "method": "agent.members",
            "parameters": {}
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy updated backend application with database migrations",
      "steps": [
        {
          "name": "Build backend Docker image",
          "type": "cli",
          "command": "cd complex-demo/Code/server && docker build -t backend:latest -f Dockerfile .",
          "parameters": {},
          "files": []
        },
        {
          "name": "Tag and push to JFrog registry",
          "type": "cli",
          "command": "docker tag backend:latest forgea37.jfrog.io/complex-demo-docker-local/dev-backend:latest && docker push forgea37.jfrog.io/complex-demo-docker-local/dev-backend:latest",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update deployment image tag",
          "type": "prompt",
          "prompt": "Update the deployment.yaml to use the new image tag with current timestamp for the backend container.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/deployment.yaml"
          ]
        },
        {
          "name": "Apply deployment update",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/backend/deployment.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run database migrations",
          "type": "cli",
          "command": "kubectl exec -it deployment/backend -n backend-dev -- npm run migrate",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify deployment health",
          "type": "cli",
          "command": "kubectl rollout status deployment/backend -n backend-dev && kubectl get pods -n backend-dev -l app=backend",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test API endpoints",
          "type": "cli",
          "command": "kubectl port-forward -n backend-dev svc/backend-service 3001:80 & sleep 3 && curl http://localhost:3001/status && curl http://localhost:3001/api/getAllProducts",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up complete EKS cluster with networking and add-ons",
      "steps": [
        {
          "name": "Initialize Terraform workspace",
          "type": "cli",
          "command": "cd complex-demo/terraform/envs/dev/us-east-2 && terraform init",
          "parameters": {},
          "files": []
        },
        {
          "name": "Review and update EKS configuration",
          "type": "prompt",
          "prompt": "Review the EKS module configuration in main.tf and ensure it references the correct VPC outputs and has appropriate node group settings.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/envs/dev/us-east-2/main.tf",
            "complex-demo/terraform/modules/aws/eks/main.tf"
          ]
        },
        {
          "name": "Plan infrastructure changes",
          "type": "cli",
          "command": "cd complex-demo/terraform/envs/dev/us-east-2 && terraform plan -out=eks.tfplan",
          "parameters": {},
          "files": []
        },
        {
          "name": "Apply EKS cluster creation",
          "type": "cli",
          "command": "cd complex-demo/terraform/envs/dev/us-east-2 && terraform apply eks.tfplan",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure kubectl context",
          "type": "cli",
          "command": "aws eks update-kubeconfig --name $(terraform output -raw eks_cluster_name) --region us-east-2",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy AWS Load Balancer Controller",
          "type": "cli",
          "command": "kubectl apply -k 'github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify cluster components",
          "type": "cli",
          "command": "kubectl get nodes && kubectl get pods -A | grep -E 'aws-load-balancer|coredns'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement Fluent Bit logging pipeline with Elasticsearch",
      "steps": [
        {
          "name": "Review Fluent Bit configuration",
          "type": "prompt",
          "prompt": "Examine the Fluent Bit ConfigMap to understand the current log parsing and output configuration.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/logging/fluent-bit-configmap.yaml"
          ]
        },
        {
          "name": "Update Elasticsearch output",
          "type": "prompt",
          "prompt": "Modify the Fluent Bit ConfigMap to add proper index patterns and authentication for Elasticsearch output.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/logging/fluent-bit-configmap.yaml"
          ]
        },
        {
          "name": "Apply Elasticsearch secret",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/logging/elasticsearch-secret.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Fluent Bit DaemonSet",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/logging/fluent-bit-configmap.yaml && kubectl apply -f complex-demo/k8s/envs/dev/logging/fluent-bit-daemonset.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify Fluent Bit pods",
          "type": "cli",
          "command": "kubectl get pods -n kube-system -l app=fluent-bit && kubectl logs -n kube-system -l app=fluent-bit --tail=20",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test log forwarding",
          "type": "cli",
          "command": "kubectl exec -it deployment/backend -n backend-dev -- echo 'Test log message' && sleep 5 && curl -X GET 'http://elasticsearch:9200/logstash-*/_search?q=message:Test+log+message'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy multi-cloud Consul federation across AWS and GCP",
      "steps": [
        {
          "name": "Deploy Consul server cluster on AWS",
          "type": "cli",
          "command": "cd complex-demo/terraform/modules/consul/ec2-cluster && terraform apply -auto-approve -var='datacenter=aws-east'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure Consul K8s client for EKS",
          "type": "prompt",
          "prompt": "Review and update the Consul K8s client configuration to connect to the EC2-based Consul servers.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/modules/consul/k8s-client/main.tf",
            "complex-demo/terraform/modules/consul/k8s-client/variables.tf"
          ]
        },
        {
          "name": "Deploy Consul on EKS",
          "type": "cli",
          "command": "cd complex-demo/terraform/modules/consul/k8s-client && terraform apply -auto-approve -var='cluster_name=eks-cluster'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure GKE Consul client",
          "type": "cli",
          "command": "kubectl config use-context gke-cluster && cd complex-demo/terraform/modules/consul/k8s-client && terraform apply -auto-approve -var='cluster_name=gke-cluster' -var='datacenter=gcp-central'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Join Consul datacenters",
          "type": "cli",
          "command": "consul join -wan $(terraform output -raw consul_server_wan_ip)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify federation status",
          "type": "integration",
          "integration_name": "ListDatacenters",
          "integration_params": {
            "adapter_name": "consul",
            "method": "catalog.datacenters",
            "parameters": {}
          },
          "files": []
        },
        {
          "name": "Test cross-datacenter service discovery",
          "type": "cli",
          "command": "consul catalog services -datacenter=gcp-central",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up complete CI/CD pipeline with ArgoCD GitOps",
      "steps": [
        {
          "name": "Install ArgoCD using Terraform",
          "type": "cli",
          "command": "cd complex-demo/terraform/envs/dev/us-east-2 && terraform apply -target=module.argocd -auto-approve",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure ArgoCD applications",
          "type": "prompt",
          "prompt": "Review and update the ArgoCD applications.yaml to point to the correct Git repository and paths for backend and frontend deployments.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/applications.yaml"
          ]
        },
        {
          "name": "Apply ArgoCD applications",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/applications.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Trigger CircleCI build",
          "type": "integration",
          "integration_name": "trigger_pipeline",
          "integration_params": {
            "adapter_name": "circleci",
            "method": "trigger_pipeline",
            "parameters": {
              "username": "complex-demo",
              "project": "complex-demo",
              "branch": "main"
            }
          },
          "files": []
        },
        {
          "name": "Monitor ArgoCD sync",
          "type": "cli",
          "command": "argocd app get backend-dev --refresh && argocd app get frontend-dev --refresh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify deployments",
          "type": "cli",
          "command": "kubectl get applications -n argocd && kubectl get deployments -n backend-dev && kubectl get deployments -n frontend-dev",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Datadog monitoring across all environments",
      "steps": [
        {
          "name": "Apply Datadog secrets",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/monitoring/datadog-secrets.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Datadog agent on EKS",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/monitoring/datadog-aws-eks.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure APM for backend",
          "type": "prompt",
          "prompt": "Update the backend deployment.yaml to include Datadog APM environment variables and annotations for trace collection.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/backend/deployment.yaml"
          ]
        },
        {
          "name": "Create Datadog dashboard",
          "type": "integration",
          "integration_name": "CreateDashboard",
          "integration_params": {
            "adapter_name": "datadog",
            "method": "dashboards.CreateDashboard",
            "parameters": {
              "title": "Complex Demo Application",
              "widgets": [
                {
                  "definition": {
                    "type": "timeseries",
                    "requests": [
                      {
                        "q": "avg:kubernetes.cpu.usage{kube_deployment:backend}"
                      }
                    ]
                  }
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Set up log pipeline",
          "type": "integration",
          "integration_name": "CreateLogsPipeline",
          "integration_params": {
            "adapter_name": "datadog",
            "method": "logs.CreateLogsPipeline",
            "parameters": {
              "name": "Backend Application Logs",
              "filter": {
                "query": "source:nodejs service:backend"
              },
              "processors": [
                {
                  "type": "grok-parser",
                  "name": "Parse backend logs"
                }
              ]
            }
          },
          "files": []
        },
        {
          "name": "Verify monitoring",
          "type": "cli",
          "command": "kubectl get pods -l app=datadog-agent && kubectl exec -it $(kubectl get pod -l app=datadog-agent -o name | head -1) -- agent status",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick RDS deployment with Terraform",
      "steps": [
        {
          "name": "Configure RDS",
          "type": "prompt",
          "prompt": "Update the RDS module variables to use appropriate instance size and enable automated backups.",
          "parameters": {},
          "files": [
            "complex-demo/terraform/modules/aws/rds/variables.tf"
          ]
        },
        {
          "name": "Deploy RDS",
          "type": "cli",
          "command": "cd complex-demo/terraform/modules/aws/rds && terraform apply -auto-approve",
          "parameters": {},
          "files": []
        },
        {
          "name": "Get endpoint",
          "type": "cli",
          "command": "terraform output -raw rds_endpoint",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy frontend with updated image",
      "steps": [
        {
          "name": "Build frontend",
          "type": "cli",
          "command": "cd complex-demo/Code/client && docker build -t frontend:latest .",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update deployment",
          "type": "prompt",
          "prompt": "Update the frontend deployment image tag to use the newly built image.",
          "parameters": {},
          "files": [
            "complex-demo/k8s/envs/dev/frontend/deployment.yaml"
          ]
        },
        {
          "name": "Deploy",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/frontend/deployment.yaml",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Enable Prometheus monitoring",
      "steps": [
        {
          "name": "Deploy Prometheus",
          "type": "cli",
          "command": "kubectl apply -f complex-demo/k8s/envs/dev/aws/observability/prometheus.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Check metrics",
          "type": "cli",
          "command": "kubectl port-forward -n monitoring svc/prometheus 9090:9090 & sleep 3 && curl localhost:9090/api/v1/targets",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick ECR repository setup",
      "steps": [
        {
          "name": "Create ECR repos",
          "type": "cli",
          "command": "cd complex-demo/terraform/modules/aws/ecr && terraform apply -auto-approve",
          "parameters": {},
          "files": []
        },
        {
          "name": "Get login token",
          "type": "cli",
          "command": "aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin $(terraform output -raw ecr_url)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Push image",
          "type": "cli",
          "command": "docker tag backend:latest $(terraform output -raw ecr_url)/backend:latest && docker push $(terraform output -raw ecr_url)/backend:latest",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute full platform deployment using deploy.sh with custom configurations",
      "steps": [
        {
          "name": "Review deployment script",
          "type": "prompt",
          "prompt": "Examine deploy.sh to understand the deployment phases and configuration options available.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/deploy.sh"
          ]
        },
        {
          "name": "Set environment variables",
          "type": "cli",
          "command": "export ENV=dev && export REGION=us-east-2 && export SKIP_TERRAFORM=false && export ENABLE_MONITORING=true",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run pre-deployment validation",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/test-env.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Execute deployment",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/deploy.sh --phase all --log-level debug",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor deployment progress",
          "type": "cli",
          "command": "tail -f deployment-*.log | grep -E 'STEP|SUCCESS|ERROR|WARNING'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate deployment",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/validate-complete-setup.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Extract credentials",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/extract-credentials-to-env.sh > credentials.env",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Build and deploy full stack application with monitoring",
      "steps": [
        {
          "name": "Build frontend and backend images",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/build-and-push.sh --frontend --backend --tag $(git rev-parse --short HEAD)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update image tags in deployments",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/update-k8s-images.sh --tag $(git rev-parse --short HEAD)",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy full stack",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/deploy-full-stack.sh --namespace dev --wait",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure monitoring",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/monitoring-setup.sh --datadog --prometheus --grafana",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy Elasticsearch integration",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/deploy-elasticsearch-integration.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure Grafana datasource",
          "type": "prompt",
          "prompt": "Apply the Grafana Elasticsearch datasource configuration to connect Grafana with Elasticsearch for log visualization.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/monitoring/grafana-elasticsearch-datasource.yaml"
          ]
        },
        {
          "name": "Import log dashboards",
          "type": "integration",
          "integration_name": "create_dashboard",
          "integration_params": {
            "adapter_name": "grafana",
            "method": "dashboard.create_dashboard",
            "parameters": {
              "dashboard": "file:///Users/computer/complex-demo/monitoring/elasticsearch-log-dashboards.json"
            }
          },
          "files": []
        }
      ]
    },
    {
      "goal": "Set up disaster recovery with automated runbooks",
      "steps": [
        {
          "name": "Configure DR infrastructure",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/setup-disaster-recovery.sh --primary us-east-2 --secondary us-west-2",
          "parameters": {},
          "files": []
        },
        {
          "name": "Review runbook configuration",
          "type": "prompt",
          "prompt": "Examine the multi-cloud service outage runbook to understand the automated recovery procedures.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/runbooks/multi-cloud-service-outage.yaml"
          ]
        },
        {
          "name": "Create runbook automation",
          "type": "prompt",
          "prompt": "Convert the YAML runbook into an executable automation script that can be triggered during incidents.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/runbooks/multi-cloud-service-outage.yaml",
            "/Users/computer/complex-demo/runbooks/execute-dr-runbook.sh"
          ]
        },
        {
          "name": "Test DR failover",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/runbooks/execute-dr-runbook.sh --scenario regional-failure --dry-run",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate DR readiness",
          "type": "cli",
          "command": "kubectl get deployments -A --context=dr-cluster && curl https://dr.example.com/health",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Jenkins and Nexus CI/CD integration",
      "steps": [
        {
          "name": "Set up Jenkins triggers",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/ci-cd/jenkins/scripts/configure-jenkins-triggers.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure Nexus integration",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/ci-cd/jenkins/scripts/jenkins-nexus-integration-complete.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Register Nexus with Consul",
          "type": "prompt",
          "prompt": "Apply the Nexus Consul registration to enable service discovery for the artifact repository.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/ci-cd/nexus/configs/nexus-consul-registration.yaml"
          ]
        },
        {
          "name": "Apply Nexus monitoring",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/ci-cd/nexus/configs/nexus-monitoring.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test CI/CD pipeline",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/ci-cd/jenkins/scripts/demo-jenkins-nexus-value.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Monitor Nexus performance",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/ci-cd/nexus/scripts/demo-nexus-performance.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Implement configuration management with Ansible Tower",
      "steps": [
        {
          "name": "Configure Ansible Tower",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/configure-ansible-tower.sh --license-file license.txt",
          "parameters": {},
          "files": []
        },
        {
          "name": "Set up Puppet integration",
          "type": "prompt",
          "prompt": "Review and apply the Puppet integration playbook to connect Ansible with Puppet Enterprise.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/ansible/playbooks/puppet-integration.yml"
          ]
        },
        {
          "name": "Apply Puppet-Elasticsearch integration",
          "type": "cli",
          "command": "ansible-playbook -i inventory /Users/computer/complex-demo/ansible/playbooks/puppet-elasticsearch-integration.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run configuration management setup",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/setup-configuration-management.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate Ansible Tower",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/validate-ansible-tower.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate config management",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/validate-config-management.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy production-ready service mesh with SSL",
      "steps": [
        {
          "name": "Configure SSL certificates",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/configure-ssl-certificates.sh --domain example.com --email admin@example.com",
          "parameters": {},
          "files": []
        },
        {
          "name": "Set up service mesh",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/configure-service-mesh.sh --enable-mtls --enable-tracing",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure security policies",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/configure-security-policies.sh --enforce-mtls --enable-rbac",
          "parameters": {},
          "files": []
        },
        {
          "name": "Fix load balancer access",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/fix-load-balancer-access.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Create image pull secrets",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/create-image-pull-secrets.sh --registry jfrog --namespace default",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify service mesh",
          "type": "cli",
          "command": "istioctl analyze && istioctl proxy-status",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Execute comprehensive platform destruction",
      "steps": [
        {
          "name": "Backup critical data",
          "type": "cli",
          "command": "kubectl get all -A -o yaml > backup-all-resources-$(date +%Y%m%d).yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Review destroy script",
          "type": "prompt",
          "prompt": "Examine destroy.sh to understand the destruction order and safety checks.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/destroy.sh"
          ]
        },
        {
          "name": "Run pre-destroy cleanup",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/terraform/scripts/pre-destroy-cleanup.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Execute destroy with confirmation",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/destroy.sh --confirm --retain-backups",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify resource cleanup",
          "type": "cli",
          "command": "aws ec2 describe-instances --filters Name=tag:Project,Values=complex-demo --query 'Reservations[].Instances[].InstanceId' && echo 'No instances should be listed'",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Set up advanced security with threat detection",
      "steps": [
        {
          "name": "Review threat detection config",
          "type": "prompt",
          "prompt": "Examine the advanced threat detection configuration to understand the security monitoring setup.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/security-enhancements/advanced-threat-detection.yaml"
          ]
        },
        {
          "name": "Apply threat detection",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/security-enhancements/advanced-threat-detection.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Configure supply chain security",
          "type": "prompt",
          "prompt": "Apply the container supply chain security configuration for image scanning and policy enforcement.",
          "parameters": {},
          "files": [
            "/Users/computer/complex-demo/security-enhancements/container-supply-chain-security.yaml"
          ]
        },
        {
          "name": "Deploy security findings integration",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/envs/dev/logging/security-findings-integration.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Test security alerts",
          "type": "cli",
          "command": "kubectl exec -it deployment/backend -- sh -c 'nc -zv suspicious-domain.com 443' || echo 'Security alert should be triggered'",
          "parameters": {},
          "files": []
        },
        {
          "name": "Verify security posture",
          "type": "cli",
          "command": "kubectl get networkpolicies -A && kubectl get podsecuritypolicies",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Quick stress test deployment",
      "steps": [
        {
          "name": "Run server stress test",
          "type": "cli",
          "command": "cd /Users/computer/complex-demo/Code/server && artillery run src/tests/stresstests/stress_server.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Run client stress test",
          "type": "cli",
          "command": "cd /Users/computer/complex-demo/Code/client && artillery run src/tests/stresstests/stress_client.yml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update load test URLs",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/scripts/update-load-test-urls.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy ArgoCD applications",
      "steps": [
        {
          "name": "Install ArgoCD",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/argocd/install.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Apply applications",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/envs/dev/applications.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Sync apps",
          "type": "cli",
          "command": "argocd app sync backend-dev frontend-dev",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Bootstrap Terraform backend",
      "steps": [
        {
          "name": "Run bootstrap script",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/terraform/bootstrap/run-bootstrap.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Update Terragrunt",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/terraform/bootstrap/update-terragrunt.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Configure Consul status monitoring",
      "steps": [
        {
          "name": "Check Consul status",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/terraform/scripts/consul-status.sh",
          "parameters": {},
          "files": []
        },
        {
          "name": "Validate Terraform modules",
          "type": "cli",
          "command": "bash /Users/computer/complex-demo/terraform/scripts/validate-modules.sh",
          "parameters": {},
          "files": []
        }
      ]
    },
    {
      "goal": "Deploy lightweight monitoring",
      "steps": [
        {
          "name": "Deploy Prometheus lite",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/envs/dev/aws/observability/prometheus-lite.yaml",
          "parameters": {},
          "files": []
        },
        {
          "name": "Deploy New Relic lightweight",
          "type": "cli",
          "command": "kubectl apply -f /Users/computer/complex-demo/k8s/envs/dev/monitoring/newrelic-lightweight.yaml",
          "parameters": {},
          "files": []
        }
      ]
    }
  ]
}