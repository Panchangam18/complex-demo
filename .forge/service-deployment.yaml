goal: "Deploy microservices across multicloud Kubernetes clusters with progressive rollout and monitoring"
generate_pr: true

triggers:
  - display_name: "Service Deployment Trigger"
    description: "Triggered by Argo CD or manual service deployment request"
    provider: "github"
    config:
      repository: "company/microservices"
      branch: "main"
      path: "services/**/deploy/*.yaml"
    labels:
      priority: "high"
      team: "platform-engineering"

steps:
  - name: "Pre-deployment Validation"
    type: "action"
    description: "Validate service configuration and cluster readiness"
    config:
      command: |
        #!/bin/bash
        set -euo pipefail
        
        echo "=== PRE-DEPLOYMENT VALIDATION ==="
        
        # Validate service configuration
        SERVICE_NAME=${SERVICE_NAME:-"example-service"}
        SERVICE_VERSION=${SERVICE_VERSION:-"latest"}
        TARGET_CLUSTERS=${TARGET_CLUSTERS:-"eks-prod-us-east-1,gke-prod-us-central1,aks-prod-eastus"}
        
        echo "Deploying: $SERVICE_NAME:$SERVICE_VERSION"
        echo "Target clusters: $TARGET_CLUSTERS"
        
        # Validate Docker image exists
        docker manifest inspect "$SERVICE_NAME:$SERVICE_VERSION" || {
          echo "❌ Docker image not found: $SERVICE_NAME:$SERVICE_VERSION"
          exit 1
        }
        
        # Check image signature
        cosign verify --key cosign.pub "$SERVICE_NAME:$SERVICE_VERSION" || {
          echo "❌ Image signature verification failed"
          exit 1
        }
        
        # Validate Kubernetes manifests
        for manifest in deploy/k8s/*.yaml; do
          kubectl --dry-run=client --validate=true apply -f "$manifest" || {
            echo "❌ Invalid Kubernetes manifest: $manifest"
            exit 1
          }
        done
        
        # Check cluster connectivity
        IFS=',' read -ra CLUSTERS <<< "$TARGET_CLUSTERS"
        for cluster in "${CLUSTERS[@]}"; do
          echo "Checking cluster: $cluster"
          
          # Switch kubectl context
          kubectl config use-context "$cluster" || {
            echo "❌ Cannot connect to cluster: $cluster"
            exit 1
          }
          
          # Check cluster resources
          kubectl get nodes --no-headers | wc -l | xargs echo "Nodes available:"
          kubectl top nodes || echo "⚠️  Metrics not available"
          
          # Check namespace
          kubectl get namespace "$SERVICE_NAME" || {
            echo "Creating namespace: $SERVICE_NAME"
            kubectl create namespace "$SERVICE_NAME"
          }
          
          # Check RBAC
          kubectl auth can-i create deployments --namespace="$SERVICE_NAME" || {
            echo "❌ Insufficient permissions in namespace: $SERVICE_NAME"
            exit 1
          }
        done
        
        echo "✅ Pre-deployment validation complete"
      parameters:
        timeout_minutes: 10

  - name: "Argo CD Sync Preparation"
    type: "action"
    description: "Prepare Argo CD application for deployment"
    config:
      command: |
        #!/bin/bash
        set -euo pipefail
        
        echo "=== ARGO CD SYNC PREPARATION ==="
        
        # Login to Argo CD
        argocd login argocd.company.internal --username admin --password "$(vault kv get -field=password secret/argocd/admin)"
        
        # Create or update Argo CD application
        IFS=',' read -ra CLUSTERS <<< "$TARGET_CLUSTERS"
        for cluster in "${CLUSTERS[@]}"; do
          APP_NAME="${SERVICE_NAME}-${cluster}"
          
          echo "Preparing Argo CD app: $APP_NAME"
          
          # Check if application exists
          if argocd app get "$APP_NAME" &>/dev/null; then
            echo "Application $APP_NAME exists - updating"
            
            # Update application
            argocd app set "$APP_NAME" \
              --repo "https://github.com/company/microservices.git" \
              --path "services/$SERVICE_NAME/deploy/k8s" \
              --dest-server "$(kubectl config view -o jsonpath="{.clusters[?(@.name==\"$cluster\")].cluster.server}")" \
              --dest-namespace "$SERVICE_NAME" \
              --revision "$SERVICE_VERSION"
          else
            echo "Creating new application: $APP_NAME"
            
            # Create application
            argocd app create "$APP_NAME" \
              --repo "https://github.com/company/microservices.git" \
              --path "services/$SERVICE_NAME/deploy/k8s" \
              --dest-server "$(kubectl config view -o jsonpath="{.clusters[?(@.name==\"$cluster\")].cluster.server}")" \
              --dest-namespace "$SERVICE_NAME" \
              --revision "$SERVICE_VERSION" \
              --sync-policy automated \
              --self-heal \
              --auto-prune
          fi
          
          # Configure sync policy
          argocd app patch "$APP_NAME" --patch '{
            "spec": {
              "syncPolicy": {
                "automated": {
                  "prune": true,
                  "selfHeal": true
                },
                "syncOptions": [
                  "CreateNamespace=true"
                ]
              }
            }
          }'
        done
        
        echo "✅ Argo CD sync preparation complete"
      parameters:
        timeout_minutes: 5

  - name: "Progressive Rollout"
    type: "action"
    description: "Execute progressive rollout across clusters"
    config:
      command: |
        #!/bin/bash
        set -euo pipefail
        
        echo "=== PROGRESSIVE ROLLOUT ==="
        
        # Define rollout strategy
        ROLLOUT_STRATEGY=${ROLLOUT_STRATEGY:-"canary"}
        CANARY_PERCENTAGE=${CANARY_PERCENTAGE:-"10"}
        
        IFS=',' read -ra CLUSTERS <<< "$TARGET_CLUSTERS"
        
        # Start with first cluster (canary)
        CANARY_CLUSTER=${CLUSTERS[0]}
        echo "Starting canary deployment on: $CANARY_CLUSTER"
        
        # Deploy to canary cluster
        kubectl config use-context "$CANARY_CLUSTER"
        
        # Create canary deployment
        cat > /tmp/canary-deployment.yaml << EOF
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: $SERVICE_NAME
  namespace: $SERVICE_NAME
spec:
  replicas: 2
  strategy:
    canary:
      steps:
      - setWeight: $CANARY_PERCENTAGE
      - pause: {duration: 300}
      - setWeight: 50
      - pause: {duration: 300}
      - setWeight: 100
      canaryService: $SERVICE_NAME-canary
      stableService: $SERVICE_NAME-stable
      trafficRouting:
        istio:
          virtualService:
            name: $SERVICE_NAME
            routes:
            - primary
  selector:
    matchLabels:
      app: $SERVICE_NAME
  template:
    metadata:
      labels:
        app: $SERVICE_NAME
    spec:
      containers:
      - name: $SERVICE_NAME
        image: $SERVICE_NAME:$SERVICE_VERSION
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
EOF
        
        # Apply canary deployment
        kubectl apply -f /tmp/canary-deployment.yaml
        
        # Wait for canary to be ready
        kubectl rollout status rollout/$SERVICE_NAME --timeout=600s
        
        # Monitor canary metrics
        echo "Monitoring canary deployment..."
        
        # Check success metrics
        SUCCESS_RATE=$(curl -s "http://prometheus.company.internal:9090/api/v1/query?query=rate(http_requests_total{job=\"$SERVICE_NAME\",code!~\"5..\"}[5m])/rate(http_requests_total{job=\"$SERVICE_NAME\"}[5m])" | jq -r '.data.result[0].value[1]')
        
        if (( $(echo "$SUCCESS_RATE > 0.95" | bc -l) )); then
          echo "✅ Canary deployment successful (success rate: $SUCCESS_RATE)"
          
          # Promote canary
          kubectl argo rollouts promote "$SERVICE_NAME"
          
          # Deploy to remaining clusters
          for cluster in "${CLUSTERS[@]:1}"; do
            echo "Deploying to cluster: $cluster"
            
            kubectl config use-context "$cluster"
            kubectl apply -f /tmp/canary-deployment.yaml
            kubectl rollout status rollout/$SERVICE_NAME --timeout=600s
            
            # Verify deployment
            kubectl get pods -n "$SERVICE_NAME" -l app="$SERVICE_NAME"
          done
        else
          echo "❌ Canary deployment failed (success rate: $SUCCESS_RATE)"
          kubectl argo rollouts abort "$SERVICE_NAME"
          exit 1
        fi
        
        rm -f /tmp/canary-deployment.yaml
        
        echo "✅ Progressive rollout complete"
      parameters:
        timeout_minutes: 30

  - name: "Service Mesh Integration"
    type: "action"
    description: "Configure service mesh policies and traffic routing"
    config:
      command: |
        #!/bin/bash
        set -euo pipefail
        
        echo "=== SERVICE MESH INTEGRATION ==="
        
        # Configure Istio service mesh
        IFS=',' read -ra CLUSTERS <<< "$TARGET_CLUSTERS"
        for cluster in "${CLUSTERS[@]}"; do
          echo "Configuring service mesh for cluster: $cluster"
          
          kubectl config use-context "$cluster"
          
          # Enable Istio sidecar injection
          kubectl label namespace "$SERVICE_NAME" istio-injection=enabled --overwrite
          
          # Create VirtualService
          cat > /tmp/virtualservice.yaml << EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: $SERVICE_NAME
  namespace: $SERVICE_NAME
spec:
  hosts:
  - $SERVICE_NAME
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: $SERVICE_NAME
        subset: canary
  - route:
    - destination:
        host: $SERVICE_NAME
        subset: stable
EOF
          
          # Create DestinationRule
          cat > /tmp/destinationrule.yaml << EOF
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: $SERVICE_NAME
  namespace: $SERVICE_NAME
spec:
  host: $SERVICE_NAME
  trafficPolicy:
    loadBalancer:
      simple: LEAST_CONN
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 10
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
  subsets:
  - name: stable
    labels:
      version: stable
  - name: canary
    labels:
      version: canary
EOF
          
          # Create AuthorizationPolicy
          cat > /tmp/authpolicy.yaml << EOF
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: $SERVICE_NAME
  namespace: $SERVICE_NAME
spec:
  selector:
    matchLabels:
      app: $SERVICE_NAME
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/api-gateway"]
    to:
    - operation:
        methods: ["GET", "POST"]
EOF
          
          # Apply service mesh configuration
          kubectl apply -f /tmp/virtualservice.yaml
          kubectl apply -f /tmp/destinationrule.yaml
          kubectl apply -f /tmp/authpolicy.yaml
          
          # Verify service mesh configuration
          kubectl get virtualservice,destinationrule,authorizationpolicy -n "$SERVICE_NAME"
        done
        
        # Register with Consul
        echo "Registering service with Consul..."
        
        cat > /tmp/consul-service.json << EOF
{
  "ID": "$SERVICE_NAME",
  "Name": "$SERVICE_NAME",
  "Tags": ["microservice", "kubernetes", "v$SERVICE_VERSION"],
  "Address": "$SERVICE_NAME.company.internal",
  "Port": 8080,
  "Check": {
    "HTTP": "http://$SERVICE_NAME.company.internal:8080/health",
    "Interval": "10s"
  },
  "Connect": {
    "SidecarService": {}
  }
}
EOF
        
        consul services register /tmp/consul-service.json
        
        # Clean up temporary files
        rm -f /tmp/virtualservice.yaml /tmp/destinationrule.yaml /tmp/authpolicy.yaml /tmp/consul-service.json
        
        echo "✅ Service mesh integration complete"
      parameters:
        timeout_minutes: 10

  - name: "Monitoring and Alerting Setup"
    type: "action"
    description: "Configure monitoring, alerts, and dashboards for the service"
    config:
      command: |
        #!/bin/bash
        set -euo pipefail
        
        echo "=== MONITORING AND ALERTING SETUP ==="
        
        # Create ServiceMonitor for Prometheus
        cat > /tmp/servicemonitor.yaml << EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: $SERVICE_NAME
  namespace: $SERVICE_NAME
  labels:
    app: $SERVICE_NAME
spec:
  selector:
    matchLabels:
      app: $SERVICE_NAME
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    honorLabels: true
EOF
        
        # Apply ServiceMonitor to all clusters
        IFS=',' read -ra CLUSTERS <<< "$TARGET_CLUSTERS"
        for cluster in "${CLUSTERS[@]}"; do
          kubectl config use-context "$cluster"
          kubectl apply -f /tmp/servicemonitor.yaml
        done
        
        # Create Grafana dashboard
        GRAFANA_TOKEN=$(vault kv get -field=token secret/grafana/api)
        
        cat > /tmp/dashboard.json << EOF
{
  "dashboard": {
    "title": "$SERVICE_NAME Dashboard",
    "tags": ["microservice", "$SERVICE_NAME"],
    "timezone": "browser",
    "panels": [
      {
        "title": "Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(http_requests_total{job=\"$SERVICE_NAME\"}[5m])",
            "legendFormat": "{{method}} {{code}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"$SERVICE_NAME\"}[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(http_requests_total{job=\"$SERVICE_NAME\",code!~\"2..\"}[5m])/rate(http_requests_total{job=\"$SERVICE_NAME\"}[5m])",
            "legendFormat": "Error Rate"
          }
        ]
      }
    ]
  }
}
EOF
        
        # Upload dashboard to Grafana
        curl -X POST \
          -H "Authorization: Bearer $GRAFANA_TOKEN" \
          -H "Content-Type: application/json" \
          -d @/tmp/dashboard.json \
          "http://grafana.company.internal:3000/api/dashboards/db"
        
        # Create Prometheus alert rules
        cat > /tmp/alerts.yaml << EOF
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: $SERVICE_NAME-alerts
  namespace: $SERVICE_NAME
spec:
  groups:
  - name: $SERVICE_NAME
    rules:
    - alert: ${SERVICE_NAME^}HighErrorRate
      expr: rate(http_requests_total{job="$SERVICE_NAME",code!~"2.."}[5m])/rate(http_requests_total{job="$SERVICE_NAME"}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        service: $SERVICE_NAME
      annotations:
        summary: "High error rate for $SERVICE_NAME"
        description: "Error rate is {{ \$value | humanizePercentage }}"
    
    - alert: ${SERVICE_NAME^}HighLatency
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="$SERVICE_NAME"}[5m])) > 1
      for: 5m
      labels:
        severity: warning
        service: $SERVICE_NAME
      annotations:
        summary: "High latency for $SERVICE_NAME"
        description: "95th percentile latency is {{ \$value }}s"
EOF
        
        # Apply alert rules to all clusters
        for cluster in "${CLUSTERS[@]}"; do
          kubectl config use-context "$cluster"
          kubectl apply -f /tmp/alerts.yaml
        done
        
        # Clean up temporary files
        rm -f /tmp/servicemonitor.yaml /tmp/dashboard.json /tmp/alerts.yaml
        
        echo "✅ Monitoring and alerting setup complete"
      parameters:
        timeout_minutes: 5

  - name: "Health Check and Validation"
    type: "action"
    description: "Perform comprehensive health checks and validation"
    config:
      command: |
        #!/bin/bash
        set -euo pipefail
        
        echo "=== HEALTH CHECK AND VALIDATION ==="
        
        # Wait for services to be ready
        echo "Waiting for services to be ready..."
        sleep 60
        
        # Check deployment status across all clusters
        IFS=',' read -ra CLUSTERS <<< "$TARGET_CLUSTERS"
        for cluster in "${CLUSTERS[@]}"; do
          echo "Checking deployment on cluster: $cluster"
          
          kubectl config use-context "$cluster"
          
          # Check deployment status
          kubectl get deployment "$SERVICE_NAME" -n "$SERVICE_NAME" -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True" || {
            echo "❌ Deployment not available on $cluster"
            exit 1
          }
          
          # Check pod status
          READY_PODS=$(kubectl get pods -n "$SERVICE_NAME" -l app="$SERVICE_NAME" -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | grep -o "True" | wc -l)
          TOTAL_PODS=$(kubectl get pods -n "$SERVICE_NAME" -l app="$SERVICE_NAME" --no-headers | wc -l)
          
          if [ "$READY_PODS" -ne "$TOTAL_PODS" ]; then
            echo "❌ Not all pods ready on $cluster: $READY_PODS/$TOTAL_PODS"
            kubectl get pods -n "$SERVICE_NAME" -l app="$SERVICE_NAME"
            exit 1
          fi
          
          echo "✅ All pods ready on $cluster: $READY_PODS/$TOTAL_PODS"
          
          # Check service endpoints
          ENDPOINTS=$(kubectl get endpoints "$SERVICE_NAME" -n "$SERVICE_NAME" -o jsonpath='{.subsets[*].addresses[*].ip}' | wc -w)
          if [ "$ENDPOINTS" -eq 0 ]; then
            echo "❌ No service endpoints on $cluster"
            exit 1
          fi
          
          echo "✅ Service endpoints available on $cluster: $ENDPOINTS"
        done
        
        # Perform health checks
        echo "Performing health checks..."
        
        # Check health endpoint
        for cluster in "${CLUSTERS[@]}"; do
          echo "Health check for cluster: $cluster"
          
          # Get service URL
          kubectl config use-context "$cluster"
          SERVICE_URL=$(kubectl get service "$SERVICE_NAME" -n "$SERVICE_NAME" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          if [ -z "$SERVICE_URL" ]; then
            SERVICE_URL=$(kubectl get service "$SERVICE_NAME" -n "$SERVICE_NAME" -o jsonpath='{.spec.clusterIP}')
          fi
          
          # Health check
          curl -f "http://$SERVICE_URL:8080/health" || {
            echo "❌ Health check failed for $cluster"
            exit 1
          }
          
          echo "✅ Health check passed for $cluster"
        done
        
        # Check Consul service registration
        echo "Checking Consul service registration..."
        
        consul catalog service "$SERVICE_NAME" | jq -r '.[].ServiceAddress' | while read -r address; do
          curl -f "http://$address:8080/health" || {
            echo "❌ Consul service health check failed for $address"
            exit 1
          }
        done
        
        echo "✅ All Consul services healthy"
        
        # Check metrics collection
        echo "Checking metrics collection..."
        
        METRICS_COUNT=$(curl -s "http://prometheus.company.internal:9090/api/v1/query?query=up{job=\"$SERVICE_NAME\"}" | jq -r '.data.result | length')
        
        if [ "$METRICS_COUNT" -eq 0 ]; then
          echo "❌ No metrics being collected for $SERVICE_NAME"
          exit 1
        fi
        
        echo "✅ Metrics collection working: $METRICS_COUNT targets"
        
        # Performance validation
        echo "Performing performance validation..."
        
        # Simple load test
        for cluster in "${CLUSTERS[@]}"; do
          kubectl config use-context "$cluster"
          SERVICE_URL=$(kubectl get service "$SERVICE_NAME" -n "$SERVICE_NAME" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          if [ -z "$SERVICE_URL" ]; then
            SERVICE_URL=$(kubectl get service "$SERVICE_NAME" -n "$SERVICE_NAME" -o jsonpath='{.spec.clusterIP}')
          fi
          
          # Run load test
          kubectl run load-test-$(date +%s) \
            --image=williamyeh/wrk \
            --rm -i --restart=Never \
            -- -t4 -c10 -d30s --timeout=10s "http://$SERVICE_URL:8080/health"
        done
        
        echo "✅ Performance validation complete"
        
        # Final validation summary
        echo "=== DEPLOYMENT VALIDATION SUMMARY ==="
        echo "Service: $SERVICE_NAME:$SERVICE_VERSION"
        echo "Clusters: $TARGET_CLUSTERS"
        echo "Status: ✅ HEALTHY"
        echo "Monitoring: ✅ ACTIVE"
        echo "Service Mesh: ✅ CONFIGURED"
        echo "Load Balancing: ✅ ACTIVE"
        
        echo "✅ Health check and validation complete"
      parameters:
        timeout_minutes: 15 